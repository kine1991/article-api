[
  {
    "imageUrl": "http://static.nautil.us/17221_bbd24387cfc440ae2aa2a88ae96a4a79.jpg",
    "title": " Most of the Mind Can’t Tell Fact from Fiction",
    "description": "Posted by Jim Davies on May 19, 2020  Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned…",
    "category": "Culture",
    "content": "Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned it from Mary Renault’s novel The Last of the Wine. You’ll also retain false information even if you didn’t mean to. That seems like a liability: Philosophers have long concerned themselves with what they call “the paradox of fiction”—why would we find imagined stories emotionally arousing at all? The answer is that most of our mind does not even realize that fiction is fiction, so we react to it almost as though it were real.At the same time, very young children “can rationally deal with the make-believe aspects of stories,” distinguishing the actual, the possible, and the fantastical with sophistication, as Denis Dutton has written in The Art Instinct. “Not only does the artistic structure of stories speak to Darwinian sources: so does the intense pleasure taken in their universal themes of love, death, adventure, family conflict, justice, and overcoming adversity.” That may help explain why, when stories are done well, we love them so much. Just as artificial sweeteners fool our minds into thinking we’re eating sugar, stories—even weird ones like Alice’s Adventures in Wonderland—take advantage of our natural tendency to want to learn about real people, and how to treat them.Our brains can’t help but believe.There’s experimental evidence for this. Children, for example, sometimes actually believe that puppets are alive. Even animals sometimes react to pictures the same way they react to real things. The industrialized world is so full of human faces, like in ads, that we forget that it’s just ink, or pixels on a computer screen. Every time our ancestors saw something that looked like a human face, it probably was one. As a result, we didn’t evolve to distinguish reality from representation. The same perceptual machinery interprets both. The rational parts of our minds, particularly in the prefrontal cortex, do indeed know that what we’re looking at, or reading, isn’t real. One way to understand this is by thinking about optical illusions. In the Muller-Lyer illusion, we can trace and know the two horizontal lines are the same length, but at the same time appear to be different lengths. Even after you understand how an illusion operates, it continues to fool part of your mind. This is the kind of double knowledge we have when we consume fiction.These perceptual areas of our brains are very closely connected to our emotions. That’s why emotions don’t just motivate us to act in certain ways but force us to interpret the world differently. A 2011 paper, for example, explained how fear can affect vision, moods can make us more or less susceptible to visual illusions, and desire can change the apparent size of goal-relevant objects. The authors proposed that emotions offer information “about the costs and benefits of anticipated action,” knowledge that can be used swiftly, without thought, “circumventing the need for cogitating on the possible consequences of potential actions.” That’s the solution to the paradox of fiction, and why telling ourselves, “It’s only a movie,” can only partially attenuate the feelings we have about it. Our brains can’t help but believe.Jim Davies is a professor of cognitive science at Carleton University and author of Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One With the Universe. His new book, Imagination: The Science of Your Mind’s Greatest Power, comes out in November of 2019. \n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in September 2019. "
  },
  {
    "imageUrl": "http://static.nautil.us/17179_291ebc13dd6538bae3ec7959b8d770dc.jpg",
    "title": "A Window on Africa’s Resilience",
    "description": "Posted by Mark MacNamara on May 06, 2020   We called Greg Carr the other day to talk about the spread of the coronavirus in Africa. Carr, who has been featured in Nautilus, is the founder of the Gorongosa Restoration Project,…",
    "category": "Culture",
    "content": "We called Greg Carr the other day to talk about the spread of the coronavirus in Africa. Carr, who has been featured in Nautilus, is the founder of the Gorongosa Restoration Project, a partnership with the Mozambique government to revive Gorongosa National Park, that environmental treasure trove at the southern end of the Rift Valley. The 1,500 square-mile park, about the size of Rhode Island, was first given animal refuge status in the 1920s by the Portuguese, and for years was a favorite of European tourists. But in 1983 civil war broke out and the park became a no-man’s land. The place was poached to death, closed up and didn’t reopen until 1992.Renewal began in 2004 and in 2008 the government signed a restoration agreement with Carr’s foundation. The agreement, which lasts through 2043, envisions a “human rights park” that will restore both ecosystems and economic vitality. After 11 years of rebuilding infrastructure, reintroducing animals, including hippos and wildebeests, and working with local communities, Gorongosa is thriving again. The park now serves as a model for future conservation. Today some 200,000 people live around the park in a “sustainable development zone” that includes education, employment opportunities, and health service. About 700 people have full time jobs in the park; another 300, part time. Naturalist E.O. Wilson calls Gorongosa “a window on eternity.”“If there’s one thing the rest of the world can learn from Africans, it would be their resilience.”Carr is a 60-year-old entrepreneur and philanthropist who grew up in Idaho and in his mid twenties co-founded Boston Technology, a voice mail company. By the time he turned 40 he had amassed his fortune and couldn’t see the fun in doing it all over again, and so turned to philanthropy. These days he’s in Idaho Falls, on the phone six hours a day, getting the latest reports from his staff in the park, now closed until further notice.The coronavirus news from Mozambique is mixed, as it is in much of sub-Saharan Africa. With the exception of South Africa, with over 7,500 confirmed cases of COVID-19 and 148 deaths, some countries below the Equator have fewer than 100 cases. As of May 6, there were just 81 cases in Mozambique and no deaths. If these numbers don’t blow up, the quick explanation might hold that the median age in Sub Saharan Africa is under 20, just 17.6 in Mozambique; population density is low (103 people per square mile); and there’s relatively limited direct contact with heavily infected countries in other parts of the world. Still, many experts fear chaos is inevitable. Underlying conditions in Mozambique include implacable poverty and a 60-year history of colonial and civil wars. On another front, in early April, in northern Mozambique, an Isis group shot or beheaded 52 young people because they refused to be recruited. Add a 48 percent literacy rate for women, 60 percent for men. The country also suffers the world’s eighth-highest incidence of HIV; 1.5 million people have contracted the virus and nearly 40,000 people have died. Finally, a large number of Mozambicans go to South Africa for work and then return. Testing is rare in the entire country.In March, CDC Africa sent out a national directive requiring social distancing. “People are going to pay more attention to that in the cities than they are in rural Mozambique, at least until the virus really comes,” Carr said the other day. “Now, if you live in rural Mozambique, you don’t have the luxury of saying, ‘I’m isolating at home.’ People have to go out every day, to get food and water, from 40 to 60 liters a day, they have to tend to their farms. The idea of social distancing is a bit impossible for these folks.” He added, “Schools are closed and we are making our own masks for people. We all know there’s no treatment per se or certainly vaccine. If this hits, we’ll only be able to offer people Tylenol and soup.”Cases in Mozambique could shoot up as mine workers continue to return home from their jobs in South Africa. “In my opinion,” said Carr, “Mozambique does not have the capacity to deal with this type of pandemic, as there are few qualified health personnel and the high level of poverty leads people to resist isolating themselves, as they look for alternatives to take care of their families. Our Gorongosa teams are in the field, spreading prevention messages, distributing masks and water purification.” Berta Barros, head nurse at Gorongosa, told Carr recently she has three main worries: lack of COVID-19 test kits, lack of healthcare professionals to respond to sick patients, and shortage of medications for treatment. “Mozambique has a population close to 30 million and we only have 34 ventilators,” Barros said. “It’s beyond impossible to work and choose who to save.”Carr often talks about Mozambique as though he was Mozambican. “We’re very practical people,” he’ll say. “We’re not really theoretical. We’re just going to work our way through this.” He shies away from broad, open-ended questions about Africa, much less cultural comparisons and grand conclusions. “Africa is more than 1 billion people in 54 countries with, what, 2,500 languages? To make a statement like, ‘Africa is this…’ Frankly, I just think a lot of it is complete baloney.”At the same time, says Carr, “If there’s one thing the rest of the world can learn from Africans, it would be their resilience. We’ve had five years of war in Mozambique and then last year we had a cyclone that killed nearly 1,000 people. I didn’t even mention the two droughts we had in the last seven years and the armyworm that came through and ate everybody’s maize. These people had their homes washed away in a flood last year, lost everything. So they rebuild their homes and then someone says, ‘Hey, there might be a virus coming through.’ It’s just one thing after another.”What impact might the pandemic have on animals in the park? What effect will it have on just recovered antelope populations, for example, and the inevitable increase in poaching as tourism subsides? How many resources will need to be taken away from the war on other diseases to fight this? Impossible to say. But an anecdote came to Carr’s mind that suggests the vagaries of death in Southern Africa. “I got a call from a dear friend of mine yesterday, a Mozambique good friend, who said her aunt had just died. I said, ‘Wow, do you think it was COVID?’ She goes, ‘No, she’d been suffering for a while with a bad kidney.’ Life is tough in Africa. Do we know for sure this woman didn’t also have COVID and that contributed? Maybe. The truth about Africa is that disaster is hardly news. Malaria is the most prolific killer. And when they turn 50, people die and often no one knows exactly what the cause was. It’s just the way life is.”Mark MacNamara is an Asheville, North Carolina-based writer. His articles for Nautilus include “We Need to Talk About Peat” and “The Artist of the Unbreakable Code.”\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17217_300137efca1cc59de0df2f7416fe1cb1.jpg",
    "title": "Electrons May Very Well Be Conscious",
    "description": "Posted by Tam Hunt on May 14, 2020  This month, the cover of New Scientist ran the headline, “Is the Universe Conscious?” Mathematician and physicist Johannes Kleiner, at the Munich Center for Mathematical Philosophy…",
    "category": "Ideas",
    "content": "This month, the cover of New Scientist ran the headline, “Is the Universe Conscious?” Mathematician and physicist Johannes Kleiner, at the Munich Center for Mathematical Philosophy in Germany, told author Michael Brooks that a mathematically precise definition of consciousness could mean that the cosmos is suffused with subjective experience. “This could be the beginning of a scientific revolution,” Kleiner said, referring to research he and others have been conducting. Kleiner and his colleagues are focused on the Integrated Information Theory of consciousness, one of the more prominent theories of consciousness today. As Kleiner notes, IIT (as the theory is known) is thoroughly panpsychist because all integrated information has at least one bit of consciousness. You might see the rise of panpsychism as part of a Copernican trend—the idea that we’re not special. The Earth is not the center of the universe. Humans are not a treasured creation, or even the pinnacle of evolution. So why should we think that creatures with brains, like us, are the sole bearers of consciousness? In fact, panpsychism has been around for thousands of years as one of various solutions to the mind-body problem. David Skrbina’s 2007 book, Panpsychism in the West, provides an excellent history of this intellectual tradition.Electrons may have some type of extremely rudimentary mind.While there are many versions of panpsychism, the version I find appealing is known as constitutive panpsychism. It states, to put it simply, that all matter has some associated mind or consciousness, and vice versa. Where there is mind there is matter and where there is matter there is mind. They go together. As modern panpsychists like Alfred North Whitehead, David Ray Griffin, Galen Strawson, and others have argued, all matter has some capacity for feeling, albeit highly rudimentary feeling in most configurations of matter. Panpsychists look at the many rungs on the complexity ladder of nature and see no obvious line between mind and no-mind. Philosopher Thomas Nagel famously asked in 1974 what is it like to be a bat, to echolocate and fly? We can’t know with any certainty, but we can reasonably infer, based on observation of their complex behaviors and the close genetic kinship between all mammals and humans—and the fact that evolution proceeds incrementally—that bats have a rich inner life. By the same logic, we can look steadily at less-complex forms of behavior that allow us to reasonably infer some kind of mind associated with all types of matter. Yes, including even the lowly electron. While inanimate matter doesn’t evolve like animate matter, inanimate matter does behave. It does things. It responds to forces. Electrons move in certain ways that differ under different experimental conditions. These types of behaviors have prompted respected physicists to suggest that electrons may have some type of extremely rudimentary mind. For example the late Freeman Dyson, the well-known American physicist, stated in his 1979 book, Disturbing the Universe, that “the processes of human consciousness differ only in degree but not in kind from the processes of choice between quantum states which we call ‘chance’ when made by electrons.” Quantum chance is better framed as quantum choice—choice, not chance, at every level of nature. David Bohm, another well-known American physicist, argued similarly: “The ability of form to be active is the most characteristic feature of mind, and we have something that is mind-like already with the electron.”Many biologists and philosophers have recognized that there is no hard line between animate and inanimate. J.B.S. Haldane, the eminent British biologist, supported the view that there is no clear demarcation line between what is alive and what is not: “We do not find obvious evidence of life or mind in so-called inert matter…; but if the scientific point of view is correct, we shall ultimately find them, at least in rudimentary form, all through the universe.”Niels Bohr, the Danish physicist who was seminal in developing quantum theory, stated that the “very definitions of life and mechanics … are ultimately a matter of convenience…. [T]he question of a limitation of physics in biology would lose any meaning if, instead of distinguishing between living organisms and inanimate bodies, we extended the idea of life to all natural phenomena.”More recently, University of Colorado astrobiologist Bruce Jakosky, who has worked with NASA in the search for extraterrestrial life, asked rhetorically: “Was there a distinct moment when Earth went from having no life to having life, as if a switch were flipped? The answer is ‘probably not.’”Theoretical physicist Sabine Hossenfelder, author of the 2018 book Lost in Math, has taken a contrary position. “[I]f you want a particle to be conscious, your minimum expectation should be that the particle can change,” she argued in a post titled “Electrons Don’t Think.” “It’s hard to have an inner life with only one thought. But if electrons could have thoughts, we’d long have seen this in particle collisions because it would change the number of particles produced in collisions.” Yet “change” means many different things, including position in space over time. What Dyson is getting at in his remark about electrons and quantum theory is that the probabilistic distribution-outcomes of quantum experiments (like the double-slit experiment) are better explained as the product, not of pure chance (another way of saying “we don’t know”), but of numerous highly rudimentary choices by each electron in each moment about where and how to manifest. Whitehead’s variety of panpsychism, still the most worked-out version of panpsychism today, re-envisions the nature of matter in a fundamental way. For Whitehead, all actual entities, including electrons, atoms, and molecules, are “drops of experience” in that they enjoy at least a little bit of experience, a little bit of awareness. At first blush it’s a strange perspective but eventually makes a great deal of sense. Rather than being unchanging things moving around in a container of space-time—the modern view in a nutshell—Whitehead conceives of particles like electrons as a chain of successive iterations of a single electron that bear a strong likeness to each other in each iteration, but are not identical to each other. Each iteration is a little different than the last. There is no static and unchanging electron. The degree to which each iteration is more or less different than the last iteration is the place for an iota of choice, and mind. This iota of choice compounds upwards and, through the course of biological evolution, results in the complex types of mind and choice that we humans and other mammals enjoy. Whitehead, a mathematical philosopher, fleshed out in detail this process of “concrescence,” the oscillating nature of entities like electrons moment to moment, in his philosophical works Science and the Modern World, Process and Reality, and Modes of Thought. These are sometimes difficult works but are well worth the struggle to get through if you’re interested in basing physics on a more empirical metaphysical foundation.Many modern thinkers have come to embrace Whitehead and panpsychism to varying degrees, including Bohm, whose Wholeness and the Implicate Order, his magnum opus on modern physics and the nature of reality, refers to Whitehead as an inspiration. I am fleshing out in my work how we can turn these “merely” philosophical considerations about the nature of mind throughout nature into a testable set of experiments, with some early thoughts sketched here. Such experiments move debates about panpsychism out of the realm of philosophy and more firmly into the realm of science. So, yes, there is plenty of room in modern physics for electrons to “think.” Tam Hunt is a scholar and writer affiliated with the University of California, Santa Barbara. He is the author of the book Eco, Ego, Eros, that explores panpsychism across various fields, and of the General Resonance Theory of consciousness, which is panpsychist in orientation. He blogs at medium.com. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17058_523f43693bfa6c607d2fe43b43ba96fd.jpg",
    "title": "Prayer in the Time of COVID-19",
    "description": "Posted by Linda Thunstrom on April 07, 2020  In a recent viral news segment on CNN, cars are shown exiting what a narrator says is a church parking lot—dozens of people, including children, had just attended an evening service.…",
    "category": "Ideas",
    "content": "In a recent viral news segment on CNN, cars are shown exiting what a narrator says is a church parking lot—dozens of people, including children, had just attended an evening service. A reporter asks a woman in her car why, when the COVID-19 pandemic has convinced many to stay home, she went anyway. “I wouldn’t be anywhere else,” she says. The reporter follows up, “Aren’t you worried”—here the woman begins shaking her head—“that you could infect other people if you get sick inside?” “No,” she says. “I’m covered in Jesus’ blood.” As a result, she added, being infected when shopping isn’t scary or nerve-wracking. Religion is an important way people cope with catastrophes. It has been shown that people are more religiously active in the wake of crises. In a 2017 study, Philipp Ager and Antonio Ciccone found that religious participation increases with the risk of destructive rainfall;1 Emmanuelle Auriol and his colleagues likewise found, in a 2017 study, that people donate more to churches in the event of catastrophes, mainly as a form of insurance—the idea being that, should circumstances become dire, the donations would increase the odds of the divine intervening on their behalf.2 More recently, in a 2019 study, Jeanet Bentzen found that when struck by catastrophe, people seek closeness to God and pray more often.3 In a new paper, consistent with these findings, Bentzen reports recent evidence showing that religiosity has increased during this global crisis.4 Naturally, this means that people might feel a particularly strong need for their church community. Yet, with large gatherings of people being strongly discouraged, many religious leaders have moved their services online, or are careful to maintain social distance when welcoming people into their churches. Christians who prayed for hurricane victims donated less than Christians who did not pray for the victims.It makes sense that people pray more when dealing with major catastrophes. Prayer has been shown to benefit mental health.5,6 For people of faith, prayers can be part of important self-care, helping people to deal with the challenges the COVID-19 pandemic poses to mental well-being, such as anxiety caused by fear for one’s health and that of loved ones, or depression and anxiety from the self-isolation measures many countries have either recommended or enforced.Prayers can also give people a sense of purpose and guidance in difficult times. Many doctors and nurses, during moments they can spare away from the stress of treating patients, are retreating to their hospital rooftops and helipads to pray together. Religious Christians (the majority of Americans) also value receiving prayers from others. In a recent study, my collaborator Shiri Noy and I found that Christians, in the wake of personal hardship, are willing to give up monetary support in return for knowing that Christian strangers will pray for them.7But there is another side to prayer, particularly when it is trumpeted in the public arena. In March, President Trump tweeted, “We are a Country that, throughout our history, has looked to God for protection and strength in times like these….” He declared March 15 a National Day of Prayer. On March 28, Mississippi Gov. Tate Reeves wrote on his Facebook page, “I believe in the power of prayer. I believe God is in control as the Bible teaches us.” Reeves waited until April 3 to instigate a state shelter-in-place order. In public policy, prayers are risky. Prayers to God can give people a false sense of reassurance that something has been done. In the context of COVID-19, this could mean actions to control the virus or protect those on the front lines of the pandemic—the health care workers, fire fighters, and store clerks, among others. In the worst case, prayers can take the place of effective action, such as social distancing and material aid, which can prevent people from becoming sick and dying.In a recent study, I found that prayers for strangers may displace material aid in the wake of a natural disaster.8 In two out of three studies, I found that Christians who prayed for hurricane victims donated less than Christians who did not pray for the victims. While it is unknown how prayers affect other forms of support during a pandemic, it is a real risk that they have similar effects, given that the same social and psychological mechanisms may be at play. In my research, I found that prayers may take the place of material help in the wake of a natural disaster because religious Christians regard the act of praying itself to be directly helpful—hurricane victims’ material need was perceived as lower, after they had first received prayers. Further, prayers from strangers are valued only by those who share the strangers’ Christianity. (It is unknown if people similarly value prayers from other religious denominations.) My collaborator and I found that the well-being of atheists and agnostics decreases if informed that Christian strangers pray for them. Atheists and agnostics were even willing to pay money to avoid being prayed for. Ultimately, research shows that, during a social catastrophe, religion can promote prosocial behavior and prayer can offer comfort to people. But it also demonstrates that prayer risks interfering with public efforts aimed at controlling a crisis and alleviating people from its harmful effects.Linda Helena Thunstrom is an assistant professor in the Department of Economics at University of Wyoming.\n\tThe newest and most popular articles delivered right to your inbox!\nReferences1. Ager, P., & Ciccone, A. Agricultural risk and the spread of religious communities. Journal of the European Economic Association 16  1021-1068 (2017)2. Auriol, E., Lassebie, J., Panin, A., Raiber, E., & Seabright, P. God insures those who pay? Formal insurance and religious offerings in Ghana (No. 17-831). Toulouse School of Economics (2017)3. Sinding Bentzen, J. Acts of God? Religiosity and natural disasters across subnational world districts. The Economic Journal, 129 2295-2321 (2019)4. Sinding Bentzen, J. In crisis, we pray: Religiosity and the COVID-19 pandemic. Manuscript, https://www.dropbox.com/s/jc8vcx8qqdb84gn/Bentzen_religiosity_covid.pdf?dl=0 (2020)5. Maltby, J., Lewis, C. A., & Day, L. Religious orientation and psychological well-being: The role of the frequency of personal prayer. British Journal of Health Psychology 4 363–378 (1999)6. Meisenhelder, J. B., & Chandler, E. N. Frequency of prayer and functional health in Presbyterian pastors. Journal for the Scientific Study of Religion 40 323–329 (2001)7. Thunström, L. & Noy, S.  The value of thoughts and prayers, Proceedings of the National Academy of Sciences, 116 19797-19798 (2019)8. Thunström, L. Thoughts and prayers—Do they crowd out charity donations? Journal of Risk and Uncertainty 1-28. doi.org/10.1007/s11166-020-09322-9 (2020) "
  },
  {
    "imageUrl": "http://static.nautil.us/17189_a431593a04b99572ab390b80e596b75e.jpg",
    "title": "The Book That Invented the World",
    "description": "Posted by Ed Simon on May 12, 2020  Now that we’re corralled into our homes and apartments, something seems pre-modern in how our worlds have shrunk. Unlike past quarantines, we’re also connected by digital technology…",
    "category": "Ideas",
    "content": "Now that we’re corralled into our homes and apartments, something seems pre-modern in how our worlds have shrunk. Unlike past quarantines, we’re also connected by digital technology to the rest of the globe, calling to mind poet John Donne’s line from a 1633 poem about making “one little room an everywhere.” Donne came of age able to envision a mental map of the globe based on new and detailed evidence about a dizzying array of locations. His poetry is replete with globes, maps, and atlases. What’s considered to be the first atlas was first available in an Antwerp printshop 450 years ago, on May 20, only two years before Donne was born. It was large, handsome, and expensive, with the grandiose title of Theatrum Orbis Terrarum, or in English Theater of the Orb of the World. Donne was undoubtedly familiar with it. Produced by the cartographer Abraham Ortelius, it was one of the most popular books of the era. Ortelius had invented the world. Never before had all cartographic knowledge been compiled together; never before could a reader imagine the totality of the Earth so completely. Simon Garfield writes in On the Map: A Mind-Expanding Exploration of the Way the World Looks that the Theatrum’s “colors were rich and saturated, the lettering (in Latin) elaboratively cursive. The cartouches… burst with vivid additional information.” Within the folio were some 53 beautifully illustrated and colored maps based on the illustrations of 87 cartographers (who were all duly given credit), including the most up-to-date work of Gerardus Mercator. The Theatrum depicted lands from California to Cathay, from the Kara Sea to the Cape of Good Hope. The book was “a huge and instant success,” Garfield writes, “despite the fact it was the most expensive book ever produced.” Though Mercator wouldn’t appropriate the term until 1595, the Theatrum was the first of a type—an atlas. Asking a Medieval person to imagine the world and their place on it would demand a radically different sort of cognitive map than one a modern person might rely on. This affects pragmatic matters (of navigation and so forth), but also what could be termed poetic ones as well. Philosopher Bertrand Westphal writes in Geocriticism: Real and Fictional Spaces that the “perception of space and the representation of space do not involve the same things,” and this is a crucial point. Ortelius’ atlas gave women and men this new perception of space, a new cognitive map that would allow someone to envision their place and presence on the globe.Measurement rather than metaphor became a priority.Despite forgivable errors in the shape of this peninsula or the exact location of that island, what marks the Theatrum is just how correct it actually is. Comparing Ortelius’ atlas to the famed Waldseemüller map of 1507, which first makes mention of the Americas as their own continents, one is struck by how much geographic knowledge had been amassed in the intervening six decades. That earlier map, so named because it was made by the Alsatian cartographer Martin Waldseemüller who was an important influence on Ortelius, presents both North and South America as long thin ribbons floating in an  sea, details about the continents limited to the narrow band of the coasts to which European explorers ventured. Even then, there is something that looks a bit like Florida, and another portion that evokes Mexico, so that you can begin to see the proper shapes of the continents take form as if it were a Polaroid picture developing. The relative reliability of these maps is interesting, not least of which because their makers were concerned with accuracy at all. By contrast, in the Middle Ages maps weren’t necessarily intended to represent the physical world, as Turchi explains, but rather “their aim was, arguably more ambitious: to diagram history and anthropology, myth and scripture.” Medieval maps were from an entirely different genre than those presented by Ortelius. Peter Turchi quips in Maps of the Imagination: The Writer as Cartographer that “To ask for a map is to say, ‘Tell me a story.’” To condemn Medieval maps is to commit a mistake of genre, for the sort of story that they were telling was not the same as that offered by Ortelius. Medieval maps were concerned less with accuracy than with allegory and theology. Valerie Flint writes in The Imaginative Landscape of Christopher Columbus that Medieval maps known as mappae mundi were “for the greater part, less geographical descriptions than religious polemics; less maps than a species of morality.” One popular variety of Medieval map to which Flint is referring was something historians called a “T-O Map,” for the shape that it makes in presenting an idealized version of the world. These maps, basing themselves in scriptural precedent, envisioned Jerusalem at the center of the world, with Europe in the lower left quadrant on one side of the straight line of the Mediterranean, Africa in the lower right quadrant, and Asia above the straight line of the Red Sea. The whole of the world appears as a T inside of an O.  For Medieval Christians the map expressed certain religious truths about the centrality of the Holy City, the way in which the world geographically divided in a Trinitarian way, and the manner in which a paradisiacal East (for that was the direction of Eden) should be oriented at the top of the picture.Medieval Europeans, it should be emphasized, didn’t confuse T-O maps for what the world actually looked like. The purpose of such diagrams wasn’t for navigation or commerce, for colonialism or science, but rather to express something symbolic about reality. By the Age of Discovery however, measurement rather than metaphor became a priority. Garfield writes that the “vision of seeing the world anew, and the ability to express it, was what set Waldseemüller, Mercator, and Ortelius apart.” Ortelius’ empiricism, and his concern with ever more correct representations, should rightly strike us as modern. According to Garfield, to examine the Theatrum is to see the perspective of its creator for whom “geographical guesswork and overbearing religion had been banished in favor of science and reason.”Geographer Yi-Fu Tuan argues in Space and Place: The Perspective of Experience that locations “have their factual and their mythical geographies”but it is “not always easy to tell them apart.” Even facts, because they must be filtered by our interpretations, are “necessarily imbued with myths,” he writes. Renaissance map-making may have been more concerned with factual geography, but that doesn’t necessarily mean abandoning the mythical. Ortelius wasn’t the first mapmaker to be concerned with what the coastlines actually looked like, or with making sure that islands were in the right location. But he was the first to gather all of that detailed material in a single place. Those who purchased the Theatrum were not unlike those first seeing The Blue Marble, a photograph of Earth the members of the 1972 Apollo 17 mission took from space. As with that image, Ortelius’ atlas birthed a new mental geography, a new imagined space. If Medieval thinkers saw themselves as living in a symbolic and allegorical geographic order, then the Theatrum presented the physical world in its totality. The cartographer didn’t prove that the world was round (people already knew that) or that the world was large (they knew that too) but he gave people the mental images necessary to imagine themselves on that large, round globe. Ortelius gave us not disenchantment, but a differing enchantment—a sense of the sheer magnitude of the planet. Ed Simon is a staff writer at The Millions and an editor at Berfrois. His upcoming book Printed in Utopia: The Renaissance’s Radicalism will be released by Zero Books this summer. Follow him on Twitter @WithEdSimon.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17101_dfff88bd8e1ea034d76f39a71fb4ed4d.jpg",
    "title": "The Case Against Thinking Outside of the Box",
    "description": "Posted by Jordan Shapiro on April 20, 2020  Many of us are stuck now, sheltered in our messy dwellings. A daily walk lets me appreciate the urban landscaping; but I can’t stop to smell anything because a blue cotton bandana shields…",
    "category": "Ideas",
    "content": "Many of us are stuck now, sheltered in our messy dwellings. A daily walk lets me appreciate the urban landscaping; but I can’t stop to smell anything because a blue cotton bandana shields my nostrils. Indoors, constant digital dispatches chirp to earn my attention. I click on memes, status updates, and headlines, but everything is more of the same. How many ways can we repackage fear and reframe optimism? I mop the wood-laminate floor of my apartment because I hope “ocean paradise” scented Fabuloso will make my home smell a little less confining. My thoughts waft toward the old cliché: Think outside the box. I’ve always hated when people say that.To begin with, the directions are ineffectual. You can’t tell someone to think outside the box and expect them to do it. Creativity doesn’t happen on demand. Want proof? Just try to make yourself think a brilliant thought, something original, innovative, or unique. Go ahead. Do it. Right now. You can’t, no matter how hard you try. This is why ancient people believed that inspiration comes from outside. It’s external, bestowed on each of us like a revelation or prophecy—a gift from the Muses. Which means your genius does not belong to you. The word “genius” is the Latin equivalent of the ancient Greek “daemon” (δαίμονες)—like a totem animal, or a spirit companion. A genius walks beside us. It mediates between gods and mortals. It crosses over from one realm to the next. It whispers divine truth.We are paralyzed by the prospect of chaos, uncertainty, and entropy.In modern times, our mythology moves the daemons away from the heavens and into the human soul. We say, “Meditate and let your spirit guide you.” Now we think genius comes from someplace deep within. The mind? The brain? The heart? Nobody knows for sure. Yet, it seems clear to us that inspiration belongs to us; it’s tangibly contained within our corporeal boundaries. That’s why we celebrate famous artists, poets, physicists, economists, entrepreneurs, and inventors. We call them visionaries. We read their biographies. We do our best to emulate their behaviors. We study the five habits of highly successful people. We practice yoga. We exercise. We brainstorm, doodle, sign up for online personal development workshops. We do whatever we can to cultivate the fertile cognitive soil in which the springtime seeds of inspiration might sprout. But still, even though we believe that a genius is one’s own, we know that we cannot direct it. Therefore, no matter how many people tell me to think outside the box, I won’t do it. I can’t. Even if I could, I’m not sure thinking outside the box would be worthwhile. Consider the origins of the phrase. It started with an old brain teaser. Nine dots are presented in a perfect square, lined up three by three. Connect them all, using only four straight lines, without lifting your pencil from the paper. It’s the kind of puzzle you’d find on the back of a box of Lucky Charms breakfast cereal, frivolous but tricky. The solution involves letting the lines expand out onto the empty page, into the negative space. Don’t confine your markings to the dots themselves. You need to recognize, instead, that the field is wider than you’d assume. In other words, don’t interpret the dots as a square, don’t imagine that the space is constricted. Think outside the box! For years, pop-psychologists, productivity coaches, and business gurus have all used the nine-dot problem to illustrate the difference between “fixation” and “insight.” They say that we look at markings on a page and immediately try to find a pattern. We fixate on whatever meaning we can ascribe to the image. In this case, we assume that nine dots make a box. And we imagine we’re supposed to stay within its boundaries—contained and confined. We bring habitual assumptions with us even though we’re confronting a unique problem. Why? Because we are paralyzed by the prospect of chaos, uncertainty, and entropy. We cling to the most familiar ways of organizing things in order to mitigate the risk that new patterns might not emerge at all, the possibility that meaning itself could cease to exist. But this knee-jerk reaction limits our capacity for problem-solving. Our customary ways of knowing become like a strip of packing tape that’s accidentally affixed to itself—you can struggle to undo it, but it just tangles up even more. In other words, your loyalty to the easiest, most common interpretations is the sticky confirmation bias that prevents you from arriving at a truly insightful solution. At least that’s what the experts used to say. And we all liked to believe it. But our minds don’t really work that way. The box parable appeals because it reinforces our existing fantasies about an individual’s proclivity to innovate and disrupt by thinking in unexpected ways. It’s not true. Studies have found that solving the nine-dot problem has nothing to do with the box. Even when test subjects were told that the solution requires going outside the square’s boundaries, most of them still couldn’t solve it. There was an increase in successful attempts so tiny that it was considered statistically insignificant, proving that the ability to arrive at a solution to the nine-dot problem has nothing to do with fixation or insight. The puzzle is just difficult, no matter which side of the box you’re standing on.Still, I bet my twelve-year-old son could solve it. Yesterday, we unpacked a set of oil paints, delivered by Amazon. He was admiring the brushes and canvases. He was thinking about his project, trying to be creative, searching for insight. “Think inside the outside of the box,” he said.  “What does that mean?” I pushed the branded, smiling A-to-Z packaging aside and I looked at him like he was crazy. “Like with cardboard, you know, with all the little holes inside.” He was talking about the corrugations, those ridges that are pasted between layers of fiberboard. They were originally formed on the same fluted irons used to make the ruffled collars of Elizabethan-era fashion. At first, single faced corrugated paper—smooth on one side, ridged on the other—was used to wrap fragile glass bottles. Then, around 1890, the double-faced corrugated fiberboard with which we’re familiar was developed. And it transformed the packing and shipping industries. The new paperboard boxes were sturdy enough to replace wooden crates. It doesn’t take an engineering degree to understand how it works: The flutes provide support; the empty space in between makes it lightweight. My son is right; it’s all about what’s inside the outside of the box.Now I can’t stop saying it to myself, “Think inside the outside of the box.” It’s a perfect little metaphor. In a way, it even sums up the primary cognitive skill I acquired in graduate school. One could argue that a PhD just means you’ve been trained to think inside the outside of boxes. What do I mean by that? Consider how corrugation gives cardboard it’s structural integrity. The empty space—what’s not there—makes it strong and light enough that it’s a useful and efficient way to carry objects. Similarly, it’s the intellectual frameworks that make our interpretations and analyses of the world hold up. An idea can’t stand on its own; it needs a structure and a foundation. It needs a box. It requires a frame. And by looking at how those frames are assembled, by seeing how they carry a concept through to communication, we’re able to do our best thinking. We look at the empty spaces—the invisible, or tacit assumptions—which lurk within the fluted folds of every intellectual construction. We recognize that our conscious understanding of lived experience is corrugated just like cardboard. The famous sociologist Erving Goffman said as much in 1974 when he published his essay on “Frame Analysis.” He encouraged his readers to identify the principles of organization which govern our perceptions. This work went on to inspire countless political consultants, pundits, publicists, advertisers, researchers, and marketers. It’s why we now talk often about the ways in which folks “frame the conversation.” But I doubt my son has read Goffman. He just stumbled on a beautifully succinct way to frame the concept of critical thinking. Maybe he was inspired by Dr. Seuss. When my kids were little, they asked for the same story every night, “Read Sneetches Daddy!” I could practically recite the whole thing from memory: “Now, the Star-belly Sneetches had bellies with stars. The Plain-belly Sneetches had none upon thars.” It’s an us-versus-them story, a fable about the way a consumption economy encourages people to compete for status, and to alienate the “other.” If you think inside the outside of the box, it’s also a scathing criticism of a culture that’s obsessed with personal and professional transformation—always reinventing and rebranding. One day, Sylvester McMonkey McBean shows up on the Sneetches’ beaches with a peculiar box-shaped fix-it-up machine. Sneetches go in with plain-bellies and they come out with stars. Now, anyone can be anything, for a fee. McBean charges them a fortune; he exploits the Sneetches’ insecurities. He builds an urgent market demand for transformational products. He preys on their most familiar—and therefore, cozy and comforting—norms of character assessment. He disrupts their identity politics, makes it so that there’s no clear way to tell who rightfully belongs with which group. And as a result, chaos ensues. Why? Because the Sneetches discover that longstanding divisive labels and pejorative categories no longer provide a meaningful way to organize their immediate experiences. They’ve lost their frames, the structural integrity of their worldview. They feel unhinged, destabilized, unboxed, and confused.Social, cultural, economic, spiritual, psychological, emotional, intellectual: Everything is outside the box.It should sound familiar. After all, we’ve been living through an era in history that’s just like the Sneetches’. The patterns and categories we heretofore used to define self and other are being challenged every day—sometimes for good, sometimes for bad. How can we know who belongs where in a digital diaspora, a virtual panacea, where anyone can find “my tribe”? What do identity, allegiance, heredity, and loyalty even mean now that these ideas can be detached from biology and birthplace? Nobody knows for sure. And that’s just the beginning: We’ve got Sylvester-McMonkey-McBean-style disruption everywhere we look. Connected technologies have transformed the ways in which we make sense of our relationships, how we communicate with one another, our definitions of intimacy. Even before the novel coronavirus, a new global paradigm forced us to live and work in a world that’s organized according to a geopolitical model we can barely comprehend. Sure, the familiar boundaries of statehood sometimes prohibited migrant foot traffic—but information, microbes, and financial assets still moved swiftly across borders, unimpeded. Similarly, cross-national supply-chains rearranged the rules of the marketplace. High-speed transportation disrupted how we perceive the limits of time and space. Automation upset the criteria through which we understand meritocracy and self-worth. Algorithms and artificial intelligence changed the way we think about labor, employment, and productivity. Data and privacy issues blurred the boundaries of personal sovereignty. And advances in bioengineering shook up the very notion of human nature.Our boxes were already bursting. And now, cloistered at home in the midst of a pandemic, our most mundane work-a-day routines are dissolved, making it feel like our core values and deeply-held beliefs are about to tumble out all over the place. We can already envision the mess that is to come—in fact, we’re watching it unfurl in slow motion. Soon, the world will look like the intellectual, emotional, and economic equivalent of my 14-year-old’s bedroom. Dirty laundry is strewn across the floor, empty candy wrappers linger on dresser-tops, mud-caked sneakers are tossed in the corner, and the faint yet unmistakable stench of prepubescent body odor is ubiquitous. Nothing is copasetic. Nothing is in its place. Instead, everything is outside the box. It’s not creative, inspiring, or insightful. No, it’s disorienting and anxiety-provoking. I want to tidy it up as quickly as possible. I want to put things back in their familiar places. I want to restore order and eliminate chaos. But no matter how hard I try, I can’t do it, because the old boxes are ripped and torn. Their bottoms have fallen out. Now, they’re useless. Social, cultural, economic, spiritual, psychological, emotional, intellectual: Everything is outside the box. And this new sheltered-in-place experience won’t fit into old containers.Jordan Shapiro, Ph.D., is a senior fellow for the Joan Ganz Cooney Center at Sesame Workshop and Nonresident Fellow in the Center for Universal Education at the Brookings Institution. He teaches at Temple University, and wrote a column for Forbes on global education and digital play from 2012 to 2017. His book, The New Childhood, was released by Little, Brown Spark in December 2018.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17182_7db8742948ee2a4a67999d26941bfa01.jpg",
    "title": "Why People Feel Misinformed, Confused, and Terrified About the Pandemic",
    "description": "Posted by Robert Bazell on May 07, 2020   When I worked as a TV reporter covering health and science, I would often be recognized in public places. For the most part, the interactions were brief hellos or compliments. Two periods…",
    "category": "Culture",
    "content": "When I worked as a TV reporter covering health and science, I would often be recognized in public places. For the most part, the interactions were brief hellos or compliments. Two periods of time stand out when significant numbers of those who approached me were seeking detailed information: the earliest days of the pandemic that became HIV/AIDS and during the anthrax attacks shortly following 9/11. Clearly people feared for their own safety and felt their usual sources of information were not offering them satisfaction. Citizens’ motivation to seek advice when they feel they aren’t getting it from official sources is a strong indication that risk communication is doing a substandard job. It’s significant that one occurred in the pre-Internet era and one after. We can’t blame a public feeling misinformed solely on the noise of the digital age.America is now opening up from COVID-19 lockdown with different rules in different places. In many parts of the country, people have been demonstrating, even rioting, for restrictions to be lifted sooner. Others are terrified of loosening the restrictions because they see COVID-19 cases and deaths still rising daily. The officials deciding what to open, and when, seldom offer thoughtful rationales. Clearly, risk communication about COVID-19 is failing with potentially dire consequences.A big part of maintaining credibility is to admit to uncertainty—something politicians are loath to do.Peter Sandman is a foremost expert on risk communication. A former professor at Rutgers University, he was a top consultant with the Centers for Disease Control in designing crisis and emergency risk-communication, a field of study that combines public health with psychology. Sandman is known for the formula Risk = Hazard + Outrage. His goal is to create better communication about risk, allowing people to assess hazards and not get caught up in outrage at politicians, public health officials, or the media. Today, Sandman is a risk consultant, teamed with his wife, Jody Lanard, a pediatrician and psychiatrist. Lanard wrote the first draft of the World Health Organization’s Outbreak Communications Guidelines. “Jody and Peter are seen as the umpires to judge the gold standard of risk communications,” said Michael Osterholm of the Center for Infectious Disease Research and Policy at the University of Minnesota. Sandman and Lanard have posted a guide for effective COVID-19 communication on the center’s website.I reached out to Sandman to expand on their advice. We communicated through email.Sandman began by saying he understood the protests around the country about the lockdown. “It’s very hard to warn people to abide by social-distancing measures when they’re so outraged that they want to kill somebody and trust absolutely nothing people say,” he told me. “COVID-19 outrage taps into preexisting grievances and ideologies. It’s not just about COVID-19 policies. It’s about freedom, equality, too much or too little government. It’s about the arrogance of egghead experts, left versus right, globalism versus nationalism versus federalism. And it’s endlessly, pointlessly about Donald Trump.”Since the crisis began, Sandman has isolated three categories of grievance. He spelled them out for me, assuming the voices of the outraged:• “In parts of the country, the response to COVID-19 was delayed and weak; officials unwisely prioritized ‘allaying panic’ instead of allaying the spread of the virus; lockdown then became necessary, not because it was inevitable but because our leaders had screwed up; and now we’re very worried about coming out of lockdown prematurely or chaotically, mishandling the next phase of the pandemic as badly as we handled the first phase.”• “In parts of the country, the response to COVID-19 was excessive—as if the big cities on the two coasts were the whole country and flyover America didn’t need or didn’t deserve a separate set of policies. There are countless rural counties with zero confirmed cases. Much of the U.S. public-health profession assumes and even asserts without building an evidence-based case that these places, too, needed to be locked down and now need to reopen carefully, cautiously, slowly, and not until they have lots of testing and contact-tracing capacity. How dare they destroy our economy (too) just because of their mishandled outbreak!”• “Once again the powers-that-be have done more to protect other people’s health than to protect my health. And once again the powers-that-be have done more to protect other people’s economic welfare than to protect my economic welfare!” (These claims can be made with considerable truth by healthcare workers; essential workers in low-income, high-touch occupations; residents of nursing homes; African-Americans; renters who risk eviction; the retired whose savings are threatened; and others.)In their article for the Center for Infectious Disease Research and Policy, Sandman and Lanard point out that coping with a pandemic requires a thorough plan of communication. This is particularly important as the crisis is likely to enter a second wave of infection, when it could be more devastating. The plan starts with six core principles: 1) Don’t over-reassure, 2) Proclaim uncertainty, 3) Validate emotions—your audience’s and your own, 4) Give people things to do, 5) Admit and apologize for errors, and 6) Share dilemmas. To achieve the first three core principles, officials must immediately share what they know, even if the information may be incomplete. If officials share good news, they must be careful not to make it too hopeful. Over-reassurance is one of the biggest dangers in crisis communication. Sandman and Lanard suggest officials say things like, “Even though the number of new confirmed cases went down yesterday, I don’t want to put too much faith in one day’s good news.” Sandman and Lanard say a big part of maintaining credibility is to admit to uncertainty—something politicians are loath to do. They caution against invoking “science” as a sole reason for action, as science in the midst of a crisis is “incremental, fallible, and still in its infancy.” Expressing empathy, provided it’s genuine, is important, Sandman and Lanard say. It makes the bearer more human and believable. A major tool of empathy is to acknowledge the public’s fear as well as your own. There is good reason to be terrified about this virus and its consequences on society. It’s not something to hide.Sandman and Lanard say current grievances with politicians, health officials, and the media, about how the crisis has been portrayed, have indeed been contradictory. But that makes them no less valid. Denying the contradictions only amplifies divisions in the public and accelerates the outrage, possibly beyond control. They strongly emphasize one piece of advice. “Before we can share the dilemma of how best to manage any loosening of the lockdown, we must decisively—and apologetically—disabuse the public of the myth that, barring a miracle, the COVID-19 pandemic can possibly be nearing its end in the next few months.”Robert Bazell is an adjunct professor of molecular, cellular, and developmental biology at Yale. For 38 years, he was chief science correspondent for NBC News.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17119_bf65f07beaf1780ac1cf7dcf51cc8bfc.png",
    "title": "Straight Talk About a COVID-19 Vaccine",
    "description": "Posted by Robert Bazell on April 23, 2020  Wayne Koff is one of the world’s experts on vaccine development, the president and CEO of the Human Vaccines Project. He possesses a deep understanding of the opportunities and challenges…",
    "category": "Biology",
    "content": "Wayne Koff is one of the world’s experts on vaccine development, the president and CEO of the Human Vaccines Project. He possesses a deep understanding of the opportunities and challenges along the road to a safe and effective vaccine against COVID-19. He has won prestigious awards, published dozens of scientific papers, held major positions in academia, government, industry, and nonprofit organizations. But Koff, 67, has never produced a successful vaccine.“I have been an abject failure,” he says. He smiles with a charming, self-deprecating sense of humor. “That’s what the message is.”The real reason for Koff’s lack of success is that he spent most of his career searching for a vaccine against HIV, the virus that causes AIDS. It remains, as he and many others put it, “the perfect storm” of a viral infection resistant to a vaccine development. Almost 40 years after doctors first recognized the disease in five men in Los Angeles—and 70 million people have been infected worldwide—there are no adequate animal models. Neutralizing antibodies, the backbone of many vaccines, do not stop it, and most importantly, HIV begins its assault on the body by attacking CD4 T cells, which serve as the command center of much of the immune system.As for COVID-19, “We’re all hoping this one is going to be easier,” says Koff, a slight, bearded man with thick, curly salt-and-pepper hair. “There are research issues that still have to be addressed on a COVID vaccine. But they are a lot more straightforward than what we were dealing with in HIV.”Let’s say we have a vaccine in 18 months. How do you make 1 billion doses or 4 billion doses or whatever it’s going to take to immunize everybody?Koff and others started the Human Vaccines Project in 2016, modeled on the Human Genome Project. The project works with industry and academia to study the human immune system and develop vaccines, incorporating every modern-day tool, including artificial intelligence, computational biology, and big data sets. Today it is partnered with the Harvard T.H. Chan School of Public Health.With COVID-19, Koff says, scientists “know the target is the spike protein binding site.” This is where the proteins sticking out from the virus attach to the cells in the human respiratory system. “If you can elicit antibodies against those proteins, they should be neutralizing.” He puts a strong emphasis on should. To prove antibodies will prevent infection, scientists must watch a population of people who’ve been infected for months or longer. It’s a good bet, based on similar viruses, that antibodies will appear and protect—although no one right now can predict how long and how well.Depending on which count you use, more than 70 companies, universities, and other institutions are offering candidate vaccines. Koff says the real number of companies is lower. During the AIDS crisis, he says, “a lot of people claimed they had an experimental HIV vaccine in development. Some of those were a one-person lab who had created a paper company to attract investors.”But even with a lower number, almost everyone involved in the search for a vaccine agrees that several different approaches from different research organizations need to proceed in parallel. The world does not have the time to bet on one horse. The race will be neither simple nor cheap.“The probability of success, depending on whose metric is used in vaccines, is somewhere between 6 and 10 percent of candidate vaccines that make it from the animal model through licensure,” Koff says. “That process costs $1 billion or more. So you can do the math.”Koff sees big potential problems at the outset. “In the best of all worlds, let’s say we have a vaccine in 18 months. Who knows where the epidemic is going to be then and what its impact is going to be? How do you make 1 billion doses or 4 billion doses or whatever it’s going to take to immunize everybody? Will we need one dose or two or three? These are issues people just haven’t faced before.”COVID-19 also presents some unique dangers for vaccine safety. Based on how the virus behaves when it infects some people, there’s a chance a vaccine could dangerously overstimulate the immune system, a reaction called immune enhancement. “I’m hoping it’s more theoretical than real,” Koff says. “But that has to be addressed and it may slow down the entire process.” To ensure safety, he says, “It may mean we have to test the vaccine in a larger number of people. It’s one thing to do a 50-person trial in healthy adults as a safety signal. It’s another thing to run a trial of 4,000 or 5000 or more individuals.”The world does not have the time to bet on one horse. The race will be neither simple nor cheap.A virus also sometimes causes mysterious, potentially deadly blood clots. This means an experimental vaccine could hypothetically induce the same damage. “This is a bad bug,” Koff says. “We’re just starting to understand that pathogenesis.”A big question is who should be the first volunteers for widespread vaccine testing. “Who are the high-risk groups?” asks Koff. “Is it nursing-home residents and staff, health-care workers and people on the front lines, or people someplace else like grocery stores? We must also make sure a vaccine is effective for the elderly and people in the developing world.”Many vaccines work well in young and healthy people but not in older adults because immunity declines with age. Influenza vaccine is a prime example. Rotavirus vaccine, which protects against the deadliest killer—diarrheal disease in children—works better in the developed world. In the developing world, the virus often circulates year-round. Infants get antibodies from breast milk but not enough to prevent disease. Worse, those antibodies can make the vaccine less effective.Another hypothetical obstacle is that a mutation in the COVID-19 virus could render a vaccine designed today less effective in the future. While the virus mutates frequently, so far there has been little change in the critical part of the spike that binds to human cells.Of course, neither Koff nor all the others working for a COVID-19 vaccine focus solely on the potential obstacles. At one time, all vaccines against viruses either killed viruses, such as the Salk polio vaccine, or rendered them harmless, such as the Sabin polio vaccine. Now there is a multiplicity of ways to stimulate an immune response to prevent infection or reduce the consequences. These include genetically engineered protein subunits (peptides) or virus-like particles. Such approaches have led to successful vaccines against hepatitis B and human papilloma virus, which causes cervical cancer. Researchers now use “vectors”—harmless viruses attached to the protein subunits and virus particles to transmit them into the body. There are also many new adjuvants, chemicals that boost immune response to a vaccine.Newer platforms include direct injection of messenger-RNA. M-RNA is the chemical used to translate the information in DNA into proteins in all cells. The Moderna Company, which received a $483 million grant from the U.S. government, and has begun early clinical trials, uses m-RNA to try to make the body produce proteins to protect against the COVID-19 virus. INOVIO Pharmaceuticals uses pieces of DNA called plasmids to achieve the same objective. It has also begun phase 1 studies.“There are about eight platforms, and it would be good to see a couple vaccines in each of those advance,” Koff says. Predicting which of these most likely to succeed or fail he says would be “simply foolish.”Many groups, including the Human Vaccines Initiative, are plotting routes to test any possible vaccine more quickly than tradition dictates with an “adaptive trial design.” Usually trials begin with a phase 1 study of some 50 healthy people to search for any immediate signs of toxicity, then moves onto about 200 people in a phase 2, still looking for hazards and a signal of immunity, and then to phase 3 in thousands of people. But the plan here is to start phases 2 and 3 even before its predecessors are finished, and keep recruiting additional volunteers so long as no danger signals arise.Good animal models are appearing almost daily. Macaque monkeys, hamsters, and genetically engineered mice have all been infected in the laboratory and could determine whether potential vaccines exhibit various types of immunity. Members of Congress from both sides of the aisle have suggested that healthy human volunteers should be allowed to agree to be test subjects, allowing themselves to be infected. Stanley Plotkin, a vaccine researcher at the University of Pennsylvania, was among the first to suggest the idea.Arthur Caplan, a bioethicist at New York University, says that “deliberately causing disease in humans is normally abhorrent.” But COVID-19 is anything but a normal circumstance. In this case, Caplan says, “asking volunteers to take risks without pressure or coercion is not exploitation but benefitting from altruism.” At least 1,500 people have already volunteered to be such human guinea pigs, although none of the experimental vaccines is far enough along to try such challenging experiments.Koff says the key to a successful vaccine is a cooperative effort. “It’s going to take a whole different way of thinking to move this onto the expedited train,” he says. “The old dog-eat-dog, ‘I’m going to beat you to the end of the game,’ isn’t going to help us with this.” Seth Berkley, who worked with Koff at the International AIDS Vaccine Initiative, and now heads GAVI, an international vaccine organization, agrees that a COVID-19 vaccine needs a Manhattan Project approach. “An initiative of this scale won’t be easy,” Berkley says. “Extraordinary sharing of information and resources will be critical, including data on the virus, the various vaccine candidates, vaccine adjuvants, cell lines, and manufacturing advances.”Koff has no regrets about spending so many years on an AIDS vaccine without results. He learned a great deal, he says, which he’s putting to work in the COVID-19 crisis. “The reason COVID-19 vaccines should be a lot easier is because most of the platforms, the novel approaches, and the clinical infrastructure for the testing of vaccines, came out of HIV.” He pauses. “We’re far better prepared.”Robert Bazell is an adjunct professor of molecular, cellular, and developmental biology at Yale. For 38 years, he was chief science correspondent for NBC News.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17128_03e2e05daeebab05db85d5aa0a199255.png",
    "title": "How COVID-19 Will Pass from Pandemic to Prosaic",
    "description": "Posted by Anastasia Bendebury & Michael Shilo DeLay on April 24, 2020  On January 5, six days after China officially announced a spate of unusual pneumonia cases, a team of researchers at Shanghai’s Fudan University deposited the full genome sequence of…",
    "category": "Biology",
    "content": "On January 5, six days after China officially announced a spate of unusual pneumonia cases, a team of researchers at Shanghai’s Fudan University deposited the full genome sequence of the causal virus, SARS-CoV-2, into Genbank. A little more than three months later, 4,528 genomes of SARS-CoV-2 have been sequenced,1 and more than 883 COVID-related clinical trials2 for treatments and vaccines have been established. The speed with which these trials will deliver results is unknown—the delicate bаlance of efficacy and safety can only be pushed so far before the risks outweigh the benefits. For this reason, a long-term solution like vaccination may take years to come to market.3The good news is that a lack of treatment doesn’t preclude an end to the ordeal. Viral outbreaks of Ebola and SARS, neither of which had readily available vaccines, petered out through the application of consistent public health strategies—testing, containment, and long-term behavioral adaptations. Today countries that have previously battled the 2002 SARS epidemic, like Taiwan, Hong Kong, and Singapore, have shown exemplary recovery rates from COVID. Tomorrow, countries with high fatality rates like Sweden, Belgium, and the United Kingdom will have the opportunity to demonstrate what they’ve learned when the next outbreak comes to their shores. And so will we.The first Ebola case was identified in 1976,4 when a patient with hemorrhagic symptoms arrived at the Yambuku Mission Hospital, located in what is now the Democratic Republic of Congo (DRC). Patient samples were collected and sent to several European laboratories that specialized in rare viruses. Scientists, without sequencing technology, took about five weeks to identify the agent responsible for the illness as a new member of the highly pathogenic Filoviridae family.The first Ebola outbreak sickened 686 individuals across the DRC and neighboring Sudan. 453 of the patients died, with a final case fatality rate (CFR)—the number of dead out of number of sickened—of 66 percent. Despite the lethality of the virus, sociocultural interventions, including lockdowns, contact-tracing, campaigns to change funeral rites, and restrictions on consumption of game meat all proved effective interventions in the long run.That is, until 2014, when there was an exception to the pattern. Ebola appeared in Guinea, a small country in West Africa, whose population had never before been exposed to the virus. The closest epidemic had been in Gabon, 13 years before and 2,500 miles away. Over the course of two years, the infection spread from Guinea into Liberia and Sierra Leone, sickening more than 24,000 people and killing more than 10,000.Countries that have previously battled the 2002 SARS epidemic, like Taiwan and Hong Kong, have shown exemplary recovery rates.During the initial phase of the 2014 Ebola outbreak, rural communities were reluctant to cooperate with government directives for how to care for the sick and the dead. To help incentivize behavioral changes, sociocultural anthropologists like Mariane Ferme of the University of California, Berkeley, were brought in to advise the government. In a recent interview with Nautilus, Ferme indicated that strategies that allowed rural communities to remain involved with their loved ones increased cooperation. Villages located far from the capital, she said, were encouraged to “deputize someone to come to the hospital, to come to the burial, so they could come back to the community and tell the story of the body.” For communities that couldn’t afford to send someone to the capital, she saw public health officials adopt a savvy technological solution—tablets to record video messages that were carried between convalescent patients and their families.However, there were also systemic failures that, in Ferme’s opinion, contributed to the severity of the 2014 West African epidemic. In Sierra Leone, she said, “the big mistake early on was to distribute [weakly causal] information about zoonotic transmission, even when it was obviously community transmission.” In other words, although there had been an instance of zoonotic transmission—the virus jumping from a bat to a human—that initiated the epidemic, the principle danger was other contagious individuals, not game meat. Eventually, under pressure from relief groups, the government changed its messaging to reflect scientific consensus.But the retraction shook public faith in the government and bred resentment. The mismatch between messaging and reality mirrors the current pandemic. Since the COVID outbreak began, international and government health officials have issued mixed messages. Doubts initially surfaced about the certainty of the virus being capable of spreading from person to person, and the debate over the effectiveness of masks in preventing infection continues.Despite the confused messaging, there has been general compliance with stay-at-home orders that has helped flatten the curve. Had the public been less trusting of government directives, the outcome could have been disastrous, as it was in Libera in 2014. After a two-week lockdown was announced, the Liberian army conducted house-to-house sweeps to check for the sick and collect the dead. “It was a draconian method that made people hide the sick and dead in their houses,” Ferme said. People feared their loved ones would be buried without the proper rites. A direct consequence was a staggering number of active cases, and an unknown extent of community transmission. But in the end, the benchmark for the end of Ebola and SARS was the same. The WHO declared victory when the rate of new cases slowed, then stopped. By the same measure, when an entire 14-day quarantine period passes with no new cases of COVID-19, it can be declared over.It remains possible that even if we manage to end the epidemic, it will return again. Driven by novel zoonotic transmissions, Ebola has flared up every few years. Given the extent of COVID-19’s spread, and the potential for the kind of mutations that allow for re-infection, it may simply become endemic.Two factors will play into the final outcome of COVID-19 are pathogenicity and virulence. Pathogenicity is the ability of an infectious agent to cause disease in the host, and is measured by R0—the number of new infections each patient can generate. Virulence, on the other hand, is the amount of harm the infectious agent can cause, and is best measured by CFR. While the pathogenicity of Ebola, SARS, and SARS-CoV-2 is on the same order—somewhere between 1 to 3 new infections for each patient, virulence differs greatly between the two SARS viruses and Ebola.The case fatality rate for an Ebola infection is between 60 to 90 percent. The spread in CFR is due to differences in infection dynamics between strains. The underlying cause of the divergent virulence of Ebola and SARS is largely due to the tropism of the virus, meaning the cells that it attacks. The mechanism by which the Ebola virus gains entry into cells is not fully understood, but it has been shown the virus preferentially targets immune and epithelial cells.5 In other words, the virus first destroys the body’s ability to mount a defense, and then destroys the delicate tissues that line the vascular system. Patients bleed freely and most often succumb to low blood pressure that results from severe fluid loss. However, neither SARS nor SARS-CoV-2 attack the immune system directly. Instead, they enter lung epithelial cells through the ACE2 receptor, which ensures a lower CFR. What is interesting about these coronaviruses is that despite their similar modes of infection, they demonstrate a range of virulence: SARS had a final CFR of 10 percent, while SARS-CoV-2 has a pending CFR of 1.4 percent. Differences in virulence between the 2002 and 2019 SARS outbreaks could be attributed to varying levels of care between countries.The chart above displays WHO data of the relationship between the total number of cases in a country and the CFR during the 2002-2003 SARS-CoV epidemic. South Africa, on the far right, had only a single case. The patient died, which resulted in a 100 percent CFR. China, on the other hand, had 5,327 cases and 349 deaths, giving a 7 percent CFR. The chart below zooms to the bottom left corner of the graph, so as to better resolve critically affected countries, those with a caseload of less than 1,000, but with a high CFR.Here is Hong Kong, with 1,755 cases and a 17 percent CFR. There is also Taiwan, with 346 cases and an 11 percent CFR. Finally, nearly tied with Canada is Singapore with 238 cases and a 14 percent CFR.With COVID-19, it’s apparent that outcome reflects experience. China has 82,747 cases of COVID, but has lowered their CFR to 4 percent. Hong Kong has 1,026 cases and a 0.4 percent CFR. Taiwan has 422 cases at 1.5 percent CFR, and Singapore with 8,014 cases, has a 0.13 percent CFR.It was the novel coronavirus identification program established in China in the wake of the 2002 SARS epidemic that alerted authorities to SARS-CoV-2 back in November of 2019. The successful responses by Taiwan, Hong Kong, and Singapore can also be attributed to a residual familiarity with the dangers of an unknown virus, and the sorts of interventions that are necessary to prevent a crisis from spiraling out of control.In West Africa, too, they seem to have learned the value of being prepared. When Ferme returned to Liberia on March 7, she encountered airport staff fully protected with gowns, head covers, face screens, masks, and gloves. By the time she left the country, 10 days later, she said, “Airline personnel were setting up social distancing lines, and [rural vendors] hawking face masks. Motorcycle taxis drivers, the people most at risk after healthcare workers—all had goggles and face masks.”The sheer number of COVID-19 cases indicates the road to recovery will take some time. Each must be identified, quarantined, and all contacts traced and tested. Countries that failed to act swiftly, which allowed their case numbers to spiral out of control, will pay in lives and dollars. Northwestern University economists Martin Eichenbaum et al. modeled6 the cost of a yearlong shutdown to be $4.2 trillion, a cost that proactive countries will not face. A recent Harvard study7 published in Science suggests the virus will likely make seasonal appearances going forward, potentially requiring new waves of social distancing. In other words, initial hesitancy will have repercussions for years. In the future, smart containment principles,6 where restrictions are applied on the basis of health status, may temper the impact of these measures.Countries that failed to act swiftly, which allowed their case numbers to spiral out of control, will pay in lives and dollars.Inaction was initially framed as promoting herd immunity, where spread of the virus is interrupted once everyone has fallen sick with it. This is because getting the virus results in the same antibody production process as getting vaccinated—but doesn’t require the development of a vaccine. The Johns Hopkins Bloomberg School of Public Health estimates that 70 percent of the population will need to be infected with or vaccinated against the virus8 for herd immunity to work. Progress toward it has been slow, and can only be achieved through direct infection with the virus, meaning many will die. A Stanford University study in Santa Clara County9 suggests only 2.5 percent to 4.2 percent of the population have had the virus. Another COVID hotspot in Gangelt, Germany, suggests 15 percent10—higher, but still nowhere near the 70 percent necessary for herd immunity. Given the dangers inherent in waiting on herd immunity, our best hope is a vaccine.A key concern for effective vaccine development is viral mutation. This is because vaccines train the immune system to recognize specific shapes on the surface of the virus—a composite structure called the antigen. Mutations threaten vaccine development because they can change the shape of the relevant antigen, effectively allowing the pathogen to evade immune surveillance. But, so far, SARS-CoV-2 has been mutating slowly, with only one mutation found in the section most accessible to the immune system, the spike protein. What this suggests is that the viral genome may be sufficiently stable for vaccine development.What we know, though, is that Ebola was extinguished due to cooperation between public health officials and community leaders. SARS-CoV ended when all cases were identified and quarantined. The Spanish Flu in 1918 vanished after two long, deadly seasons.The final outcome of COVID-19 is still unclear. It will ultimately be decided by our patience and the financial bottom line. With 26 million unemployed and protests erupting around the country, it seems there are many who would prefer to risk life and limb rather than face financial insolvency. Applying smart containment principles in the aftermath of the shutdown might be the best way to get the economy moving again, while maintaining the safety of those at greatest risk. Going forward, vigilance and preparedness will be the watchwords of the day, and the most efficient way to prevent social and economic ruin.Anastasia Bendebury and Michael Shilo DeLay did their PhDs at Columbia University. Together they created Demystifying Science, a science literacy organization devoted to providing clear, mechanistic explanations for natural phenomena. Find them on Twitter @DemystifySci.\n\tThe newest and most popular articles delivered right to your inbox!\nReferences1. Genomic epidemiology of novel coronavirus - Global subsampling. Nextstrain www.nextstrain.org.2. Covid-19 TrialsTracker. TrialsTracker www.trialstracker.net.3. Struck, M. Vaccine R&D success rates and development times. Nature Biotechnology 14, 591-593 (1996).4. Breman, J. & Johnson, K. Ebola then and now. The New England Journal of Medicine 371 1663-1666 (2014).5. Baseler, L., Chertow, D.S., Johnson, K.M., Feldmann, H., & Morens, D.M. THe pathogenesis of Ebola virus disease. The Annual Review of Pathology 12, 387-418 (2017).6. Eichenbaum, M., Rebell, S., & Trabandt, M. The macroeconomics of epidemics. The National Bureau of Economic Research Working Paper: 26882 (2020).7. Kissler, S., Tedijanto, C., Goldstein, E., Grad, Y., & Lipsitch, M. Projecting the transmission dynamics of SARS-CoV-2 through the postpandemic period. Science eabb5793 (2020).8. D’ Souza, G. & Dowdy, D. What is herd immunity and how can we achieve it with COVID-19? Johns Hopkins COVID-19 School of Public Health Insights www.jhsph.edu (2020).9. Digitale, E. Test for antibodies against novel coronavirus developed at Stanford Medicine. Stanford Medicine News Center Med.Stanford.edu (2020).10. Winkler, M. Blood tests show 14%of people are now immune to COVID-19 in one town in Germany. MIT Technology Review (2020). "
  },
  {
    "imageUrl": "http://static.nautil.us/17094_08ed87bd895f5562aacffd47d4c086eb.png",
    "title": "What Role Will Immunity Play in Conquering COVID-19?",
    "description": "Posted by Helen Stillwell on April 17, 2020  This story was updated post-publication to include information from a study published on the preprint server medRxiv on April 17, 2020.With more than half a million cases of COVID-19 in…",
    "category": "Biology",
    "content": "This story was updated post-publication to include information from a study published on the preprint server medRxiv on April 17, 2020.With more than half a million cases of COVID-19 in the United States1 and the number of deaths increasing daily, it remains unclear when and how we might return to some semblance of pre-pandemic life. This leaves many grappling with an important question: Do you become immune after SARS-CoV-2 infection? And, if so, how long might that immunity last?In 2019, the virus SARS-CoV-2 jumped to a human host for the first time, causing the disease COVID-19. When you become infected with a new virus, your body does not possess the antibodies necessary to mount a targeted immune response. Antibodies, proteins belonging to the immunoglobulin family, consist of four chains of amino acids that form a characteristic Y-shaped structure. Antibodies are manufactured by the immune system to bind to antigens (viral proteins) to neutralize viral infectivity.When you inhale an aerosolized droplet containing SARS-CoV-2, the virus encounters the cells of the mucous membrane lining the respiratory tract. If effective contact is made, the virus binds to a particular receptor on these cells called ACE-2. After binding ACE-2, a host enzyme is co-opted to cleave the virus’ surface protein, called the spike protein, allowing the virus to enter the cell.It appears that individuals with COVID-19 do create neutralizing antibodies—the basis of immunity.Within the first few hours of infection, the body’s first line of defense—the innate immune response—is activated. The innate immune response is non-specific. When a “foreign” molecule is detected, innate immune cells signal to other cells to alter their response or prepare to combat infection.In the following days, the adaptive immune response is activated, which is more specific. The adaptive immune response will peak one to two weeks post-infection and consists of antibodies and specialized immune cells. It is called the “adaptive” immune response because of its ability to tailor the response to a specific pathogen. Antibodies can neutralize viral infectivity by preventing virus from binding to receptors, blocking cell entry, or causing virus particles to aggregate.2 Once an infection has resolved, some of these antibodies remain in the body as immunological memory to be recruited for protection in the case of reinfection. To be immune to a virus is to possess this immunological memory.Many vaccines work by activating the adaptive immune response. Inactivated virus, viral protein, or some other construct specific to a particular virus are introduced into the body as vaccines to initiate an immune response. Ideally, the body creates antibodies against the viral construct so that it can mount a succinct response when infected by the virus. However, in order to work effectively, a vaccine must provoke an immune response that is sufficiently robust. If the body only produces low concentrations of neutralizing antibodies, adequate immunological memory may not be sustained.While there is still much that we have to learn about SARS-CoV-2, it appears that individuals with COVID-19 do create neutralizing antibodies—the basis of immunity. However, we don’t know for certain how long that immunity might offer protection. On the question of COVID-19 re-infection, Matt Frieman, a coronavirus researcher at the University of Maryland School of Medicine, commented in a recent interview with NPR: “We don’t know very much … I think there’s a very likely scenario where the virus comes through this year, and everyone gets some level of immunity to it, and if it comes back again, we will be protected from it—either completely or if you do get reinfected later, a year from now, then you have much less disease. That’s the hope, but there is no way to know that.”3Immunity to a virus is measured by serological testing—patient blood is collected and analyzed for the presence of antibodies against a particular virus. Serological data is most informative when collected long-term, so the data we have been able to obtain on SARS-CoV-2 is limited. However, data on other coronaviruses that we’ve had the opportunity to study in more depth can inform our estimations on how this outbreak may evolve.First, we can look to the coronaviruses that are known to cause the common cold. Following infection with one of these coronaviruses, disease is often mild; therefore, the concentration of antibodies detected in the blood is low. This is because mild disease often indicates a less robust immune response. Interestingly, it is not the virus itself that causes us to feel sick, but, rather, our body’s response to it. Typically, the sicker we feel, the stronger the immune response; therefore, after a cold, we are often only protected for a year or two against the same virus.4 While SARS-CoV-2 wouldn’t necessarily act like these common coronaviruses, the body’s response to these coronaviruses serves as a point of reference upon which to make predictions in the absence of virus-specific data.We can also look to coronaviruses that are known to cause severe disease, such as SARS-CoV, which caused the 2002-2003 outbreak of SARS in China. One study discovered that antibodies against SARS-CoV remained in the blood of healthcare workers for 12 years after infection.5 While it is not certain that SARS-CoV-2 will provoke a response similar to that of SARS-CoV, this study provides us with information that can inform our estimates on immunity following COVID-19 and provide hope that immunity will provide long-term protection.If immunity to SARS-CoV-2 diminishes as it does for common cold coronaviruses, it is likely that wintertime outbreaks will recur.Scientists have also been working to analyze antibodies in samples from individuals infected with SARS-CoV-2. A research group in Finland recently published a study detailing the serological data collected from a COVID-19 patient over the course of their illness.6 Antibodies specific to SARS-CoV-2 were present within two weeks from the onset of symptoms. Similarly, another recent report analyzing patients with confirmed COVID-19 indicated that it took approximately 11-14 days for neutralizing antibodies to be detected in blood.7 Both of these studies, while preliminary, suggest that the basis for immunity is present in patients infected with SARS-CoV-2.Another report looked at the possibility for recurrence of COVID-19 following re-infection with SARS-CoV-2.8 In this study, rhesus macaques were infected with SARS-CoV and allowed to recover after developing mild illness. Once blood samples were collected and confirmed to test positive for neutralizing antibodies, half of the infected macaques were re-challenged with the same dose of SARS-CoV-2. The re-infected macaques showed no significant viral replication or recurrence of COVID-19. While macaques “model” human immunity, not predict it, these data further support the possibility that antibodies manufactured in response to SARS-CoV-2 are protective against short-term re-infection.We can also analyze a virus’ structure, and the information gained from sequencing the viral genome, when trying to predict its behavior. All viruses continually undergo mutation in the process of rapid replication. They lack the necessary machinery to repair changes incurred to the genetic sequence (we as humans also incur mutations to our genetic sequence daily, but we have more sophisticated genetic repair mechanisms in place). The occurrence of significant genetic changes to the viral genome that result in viable genetic changes to a virus is termed antigenic variation. We see a lot of antigenic variation in influenza viruses (thus the need to create new vaccines each year); but the coronaviruses seem to be relatively stable antigenically.4 This is because most coronaviruses have an enzyme that allows them to correct genetic errors sustained during replication. The more stable a virus remains over time, the more likely that antibodies manufactured in response to infection or vaccination will remain effective at neutralizing viral infectivity.All this considered, it appears that immunity is retained following SARS-CoV-2 infection. So too, that immunity might persist long enough to warrant the implementation of vaccination. However, we still have much to learn about this virus, and whether there may be some cross-immunity between SARS-CoV-2 and other coronaviruses. The widespread variation in patient immune responses adds an additional layer of complexity. We still don’t have a good understanding of why people have different responses to viral infection—some of this variation is owed to genetic variation, but how and why some people have more robust immune responses and more severe disease is still unknown.4 In some cases, individuals show a high immune response because the concentration of virus is high. In other cases, individuals show a high immune response because they differ in some aspect of immune regulation or efficiency. However, as levels of immunity increase generally across a population, the population approaches what is called “herd immunity”—when the percentage of a population immune to a particular virus is sufficiently high that viral load drops below the threshold required to sustain the infection in that population.9How the pandemic will evolve in the coming months is uncertain. Outcomes depend on a myriad of factors—the duration of immunity, the dynamics of transmission and how we mitigate those dynamics through social distancing, the development of therapeutics and or vaccines, and the ability of healthcare systems to handle COVID-19 caseloads. If immunity to SARS-CoV-2 diminishes as it does for common cold coronaviruses, it is likely that wintertime outbreaks will recur in coming years.10 Whether immunity to other coronaviruses might offer some cross protective immunity to SARS-CoV-2 will also play a role, albeit to a lesser extent. Widespread serological testing to assess the duration of immunity to SARS-CoV-2 is imperative, but many countries still lack this capability.A recent study looking at serological data from 3,300 symptomatic and asymptomatic individuals in California estimates that there may be as many as 48,000-81,000 people who have been infected with SARS-Cov-2 in Santa Clara County, which is 50- to 85-fold more cases than we previously thought.11 This small-scale survey emphasizes the importance of serological testing in determining the true extent of infection.The continuation of rigid social distance also hangs in a balance—one-time social distancing measures may drive the SARS-CoV-2 epidemic peak into the fall and winter months, especially if there is increased wintertime transmissibility.10 New therapeutics, vaccines, or measures such as contact tracing and quarantine—once caseloads have been reduced and testing capacity increased—might reduce the need for rigid social distancing. However, if such measures are not put in place, mathematical models predict that surveillance and recurrent social distancing may be required through 2022.10 Only time will tell.Helen Stillwell is a research associate in immunobiology at Yale University.\n\tThe newest and most popular articles delivered right to your inbox!\nReferences1. The COVID Tracking Project https://covidtracking.com/data/us-daily (2020).2. Virology Blog: About Viruses and Viral Disease. Virus neutralization by antibodies. virology.ws (2009).3. GreenfieldBoyce, N. Do you get immunity after recovering from a case of coronavirus? NPR (2020).4. Racaniello, V., Langel, S., Leifer, C., & Barker, B. Immune 29: Immunology of COVID-19. Immune Podcast. microbe.tv (2020).5. Guo, X., et al. Long-Term persistence of IgG antibodies in SARS-CoV infected healthcare workers. bioRxiv (2020). Retrieved from doi: 10.1101/20202/02/12/200213866. Haveri, A., et al. Serological and molecular findings during SARS-CoV-2 infection: the first case study in Finland, January to February 2020. Euro Surveillance 25, (2020).7. Zhao, J., et al. Antibody responses to SARS-CoV-2 in patients of novel coronavirus disease 2019. Clinical Infectious Diseases (2020). Retrieved from doi: 10.1093/cid/ciaa3448. Bao, L., et al. Reinfection could not occur in SARS-CoV-2 infected rhesus macaques. bioRxiv (2020). Retrieved from doi: 10.1101/20202.03.13.9902269. Virology Blog: About Viruses and Viral Disease. Herd immunity. virology.ws (2008).10. Kissler, S.M. Tedijanto, C., Goldstein, E., Grad, Y.H., & Lipsitch, M. Projecting the transmission dynamics of SARS-CoV-2 through the post-pandemic period. Science eabb5793 (2020).11. Bendavid, E., et al. COVID-19 antibody seroprevalence in Santa Clara County, California. medRxiv (2020). Retrieved from doi: 10.1101/2020.04.14.20062463\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16962_bdee0997d3fb6be8515a432051fe4e5c.jpg",
    "title": " The Problem with the Way Scientists Study Reason",
    "description": "Posted by Sacha Altay on March 13, 2020  Last year, I was in Paris for the International Convention of Psychological Science, one of the most prestigious gatherings in cognitive science. I listened to talks from my field, human…",
    "category": "Biology",
    "content": "Last year, I was in Paris for the International Convention of Psychological Science, one of the most prestigious gatherings in cognitive science. I listened to talks from my field, human reasoning, but I also enjoyed those on ethology, because I find studies on non-human animals, from turtles to parrots, fascinating. Despite their typically small sample sizes, I found the scientific reasoning in the animal-studies talks sounder, and their explanations richer, than the work I heard on human reasoning.The reason is simple: Ethologists evaluate their experimental paradigm, or set-up, in light of its ecological validity, or how well it matches natural surroundings. An animal’s true habitat, and its evolutionary history, have always centered the discussion. In contrast, most experimental paradigms in human reasoning, such as the Cognitive Reflexion Test (CRT) or syllogisms, are based on logic or mathematics. One of the most famous tasks of the CRT is the bat and ball problem: A bat and a ball cost $1.10. The bat costs $1 more than the ball. How much does the ball cost? Most participants fail at this task. The correct answer is not 10 cents, but 5 cents. Perhaps the ultimate tool psychologists use to study reasoning is the syllogism: For example, “Major premise: All men are animals. Minor premise: Some animals are aggressive. Conclusion: Some men are aggressive.” (Does this conclusion follow?)As I listened to talks relying on these methods, I wondered: Do people think like that in everyday life? Probably not. Did our Pleistocene ancestors? Very unlikely. Then, how should I interpret these results? Is using abstract logic on humans like asking a turtle to climb stairs?Nikolaas Tinbergen, the founder of behavioral ecology, famously stated that ethology is the art of interviewing animals in their own language. This principle is simple but powerful. And there is no reason why it should not be applied to humans. Psychologists studying reasoning extensively rely on logic and philosophy, and neglect psychology’s more natural ally: biology. The neglect stems in part from the ease with which humans can seem to understand one another. Our psychology is equipped with specialized cognitive systems, like theory of mind, that help us negotiate social life. We spontaneously attribute intentions, reasons, and beliefs to others. These heuristics help us to predict behavior, but they also parasitize our scientific understanding of the mind, blinding us to the necessity of using biology when studying ourselves. With turtles, there’s no problem, because we have only weak intuitions about their behaviors, and it’s difficult to ask them what they think.Humans are, in other words, too familiar with one another. Fundamental laws of biology, like evolution by natural selection, are falsely believed to have weak constraints on human psychology—particularly for high-level cognitive functions, like reasoning. But the human brain, just like the turtle brain, has been shaped by millions of years of evolution. Reason is unlikely to have escaped its influence. What does it mean, then, to interview humans in their own language?In a more ecological setting, when we can argue and reason collectively, the right answer spreads like wildfire.Let’s take a concrete example. One of the most discussed tasks in the psychology of reasoning is the Wason selection task, named after English psychologist Peter Wason: “Each card has a number on one side, and a patch of color on the other. Which card or cards must be turned over to test the idea that if a card shows an even number on one face, then its opposite face is red?”Most of us will turn 8 and the red card (even though this one is useless), neglecting the orange card that could falsify the rule (if, on its opposite face, we found an even number). Humans are very bad at this task. But what happens when we use ecological stimuli? Psychologists Leda Cosmides and John Tooby created a social version of the Wason selection task: “Each card has an age on one side, and a drink on the other. Which card(s) must be turned over to test the idea that if you are drinking alcohol then you must be over 18?”If you are like most participants, it now seems obvious that you need to flip the beer and the 16 cards. You solved the problem effortlessly even though the task is, logically, the same as before. Our big brains likely evolved to solve tasks related to social interactions, not abstract logical problems. The Cosmides-Tooby selection task was ecologically valid; the first one wasn’t. Using the wrong experimental design, whether it’s the task itself or the stimuli, exposes researchers to many problems—the main one being that the results become hard to interpret. You don’t know if what you found reveals an interesting feature of the human mind—such as that human deductive reasoning is biased in the classical Wason selection task—or if it’s just a methodological artifact because the stimuli were not ecological.This is why it is important, when analyzing a biological mechanism, to consider an animal’s evolutionary history—the environment in which its ancestors evolved, and the recurrent problems they had to solve. Four broad questions can be answered: “How does it work?”; “How does it develop?”; “Why does it work like that?”; and “How did it develop in the course of evolution?” The first two questions offer proximate explanations, whereas the two last questions offer ultimate explanations.As an illustration, imagine you are asked to experiment with the strengths and weaknesses of this object:You could use it as, say, a hammer, and realize that it is terrible at hammering nails. Maybe it’s some kind of hole puncher? While smashing paper sheets, a colleague asks you, confused: “Why would you try to punch holes in paper sheets with a cherry pitter?!” Wow—all of a sudden, everything becomes clear. You know exactly how to use it. You are now able to predict what it might be good (or bad) at. You feel awful for calling a cherry pitter a bad hammer…It was inappropriate.This logic of reverse engineering, at the heart of evolutionary biology, is rarely applied to reasoning. Instead, scholars attribute to reasoning the role of correcting one’s intuition and solving problems. This role is often left implicit because it’s rarely considered an object worthy of scientific discussion.1 In fact, it has long been a black box that very few have had the courage to open with the adequate biological tools.But Hugo Mercier, who I work with at École Normale Supérieure, and Dan Sperber recently ventured there in their 2017 book, The Enigma of Reason. According to them, reasoning is not a capacity to correct false intuitions or solve problems. Nature is full of problems that organisms have to solve (like finding a mate, or food for dinner) and they constantly update their priors, or beliefs, about their environment in a broadly rational fashion. For example, in the Sahara lives Cataglyphis fortis, a species of ant using a “celestial compass” and an “odometer” to find the shorter way back to the colony after finding food. These complex inferences rely on a specialized cognitive system—falling outside the domain of reasoning—updating the ants’ priors about the environment and allowing them to solve a difficult task: finding their way home in the desert.Mercier and Sperber say that reason is a tool that evolved to solve particular problems related to communication, like evaluating information provided by others, convincing family or tribe members with arguments, and justifying one’s behavior to protect and improve one’s reputation in a complex social world. Their theory makes novel and testable hypotheses, like that reason works best when people argue with each other rather than reason alone, and that we evaluate arguments more objectively than we make them.In light of their theory, the failure and success of reasoning tasks makes more sense. For example, alone, we are mediocre at solving the bat and ball problem, but in a more ecological setting, when we can argue and reason collectively, the right answer spreads like wildfire, as Mercier and his colleagues showed in a 2017 study, “Argumentation and the Diffusion of Counter-Intuitive Beliefs.”Instead of trying hard to find biases in human cognition using weird tasks with little ecological validity, the psychology of reasoning would be more productive if more researchers follow Tinbergen’s simple lesson: Interview animals in their own language. Otherwise, we might be lost in translation for a while.Sacha Altay is a Ph.D. student in cognitive science at École Normale Supérieure in Paris. He works on argumentation, misinformation, and how we evaluate communicated information. Follow him on Twitter @Sacha_Altay.\n\tThe newest and most popular articles delivered right to your inbox!\nFootnote1. There are notable exceptions, like the work of Gerd Gigerenzer and his colleagues, on the adaptive role of biases.This classic Facts So Romantic post was originally published in April 2019. "
  },
  {
    "imageUrl": "http://static.nautil.us/17023_4768620a673136ae86da54a779b72766.jpg",
    "title": "The Pandemic Can’t Lock Down Nature",
    "description": "Posted by Brandon Keim on March 27, 2020  Needing to clear my head, I went down to the Penobscot River. There they were, swimming with the mergansers, following an early pulse of river herring to the mouth of Kenduskeag stream:…",
    "category": "Biology",
    "content": "Needing to clear my head, I went down to the Penobscot River. There they were, swimming with the mergansers, following an early pulse of river herring to the mouth of Kenduskeag stream: two harbor seals, raising sleek round heads for a few long breaths before rolling under the waves.Evidently it’s not uncommon for seals to swim the couple dozen miles between Bangor, Maine, and the Atlantic Ocean, but I’d never seen them here before. They were a balm to my buzzing thoughts: What happens next? Will I become a vector of death to my elderly mother? Is the economy going to implode? For a precious few minutes there were only the seals and mergansers and the fish who drew them there, arriving as the Penobscot’s winter icepack broke and flowed to sea, a ritual enacted ever since glaciers retreated from this continental shelf.In the months ahead we can look to nature for these respites. The nonhuman world is free of charge; sunlight is a disinfectant, physical distance easily maintained, and no pandemic can suspend it. Nature offers not just escape but reassurance.In 1946, in the aftermath of World War II, with the Nazi threat vanquished but the Cold War looming, George Orwell welcomed spring’s arrival in London’s bombed-out heart. “After the sorts of winters we have had to endure recently, the spring does seem miraculous, because it has become gradually harder and harder to believe that it is actually going to happen,” he wrote in “Some Thoughts on the Common Toad.” “Every February since 1940 I have found myself thinking that this time Winter is going to be permanent. But Persephone, like the toads, always rises from the dead at about the same moment.”So she does. And so the slumbering earth warms to life. Two nights before the seals, two nights before World Health Organization declared a pandemic, before the NBA shut down with teams on the floor and fans in the seats, before the fright went beyond viral into logarithmic, was the Worm Moon: the full moon named for the imminent stir of earthworms in thawing soil.The nonhuman world is free of charge; sunlight is a disinfectant, and physical distance is easily maintained.In burrows beneath leaf litter, hibernating toads prepare to open what Orwell called “the most beautiful eye of any living creature,” resembling “the golden-coloured semi-precious stone which one sometimes sees in signet rings, and which I think is called a chrysoberyl.” Nearly as beautiful are the eyes of painted turtles waiting on pond bottoms here in eastern Maine, the ice above now retreating from shore, mallard couples dabbling in newly open water. The birds are the surest sign of spring’s imminence. Downtown the house finches are holding daily concerts. Starlings are starting to replace their gold-streaked winter plumes with more iridescent garb. In the street today I saw two male mockingbirds joust above the pavement, their white wing-bars fluttering territorial semaphores, abandoning the contest only when a car nearly ran them down. There are many quieter signs, too: pale tips of shrubs poised to grow, a spider rappelling off a low branch, fresh fox scat in the driveway. It’s red from apples preserved under snow and lined with the fur of field mice and meadow voles whose secret winter tunnels are now revealed in the grass. Somewhere soon mother fox will give birth, nursing her blind hairless charges in underground peace. Eastern comma butterflies will gather on the trunks of those apple trees and sip their rising sap. Not long after the first orange-belted bumblebee queens will appear, inspecting potential nest sites under fallen leaves and decomposing logs. Warm rainy nights will bring salamanders and newts, just a few spotted glistening inches long, some of them decades old, out from woodland hidey-holes and down ancient paths to vernal pool bacchanals held amidst a chorus of spring peepers. Woodland ephemerals will bloom in sunshine unfiltered by still-bare treetops. My favorite are trout lilies, colonies of which illuminate forest floors with a sea of bright yellow blossoms, petals falling once the canopy unfurls. “The atom bombs are piling up in the factories, the police are prowling through the cities, the lies are streaming from the loudspeakers,” Orwell wrote, “but the earth is still going round the sun.”At this point there’s no end of studies showing how nature is good for our health, how patients recover faster in hospital rooms with windows overlooking trees, how a mindful walk in the woods will lower stress and raise moods. All true, but at this moment something deeper and more urgent is offered. An affirmation of life. Will the nightmare scenes out of Italy and Spain and now New York City spread across the land? How long will the pandemic last? Will it completely rend our already tattered social fabric? When can I again play hockey or go to a coffee shop or use a credit card machine without feeling like I’m risking my own and other lives? Who will die? Nobody knows for sure, but in a few weeks the swallows will arrive, and tonight above the fields at dusk I heard the cries of woodcock.Secretive, ground-dwelling birds with limpid black eyes and long, slender beaks attuned to the frequencies of earthworm-rustles, their feathers blend perfectly with leaf litter and old grass. They rely on this camouflage, going still rather than fleeing a walker’s approach, taking wing only as a last resort. When they do, their flight is notable for its slowness and the quavering whistle of their wings. At no other time than in spring do they dare draw attention, much less put on a show: calling out, with an urgent nasal buzz best described as a peent, and flying straight upward before spiraling against a darkening sky.Brandon Keim is a freelance nature and science journalist. The author of The Eye of the Sandpiper: Stories from the Living World, he’s now writing Meet the Neighbors, forthcoming from W.W. Norton & Company, about what it means to think of wild animals as fellow persons—and what that means for the future of nature.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17051_ba66841940a5fb383b6258d37ed444dc.png",
    "title": "It’s the End of the World and This Physicist Feels Fine",
    "description": "Posted by Sabine Hossenfelder on April 02, 2020  It’s week three of COVID lockdown here in Germany. Schools are closed, my institute is closed, my husband’s workplace is also closed. Half of the shops in the city are closed. Everything,…",
    "category": "Culture",
    "content": "It’s week three of COVID lockdown here in Germany. Schools are closed, my institute is closed, my husband’s workplace is also closed. Half of the shops in the city are closed. Everything, it seems, has been canceled. Playgrounds are deserted. The police are cruising the streets to prevent people from gathering outside. It’s eerie.Work continues, but it’s different. Meetings are now teleconferences. Skype, Zoom, WebEx, Google Hangout—we are getting used to it. I am a theoretical physicist at a research institute; I don’t do lab work and I don’t teach, so for me remote communication is mostly an inconvenience, not a real problem. But the pandemic caught me in the midst of organizing a workshop. We had to cancel reservations, apologize to our sponsors, inform participants their travel plans are now nil, and postpone the meeting to a so-far unknown future date. The same has happened, for all I can tell, with all other workshops and conferences in my field, up until the summer, at least.My biggest headache, however, has so far been that the library access on my laptop didn’t work, yet I couldn’t drop off the computer with IT to fix the problem—no one’s in their office. I eventually managed to fix it myself.An unforeseen side-effect of our self-isolation has been that I produced a music video.As you may guess, working from home with the kids around does not aid my productivity. In that, I am in the same position as many of my colleagues who have families to attend to. References to Issac Newton inventing calculus during the plague lockdown just have us roll our eyes. Newton was single and childless; he didn’t have to feed, wash, educate, and entertain offspring next to writing a scientific masterpiece. I’ll be happy if my productivity does not decrease.While we try to continue life as normally as possible, our biggest worries are with the kid’s grandparents, neither of which live nearby. My younger brother, who is an engineer, made a breathing mask for my mom. And I? I feel useless. While doctors and nurses are out there fighting for lives, I sit at home wishing them good luck. As an atheist, I can’t even say I pray for them.Physics is all about modeling natural phenomena. Unsurprisingly, many of my colleagues, including me, have lately become interested in epidemic modeling. It’s not so far off our occupational interests, and, lo and behold, the pre-print server now offers some papers with condensed-matter analogies to human society. Maybe I lack imagination, but to me it seems like a hammer looking for a nail; one doesn’t become an expert in a new discipline overnight. And then there was the astrophysicist who recently made headlines with getting magnets stuck up his nose while attempting to invent a device that would beep if you touched your face. Yes, it’s funny. But then, he was just trying to help.I promise I have no intention to propose a quantum-model for disease spread, but like everyone else I have been wondering if I can make myself somewhat more useful in these difficult times. Problem is, I normally spend my days trying to understand the expansion of the universe, black holes, dark matter, or the measurement problem in quantum mechanics. “Useful” is something different. I don’t even have magnets to stick up my nose. I like to think that my research area will one day become relevant, but that day may be 100 or 1,000 years in the future. It’s not going to prevent suffering any time soon.After some soul-searching, I decided the one thing I can do that isn’t entirely useless is write and talk. People sit at home, many anxious and worried about loved ones, and a lot of their normal modes of distraction are not available. In that situation, writing, audio, and video—anything that does not require personal contact—can be a relief. So, I blog and I feed my YouTube channel with little physics lectures, and if it makes any one’s day a little better, a little more interesting, a little more enjoyable—then I’ll have succeeded.An unforeseen side-effect of our self-isolation has been that I produced a music video with friend and colleague Tim Palmer. Like most of us, he now works from home—near Oxford, UK—and as we were sorting out the postponement of our workshop, I saw he has guitars in the back of his room. Well, one thing led to another, and we ended up making a cover version of R.E.M.’s “It’s the End of the World As We Know It,” which you can now enjoy on YouTube. I hope it makes your day a little better.Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16970_816308509e3f57309be4d4b2da8d299e.jpg",
    "title": "A Professor of Disasters and Health on COVID-19",
    "description": "Posted by Ilan Kelman on March 16, 2020  A new virus sweeps the world, closing borders, shutting down arts and sports, and killing thousands of people. Is this coronavirus pandemic, with the disease named Covid-19, simply a natural…",
    "category": "Ideas",
    "content": "A new virus sweeps the world, closing borders, shutting down arts and sports, and killing thousands of people. Is this coronavirus pandemic, with the disease named Covid-19, simply a natural disaster, a culling of overpopulation as suggested by callous commentators who seem to revel in human misery? Is it nature’s rebuttal to human-caused climate change, forcing us to reduce fossil fuel-based transportation and overconsumption (apart from toilet paper)? The answer is neither. As with almost all disasters, the Covid-19 disaster is the outcome of human choices.The Earth, with its microorganisms, tectonic activity, powerful weather, and other phenomena, has long posed dangers to humans. We know this, so it is up to us to deal with it. Sometimes we manage and sometimes we do not. Sometimes we are forced into situations with few choices, such as impoverished people living on the slopes of Mexico City’s volcano or in the subsiding floodplains of Jakarta. Not everyone can or should be a planner or engineer, to avoid houses built on soils prone to liquefying in an earthquake or offices lacking basic seismic reinforcement. Sometimes, we need to trust the zoning regulations and building codes—and their monitoring and enforcement—to keep us safe. Too often, gaps are revealed only after people have died, from the collapse of the CTV Building in Christchurch, New Zealand, during the 2011 earthquake, to New Orleans flooding during Hurricane Katrina in 2005. Those who suffer most, from Australia’s 2020 bushfires to Haiti’s 2010 earthquake, tend to have the fewest options for countering their vulnerabilities which were created by others.We know that, by disturbing ecosystems, we make pandemics beyond Covid-19 more likely to occur.When we are vulnerable to nature, it is because societal actions set people up to be harmed by nature. As we cannot blame nature for disasters, we should avoid the phrase “natural disaster.” They are just “disasters.” It could be shoddily built infrastructure, breaking or not having planning regulations, not being able to afford or not having insurance, poor communication of warnings, or fearing assault in an evacuation shelter. It is the same with disease. The World Health Organization of the United Nations was lambasted for being far too slow to observe and respond to what became the largest Ebola epidemic yet known, in West Africa between 2014 and 2016. In the years before, donor countries to the WHO had slashed the funds available, particularly hitting the division responsible for surveilling, monitoring, preparing for, and responding to possible epidemics. Experienced staff departed, communication lines to health systems around the world slackened, and institutional memory faded. Not that the UN’s organizations are perfect otherwise, displaying their own operational failings alongside geographic and cultural biases. Plus, many of the Ebola-struck countries—for instance, Guinea, Liberia, and Sierra Leone—have long lacked adequate health systems, with the governments mired in corruption, conflict, external exploitation, and incompetence. Deficient local, national, and international governance for epidemics meant that Ebola spread far faster and farther afield than it would have if health systems had been supported. A further illustration comes from infected people ending up in the United Kingdom and the United States, yet neither country experienced an Ebola outbreak nor was there ever a pandemic. When it was decided that the spread of Ebola should be stopped, knowledge, resources, and actions were harnessed to stop the spread of Ebola. Earlier choices in West Africa, especially long-term backing for health systems, would have curtailed the disease far sooner.And so we come to Covid-19. When a strange form of pneumonia appeared in patients in Wuhan, China in December 2019, medical staff reported it and soon identified the origin in one market. They isolated the new virus and publicly announced its genetic sequence. Authorities gave assurances that transmission between humans was not possible and that the virus was under control, despite evidence that neither was the case. Medical staff in Wuhan noticing the sickness explained that they were not permitted to broadcast their knowledge about it. Ai Fen, an emergency department doctor, was reprimanded and told to keep quiet. An ophthalmologist, Li Wenliang, was intimidated and silenced. He eventually died of coronavirus, with the media adorning him with the poignant label of “whistle blower.”It is a choice to institute what is now referred to as a “cover up” when a potential public health threat emerges. It is a choice not to listen to health professionals hired in key positions when they are trying to save lives through public health measures. It is a choice to have opaque dissemination procedures and to try to shut down information flow. Now that the pandemic has been created by choices early on, it is a choice that many others are making to panic-buy soap while others are not bothering to wash their hands properly or to stop touching their food or face with unwashed hands. So much of disease is about human behavior. This in no way diminishes the importance of the essential medical responses. Without vaccines, smallpox, polio, rinderpest, measles, mumps, and a whole host of other lethal diseases would continue to run rampant. Along with antibiotics and other pharmaceuticals, vaccines not only save lives daily, but also reduce the costs of running health systems by stopping illness.Health systems must have technologies and tools—dialysis machines, isolation wards, defibrillators, and stents within the dizzying array—but must not stop at technical means and buildings. Any health system must be underpinned by people, training, and experience—exactly what many of the authorities disdained when people in Wuhan suddenly fell ill. Earlier choices in China might have curtailed the spread of Covid-19 before it morphed into a pandemic. Even basic hygiene when dealing with animals might have prevented the virus from jumping species to humans.Today, diseases targeted for eradication include rubella, measles, dracunculiasis (Guinea worm disease), and polio. The latter two remain endemic in conflict zones, often reappearing due to war, like polio did in 2013, in Syria, where it had disappeared a decade previously. Similarly, dracunculiasis is close to being eradicated, stubbornly remaining in areas wracked by violence including Chad and South Sudan. Choices to target these diseases are nonetheless preventing epidemics of them, with eradication in sight. London and Paris famously eliminated cholera in the 19th century by building sewage systems, among other actions. Malaria used to be prevalent in southern England and across the US. Dedicated efforts eradicated it and continue to prevent its re-introduction, despite cases from travelers and near international airports. We can continue these efforts by choice or we can let malaria return.We know that, by disturbing ecosystems, we make pandemics beyond Covid-19 more likely to occur. “In Africa, we see a lot of incursion driven by oil or mineral extraction in areas that typically had few human populations,” Dennis Carroll, an infectious disease researcher, told Nautilus editor Kevin Berger. “The problem is not only moving workers and establishing camps in these domains, but building roads that allow for even more movement of populations. Roads also allow for the movement of wildlife animals, which may be part of a food trade, to make their way into urban settlements. All these dramatic changes increase the potential spread of infection.” It is no mystery why pandemics happen. Those with the knowledge, wisdom, and resources must choose to decide to avoid these disasters that afflict everyone.Ilan Kelman is Professor of Disasters and Health at University College London and the author of Disaster By Choice: How Our Actions Turn Natural Hazards into Catastrophes. Follow him on Twitter/Instagram @IlanKelman.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16988_1f49216e5390c53da48b0e5a77a5c48c.jpg",
    "title": "Will We Have a COVID-19 Vaccine Before 2021?",
    "description": "Posted by Helen Stillwell on March 19, 2020  Is there a vaccine for SARS-CoV-2? The internet is awash with articles and posts seeking to shed light on this very question.Currently, there is no approved vaccine against SARS-CoV-2.…",
    "category": "Biology",
    "content": "Is there a vaccine for SARS-CoV-2? The internet is awash with articles and posts seeking to shed light on this very question.Currently, there is no approved vaccine against SARS-CoV-2. While several companies have announced vaccine candidates in development, it is still unlikely that a vaccine will play a significant role in the current outbreak.In the United States, vaccine candidates cannot proceed through the appropriate preclinical, clinical, regulatory, and manufacturing pipelines in a mere few weeks. Typically, under non-outbreak circumstances, vaccine development can cost up to $1 billion or more, and it often takes many years to reach approval.1 That being said, certain allowances can be made in the case of an outbreak, and we are getting better at developing platform technologies that allow for the development of viable candidates at increasingly faster rates.Globally, the therapeutic pipeline for SARS-CoV-2 contains about 15 potential vaccine candidates.2 These candidates employ various different technologies, including messenger RNA (mRNA), DNA-based, nanoparticles, synthetic, and modified virus-like particles.The two principal candidates are those being developed by Inovio Pharmaceuticals and Moderna Therapeutics (in partnership with the National Institute of Allergy and Diseases [NIAID]). Both vaccines rely on specific mechanisms that fall under the umbrella term “platform technologies.” That is, both vaccines consist of a primary nucleic construct (in this case RNA or DNA) that is amenable to the insertion of pertinent genetic sequences from a virus of interest so that the construct can be adapted for use against various different viruses. They are platforms that allow for more efficient development of vaccine candidates against emerging viruses, such as SARS-CoV-2.Both candidates are partially funded by the Coalition for Epidemic Preparedness Innovations (CEPI), a global partnership between public, private, philanthropic, and civil society organizations that possesses the resources to fast-track the development of vaccines against emerging infectious disease and enable access to these vaccines during outbreaks. While it will likely take a year or more for the majority of the vaccine candidates to initiate Phase I clinical trials, those funded by CEPI are able to accelerate these timelines.The Inovio vaccine is a DNA-based vaccine. It is in the preclinical stage of development, and phase I testing is projected to occur in the next few months. Phase I clinical trials for vaccines typically include 20-100 healthy volunteers who are administered a vaccine candidate for the purpose of evaluating safety and determining ideal dose.The Moderna-NIAID vaccine is a mRNA vaccine. Typically, mRNA vaccines include an open reading frame (ORF) for the target antigen and are flanked by untranslated regions (UTRs) with a terminal poly(A) tail.3 Theoretically, after vaccine delivery, mRNA vaccines are translated to drive transient expression of antigen to promote an immune response.Recently, Moderna escalated development and sent vaccine to NIAID to begin the process of initiating a phase I trial to test the safety and immunogenicity of the vaccine. The trial is projected to begin at the end of April, with preliminary results in July or August.4 The time it took Moderna to develop and prepare the vaccine after learning of the virus’ genetic sequence from Chinese scientists in January is extraordinary. Following the outbreak of SARS-CoV in China in 2002, it took approximately 20 months for NIAID to get a vaccine into the first stage of human testing, according to NIAID Director Anthony Fauci.Yet, it is still unclear whether Moderna’s vaccine candidate will provoke a sufficient immune response to be effective against SARS-CoV-2. The premise of gene-based platform technologies rests on the ability to target segments of the viral genome that are involved in provoking host immune response. Although we can make well-informed choices on what sequences provoke immunogenicity, we won’t know if the optimal sequence has been selected until human trials are completed. So too, there is no precedence for a vaccine of this kind since there are not yet any approved human vaccines that use this gene-based technology .Virologist Jose Esparza commented the following on Moderna’s vaccine candidate: “The rapid manufacturing of the RNA vaccine is great. But preclinical experiments are important to assess safety before carefully moving ahead with small phase I trials in human volunteers. Special attention should be placed to a potential Antibody Dependent Enhancement of Infectivity triggered by the induction of binding but no neutralizing antibodies.”Indeed, the fast development of a vaccine and imminent phase I testing do not guarantee its efficacy. We will not know until after human trials whether the sequence Moderna and NIAID selected provokes a sufficient immune response to impart protection. And, even if the first studies show encouraging results, the vaccine might not be widely available until 2021 due to the later phase clinical trials and regulatory supervision that will be required to allow for its use in the general public.Helen Stillwell is a research associate in David Hafler’s immunobiology lab at Yale University.\n\tThe newest and most popular articles delivered right to your inbox!\nReferences1. Dunn A. The Wuhan coronavirus has now claimed more lives than SARS. Top scientists told us it could take years and cost $1 billion to make a vaccine to fight the epidemic. Business Insider. (2020). https://www.businessinsider.com/wuhan-coronavirus-vaccine-could-take-years-timeline-and-cost-2020-2.2. Pang J, Wang MX, Ang IYH, Tan, SHX, Lewis RF, Chen, JI, Gutierrez RA, Gwee SXW, Chua PEY, Yan Q, Ng XY, Yap RKS, Tan HY, Teo YY, Tan CC, Cook AR, Yap JCH, Hsu LY. Potential Rapid Diagnostics, Vaccine Therapeutics for 2019 Novel Coronavirus (2019-nCoV): A Systematic Review. J. Clin. Med. (2020) 9(3). doi: 10.3390/jcm9030623.3. Zhang C, Maruggi G, Shan H, Li J. Advances in mRNA Vaccines for Infectious Diseases. Front. Immunol. (2019). doi: 10.3389/fimmu.2019.005944. Loftus P. Drugmaker Moderna Delivers First Experimental Coronavirus Vaccine for Human Testing. The Wall Street Journal. (2020). https://www.wsj.com/articles/drugmaker-moderna-delivers-first-coronavirus-vaccine-for-human-testing-11582579099.This post was originally published on virology blog, and is reprinted with permission. "
  },
  {
    "imageUrl": "http://static.nautil.us/16972_5e84bd89fe45860ca1ebaf025f494461.jpg",
    "title": "The Good and Bad News from a Coronavirus Pandemic Model",
    "description": "Posted by Robert Bazell on March 17, 2020  How many people could die from a novel coronavirus infection? Of course, no one knows. But just before anyone had a hint of COVID-19, we got an estimate from a panel of health, security,…",
    "category": "Biology",
    "content": "How many people could die from a novel coronavirus infection? Of course, no one knows. But just before anyone had a hint of COVID-19, we got an estimate from a panel of health, security, and economic experts: 65 million deaths worldwide within 18 months.That is a high estimate—likely far too high—from a model with a bunch of assumptions. But it is based on enough solid scientific, political, and business expertise to make you stop wondering why you can’t go to your local restaurant, visit your grandmother in the nursing home, and why there is every reason to be very concerned.In October 2019, the Johns Hopkins Center for Health Security, the Bill and Melinda Gates Foundation, and the World Economic Forum hosted a pandemic tabletop exercise called Event 201. It simulated the appearance of a novel coronavirus disease that spread from bats to pigs to people. “There is no possibility of a vaccine being available in the first year,” reads a line in the fictional scenario.The exercise is part of an ongoing effort, notably since 9/11, to prepare for pandemic threats, with reports and policy recommendations that have consumed a lot of time and millions of dollars. Previously the Johns Hopkins Center conducted exercises for the threats of pandemic influenza, a bioterrorist attack, and a smallpox release.There is no doubt the virus is spreading everywhere.A novel coronavirus was an obvious choice for the next exercise. Since the SARS outbreak in 2003 and the appearance of Middle East Respiratory Syndrome (MERS)—both coronaviruses—many experts have been warning that coronaviruses along with various influenzas, such as variants of avian flu, represent the greatest pandemic threats to the world.In Event 201, 15 distinguished business people, government officials, and health experts reacted to the scenario. They predicted accurately much of what we are seeing now, including overwhelmed health facilities, severely disrupted supply lines, and frightening economic collapse. They listed seven recommendations for governments, international organizations, and businesses. Like those from previous reports, they have been mostly ignored.Soon after the appearance of COVID-19, The Johns Hopkins Center released a statement warning skepticism of the 65 million figure, emphasizing it was in no way a prediction about the current pandemic because “the inputs we used for modeling the potential impact of that fictional virus are not similar to nCoV-2019.”Modeling a disease outbreak is notoriously difficult. Eric Toner of Johns Hopkins, who directed Event 201, told me, “The model used for the exercise was relatively crude and never intended to do anything other than tee up some discussion in the exercise. Although we have reasonable confidence in the model during the first few months of play, as play continues, the simplicity of the model leads to very large error ranges.”In fact, the exercise included some assumptions that could make that imaginary pandemic less severe than the current one. It posited that with its imaginary virus, every person would infect only 1.7 others while COVID-19 virus appears to infect 2 to 2.5.On the mitigating side, Event 201 estimated its imaginary virus to have a mortality rate of 10 percent. The mortality rate estimate for COVID-19 remains in flux. Bodies can be counted but no one knows how many cases, symptomatic or not, there are. A recent report in the journal, The Lancet Infectious Diseases, puts the current estimate of the mortality rate at 5.7 percent.Another big difference with Event 201 and what we know today is that COVID-19 seems to spare serious symptoms in most children and young adults. The meaning of that in terms of the ultimate number of deaths remains unknown. No one doubts that children can spread the virus. It just seems they don’t get sick.Event 201 assumed that its virus in people with mild symptoms of the common cold “alarmingly” could infect others—thus passing the disease on before they know they are sick. In this regard, COVID-19 is worse. According to a report out Monday, people in China with undocumented illness were the source of 79 percent of transmissions. Most people got infected from others who felt perfectly healthy.The third—and perhaps most ominous—aspect of Event 201 is that it spent a lot of time talking about outbreaks in the megacities, such as Sao Paulo, Mumbai, Lagos, Dhaka, and Mexico City, in many of the poorest countries of the world. This is a danger we have not heard much about as the news has focused mostly on China and countries with major resources. “When the disease got into those places,” Toner said, “we could see it would explode.”There is no doubt the virus is spreading everywhere. On March 11, the World Health Organization reported a total of 47 confirmed COVID-19 cases in nine countries in Africa and 80 confirmed cases in nine countries in South America. It is not difficult to imagine the health systems of many of these countries becoming rapidly overwhelmed. It is happening already in far richer places.Italy reported 368 deaths on March 15 alone. A recent report in the Journal of the American Medical Association details how in the Lombardy region, the Italian epicenter, just one case on February 20 exploded into a crisis that overwhelmed hospital capacity. The article warns that, “Other health care systems should prepare for a massive increase in ICU demand during an uncontained outbreak of COVID-19.” The same scenario is spreading throughout much of Europe.Because Europe adapted the WHO testing for cases, it is only about one week behind the actual spread of the disease. Due to the lack of leadership in the U.S., this country remains four to six weeks behind. The U.S. tsunami is yet to strike with anywhere near its full force. Hospitals in the U.S. are above 90 percent capacity without any considerations for COVID-19 demands. According to the planners of Event 201, and other experts, we will soon see a surge in demand at U.S. hospitals that is almost impossible to imagine. The potential consequences for the megacities of the global south are, as Event 201 pointed out, even worse.We don’t know how mitigation efforts in China, other Asian areas, Europe, and the U.S. will affect the course of the pandemic. Our massive pharma and biotechnology industries could find an effective drug anytime. In a year or more, there could be an effective vaccine. But most likely, as Event 201 and similar reports and events have been warning, we could see millions die in the meantime.Event 201 and other pandemic preparedness operations have, for years, been pointing to severe gaps in the health care preparedness of our increasingly growing, interdependent world. This is not likely the last pandemic of this century, probably not of this decade. The greatest message from Event 201 may not be just to shock us into action now, but to heed the warnings seriously in the future.Robert Bazell is adjunct professor of Molecular, Cellular, and Developmental Biology at Yale. For 38 years, he was chief science correspondent for NBC News. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16977_fb81884e2893b11a86facda51150264d.jpg",
    "title": "The Pandemic Is Showing Us How to Live with Uncertainty",
    "description": "Posted by Scott Koenig on March 18, 2020  During the Spanish flu of 1918, it was Vick’s VapoRub. During the 1962 Cuban Missile Crisis, it was canned food. Now, as the number of cases of COVID-19 grows worldwide, it’s, among…",
    "category": "Culture",
    "content": "During the Spanish flu of 1918, it was Vick’s VapoRub. During the 1962 Cuban Missile Crisis, it was canned food. Now, as the number of cases of COVID-19 grows worldwide, it’s, among other things, toilet paper. In times of precarity, people often resort to hoarding resources they think are likely to become scarce—panic buying, as it’s sometimes called. And while it’s easy to dismiss as an overreaction, it underscores just how difficult it can be, for both the general public and public health authorities, to choose the right response to a dangerous, rapidly evolving situation.“One of the reasons we have so many challenges is that there’s just so much uncertainty, especially in the early days of an outbreak,” said Glen Nowak, a former director of media relations and communications at the Centers for Disease Control and Prevention (CDC), now a professor of advertising at the University of Georgia. Even for authorities, Nowak said, the number of moving parts and open questions during a public health crisis—where the disease originated, how infectious and deadly it is, how many people are already infected and who’s at risk—can be overwhelming. This means that the rest of us, despite the experts’ best efforts at communicating, often have to make do with limited and possibly even conflicting information. “What people are often thinking of, from a psychological standpoint, is ‘What is the best way for me to cope with this uncertainty?’” Nowak told me. For many people, coping may take the form of hoarding supplies in an attempt to assert control over the situation. Or it might mean looking to others for guidance—and if everyone else in your community is taking all the toilet paper, are you going to be the odd one out?Everyone making sacrifices right now could be excused for feeling freaked out.It doesn’t help that this latest outbreak has involved an especially high level of uncertainty, due in large part to a lack of testing in the United States, which has made it nearly impossible to know just how far and how quickly the virus is spreading. Caught in a whirlwind of unknowns, many public health officials have chosen to err on the side of precaution. “Some of the measures being taken are smart preventive efforts to keep people from catching this virus,” said Politico reporter Dan Diamond on NPR’s Fresh Air. “But some are simply aggressive measures, because in the absence of knowing, it’s always safer to do more rather than less.” Many state and local governments have banned large public gatherings, closed schools, bars, and restaurants, and encouraged their citizens to socially distance as much as possible. And in an unprecedented series of events, several of the most popular sports leagues in the world announced in rapid succession that they would suspend their seasons.Unsettling as it is to see unfold, this better-safe-than-sorry approach has a lot of merit, not least because of how the human mind tends to react to uncertainty. Taking aggressive measures comes with the potential cost of setting off widespread panic, which has its downsides. But the cost of waiting for more information could be much higher. “That’s a very fine line these [public health] organizations have to tread,” said psychologist Andreas Kappes on Al-Jazeera’s Inside Story. “What we know is that if officials stress how uncertain it is—for instance, ‘if you go out, you might be fine, others might be fine’—if we stress that uncertainty, people become very optimistic and they kind of feel like ‘Well, things will be fine, I don’t have to do anything.’”For authorities, the key to encouraging vigilance and precaution without slamming the panic button may be to use the right language when talking about uncertainty. In his research, Kappes, together with Molly Crockett and Anne-Marie Nussberger, asked hundreds of participants whether they would stay at home if they got infected with an imaginary contagious disease, noting that while staying home would be costly for their career, it would limit the disease’s spread. Some participants were told that if they went to work while sick, it was uncertain whether they would infect coworkers, while other participants were told that they might infect someone vulnerable to the disease and thus bring that person serious harm. It turned out that emphasizing the potential impact of their actions on others’ well-being made participants more likely to sacrifice and stay home. “When the human costs of selfishness are made salient,” the researchers wrote, “people are more willing to forgo the personal and prioritize social interests, even amidst uncertainty.”This gives public health officials good reason to speak candidly about certain worst-case scenarios. “I believe most members of the public want to do the right thing,” said Sonja Rasmussen, professor of pediatrics and epidemiology at the University of Florida and a former executive at the CDC, in an email. “It’s essential that people know the implications of their actions.”So everyone making sacrifices right now—sitting in restless self-isolation at home, risking financial harm by missing work, depriving themselves of the company of friends and family—could be excused for feeling freaked out. We can’t really know for sure how well preventive measures are working. But what is certain is that if, when the dust settles, we can look back at all the “panic” and laugh at ourselves, we should consider it a blessing.Scott Koenig is a doctoral student in neuroscience at CUNY, where he studies morality, emotion, and psychopathy. Follow him on Twitter @scotttkoenig.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17034_85da0ea73f51b181e3ce81a57d62f5b8.png",
    "title": "Be Wary of a Model That Shows a Decline in COVID-19 Deaths",
    "description": "Posted by Robert Bazell on March 31, 2020  On April 15, 2020, 2,271 people in the United States will die from COVID-19. That day, the U.S. will be short 61,509 hospital beds, 33,440 slots in intensive care units, and 26,753 ventilators.…",
    "category": "Biology",
    "content": "On April 15, 2020, 2,271 people in the United States will die from COVID-19. That day, the U.S. will be short 61,509 hospital beds, 33,440 slots in intensive care units, and 26,753 ventilators. By August, the death toll will have climbed to 81,114.April 15 is not arbitrary. It is the date a new model forecasts the COVID-19 pandemic in the U.S. will peak, level off for a few days, and begin to decline. The model comes from The Institute for Health Metrics and Evaluation (IHME), supported by the Gates Foundation. It was developed at the request of the University of Washington School of Medicine, other  U.S.  hospital systems, and state governments  working “to determine when COVID-19 would overwhelm their ability to care for patients,” according to a statement from the organization. Data fed into the model includes mortality rates from the World Health Organization and U.S. local and state governments; implementation of social distancing policies across the U.S.; reports from the American Hospital Association on licensed hospital bed and ICU capacity; and COVID-19-related demand for ICU services. The model is updated weekly with new information.“Great, we can open restaurants and summer school in June or sooner.” That could be a disaster.On March 24, President Trump said he wanted “to have the country opened up and raring to go by Easter.” Deborah Birx, coordinator for the White House Coronavirus Task Force, said she used the IHME model as part of the effort to persuade Trump to recommend social distancing continue until at least April 30.How accurate is the model? Certainly, it is not perfect. “No one knows how many cases there are,” Abraham Flaxman, a member of the IHME team, told me.  “Testing varies enormously throughout the country, from nonexistent in many places to widespread in a few. But we can count deaths and people in hospital beds.” Public health experts acknowledge the limitations of any model to depict reality, but say models like IHME’s are valuable. “Without serious models in this epidemic, how can anyone make sensible policies?” said Barry Bloom of the Harvard School of Public Health.The IHME model sees COVID-19 rolling across the country in waves at different places and different times with peaks through April and into May. It forecasts New York will be the first to hit its peak on April 9 and will see 15,546 die by August. On the same day, New Jersey hits it peak with 2,096 deaths.  Michigan and Louisiana follow on April 10 (2,862 and 1,987 deaths, respectively). California peaks on April 26 and sees 4,306 deaths. The model forecasts Florida having its peak on May 3 and a total death count of 6,766.  The forecast deaths range from the highest in New York to 762 in Arkansas.While the model’s mortality peaks are alarming, experts are equally concerned about its prediction of declining rates. The death rate and use of hospital beds from COVID-19 has yet to decline anywhere in the U.S. “We are making those forecasts based on the effects of social distancing and other mitigation efforts we saw in other countries—especially China,” explained Flaxman. “We are assuming it will happen here in much the same way.”“That is exactly what scares me,” said Edward Kaplan, an epidemic modeler at Yale University. “Nowhere in this country are the restrictions as severe as they were in Wuhan. We have no idea what will happen if, for example, New York runs out of ventilators or if people are sent away from hospitals too soon and infect many others. People will look at this and say, ‘Great, we can open restaurants and summer school in June or sooner. That could be a disaster.”The IHME model contrasts sharply with a model, based at Imperial College, London, widely publicized two weeks ago. The Imperial College model predicted the pandemic could kill “as many as” 2.2 million Americans. But that model predicted the high number without social distancing, voluntary quarantine, school closures, and other mitigation measures—factors included in the IHME model. Nevertheless, what concerns Bloom is that policymakers might latch onto to the lower IHME mortality rates and assume “things won’t be so bad.” The most recent report from the Imperial College estimates that COVID-19 could have killed 40 million people without the global response.Even with the responses, the situation is bad now in many places. I only have to look at my own phone. People I know in emergency rooms and intensive care units in New York City hospitals have been messaging me warnings all week. “You cannot emphasize how terrifying it is working here now”; “Staff are getting sick all around us so fewer have to do more for longer hours”; “We don’t have enough personal protective equipment”; “These pitfalls are exposing countless people to the virus.”At some point, the number of deaths and people in hospital beds will go down, probably because social distancing is working. Public health experts emphasize that will not mean we can stop staying at home, avoiding crowds, and practicing other containment measures. Quickly ending the mitigation without a well-thought-out plan could bring the virus roaring back. That happened in several places during the 1918 Spanish flu pandemic and other outbreaks. When the flare-ups occurred, many places were depleted and even less able to handle the consequences. “With the continued lack of leadership, mixed messaging, and confusion from the federal government, the end could be almost as damaging as the beginning,” Bloom said.Robert Bazell is adjunct professor of Molecular, Cellular, and Developmental Biology at Yale. For 38 years, he was chief science correspondent for NBC News.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/17007_726bd18c52afc248015dcdbc463c6f85.jpg",
    "title": "A Warning from History About Simultaneous Disasters",
    "description": "Posted by Ilan Kelman on March 24, 2020  Parts of the world might have shut down, but nature never does. Even while people stay at home and learn about physical distancing, weather, tectonic shifts, meteorites, and solar storms…",
    "category": "Ideas",
    "content": "Parts of the world might have shut down, but nature never does. Even while people stay at home and learn about physical distancing, weather, tectonic shifts, meteorites, and solar storms do not pause. With many international borders closed and an increasing percentage of the population sick or rendered potential carriers of this latest coronavirus, what happens if some city or country needs an international operation for disaster aid?Simultaneous environmental hazards are far from rare. The 1923 earthquake, which hit Tokyo and Yokohama, led to an inferno fanned (rather than extinguished) by a typhoon, with over 100,000 dead. One of the largest volcanic eruptions of the 20th century, Mount Pinatubo in the Philippines in 1991, coincided with the landfall of a major typhoon. In Afghanistan in both May 1998 and March 2002, earthquakes killing thousands rumbled through during severe weather which, in turn, hampered relief operations. Science, policy, and practice offer centuries of experience in disaster aid. Assistance from around Europe reached Portugal after the 1755 Lisbon earthquake and tsunami, and also Iceland following the 1783-1784 volcanic eruption of Laki. The modern humanitarian system perhaps came of age during Biafra’s attempt to secede from Nigeria, leading to war from 1967-1970. More recent incarnations of humanitarian aid include the world’s response to the 2004 Indian Ocean tsunamis and the earthquakes in Haiti in 2010 and Nepal in 2015.Does post-catastrophe charity always begin at home without exception?What did these crises have in common? The availability of personnel and goods and countries open to receive them. The current disaster-response industry is premised on the assumption that masses of relief workers can hop on planes at short notice and hit the ground running. Provided that donors comply (which is not always the case), anything the disaster-affected population needs can be brought in—search-and-rescue dogs and equipment, drinking water, food, tents, other shelter materials, hygiene products, and medical supplies.Of course, precedents show that, even when available, disaster aid and relief workers are not necessarily welcome. Borders can be closed to international humanitarian assistance. In early May 2008, Cyclone Nargis roared through Burma, killing perhaps over 100,000 people. Burma’s government declined international aid, leading to discussions about whether or not other countries’ militaries should forcibly deliver relief supplies. The United Nations’ first plane landed about a week later, with Burma stating that it needed aid supplies, but not aid workers. Finally, nearly three weeks after the cyclone formed, an international relief operation was in full swing. Since 1995, North Korea has experienced a series of floods, droughts, famines, and epidemics as well as, in April 2004, a train explosion that might have killed thousands. The state permitted international aid to enter at times, especially from South Korea, but often reluctantly, occasionally linked to political concessions which were later revoked, and with no long-term impact on North Korea’s relations with the rest of the world.What does this mean for international disaster aid with COVID-19 restricting travel and countries imposing isolation? If an earthquake now hits India or Iran, like in 2001 and 2003, respectively, killing over 20,000 people in each country—or if we witness a repeat of 2005’s Hurricane Katrina in the US or 2011’s tsunami in Japan—will the world respond? Would the world wish to respond?Currently, health systems and social services are stretched to their breaking points. Doctors and nurses are dying. The United Kingdom is asking retired health care professionals to return to the frontlines. Other services need staffing on call as well. If we want our utilities to continue—water, sewage, gas, electricity, and the lifelines beyond isolation of the internet and cell phones—then we need professionals ready to step in when colleagues become sick or need to stay away from others. Even if these workers wished to join an international humanitarian operation, they would not be able to take a leave of absence due to needs at home. The dangers of letting them travel must also be considered. A 14-day isolation period is becoming standard for international arrivals to numerous countries. This is pointless if you are there to pull people from the rubble or to set up a mobile water treatment plan for thirsty disaster survivors. How do you choose whether to, on the one hand, save people from collapsed buildings or contaminated drinking water and, on the other hand, not risk (re)infecting a country with COVID-19?Earlier this month, tornadoes slammed into Tennessee, killing two dozen people. Emergency managers began wondering what people with COVID-19 symptoms or otherwise in isolation should do when tornado warnings are issued. Tornado shelters are small, crowded, and poorly ventilated. Even the 10-40 minutes in a shelter are long enough to spread the coronavirus. Not entering the shelter risks lives from the storm. Nearly three weeks later, an earthquake shook Croatia, forcing a hospital to evacuate and sending crowds into open spaces, contravening coronavirus distancing guidance.Grappling with these trade-offs is not a new problem. Some European countries are rabies-free whereas the disease remains prevalent in others. Allowing the free movement of rescue dogs around Europe revealed the introduction of some potentially unvaccinated animals from countries with rabies into countries without the disease. Should fewer rescue dogs be available in order to avoid spreading rabies? It is not just people and animals, but also goods. Imagine how the veterans of the Toilet Paper Wars of Sydney and Vancouver would feel seeing rolls being distributed in a far-off disaster-afflicted land. Those who rely on bottled water for daily medical needs might be incensed at seeing their country’s soldiers or non-profit organizations distributing it elsewhere when local stores have none.Would international outrage emerge if the novel coronarvirus were used to mask a long-planned genocide?Thus arises a major debate among disaster ethicists. Does post-catastrophe charity always begin at home without exception? Do our own ongoing disaster problems entirely preclude helping others? If generosity hurts us, is altruism wrong? Why are we even in a long-term situation where richer countries cannot handle their own disease disaster so that poorer countries must, as always, suffer even more?We could plan and prepare for addressing multiple crises simultaneously. Instead, despite decades of pandemic planning and scenario development—as well as experience dating back to, among other events, the Spanish flu of 1918-1920—many governments and companies are reacting as if a pandemic has never before been seen. Even much of the knowledge gleaned from the world’s worst Ebola outbreak to date, 2014-2016 in West Africa, seems to have vanished in four short years. Should another major environmental hazard suddenly manifest somewhere in the world we would, apparently, be surprised.Conversely, Singapore and Taiwan rapidly applied lessons learned from a coronavirus outbreak in 2003, Severe Acute Respiratory Syndrome (SARS). Whether or not Taiwan is ready for the next earthquake and Singapore for the next storm surge or Indonesian volcanic eruption has yet to be seen.As with Biafra, people suffering need humanitarian relief for more than just environmental hazards. Armed conflicts continue today from the Philippines to Yemen. In 2003, with the world distracted by the new wars in Afghanistan and Iraq, genocide was unleashed in Darfur, Sudan. Today, the Rohingya, Yazidis, and Syrians have been bumped off the front pages and donor spreadsheets. Would international outrage emerge if the novel coronarvirus were used to mask a long-planned genocide?All forms of disasters continue, irrespective of the current pandemic. We know that concurrent hazards can emerge and we know that preventing a disaster is always better than applying a cure, to help ourselves and to avoid having to coordinate and provide international disaster aid. Nowadays, prevention might be the only alternative, because a cure might not even be feasible for those suffering from disaster beyond COVID-19.Ilan Kelman is Professor of Disasters and Health at University College London and the author of Disaster By Choice: How Our Actions Turn Natural Hazards into Catastrophes. Follow him on Twitter/Instagram @ILANKELMAN.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16999_1d4bf338b9f623355c3dfc9f20aa2b53.png",
    "title": "Let’s Aim for Physical Rather Than Social Distancing",
    "description": "Posted by Kenneth E. Miller on March 20, 2020  Amid all the calls in nearly every country for social distancing, the most powerful tool we have to slow the spread of the novel coronavirus, one important fact gets lost: We are fundamentally…",
    "category": "Ideas",
    "content": "Amid all the calls in nearly every country for social distancing, the most powerful tool we have to slow the spread of the novel coronavirus, one important fact gets lost: We are fundamentally social beings, and social distancing can carry a heavy psychological price tag. This is particularly true for people who live on their own, but we can all struggle with the emotional impact of isolation and its first cousin, loneliness.Although there’s been no time to study the effects of social distancing related specifically to the novel coronavirus, we know a great deal about the impact of social isolation on mental and physical health. It’s often experienced as highly stressful, and the stress can become toxic. Isolation, particularly when it causes loneliness, increases the risk of anxiety and depression. Social isolation has the equivalent adverse impact on physical health to smoking 15 cigarettes a day. It’s a greater threat to health than obesity, and is linked to an increased risk of a wide variety of health problems. In a study highly relevant to our current situation, students who reported higher levels of loneliness responded less powerfully to flu vaccinations.The growing recognition that social distancing carries psychological and physical health risks has led to a growing call to change the term to “physical distancing,” a recognition that what we need is greater physical distance between people, not greater social distance. Yes, physical distance certainly means a reduction in social contact, and no amount of rebranding the term will can change that. But the idea is still a good one: Let’s focus on reducing physical contact, while maintaining—to the extent possible—the social connections that help us thrive and stay healthy. Let’s protect ourselves psychologically as well as physically, while doing everything possible to slow the spread of the virus—to “flatten the curve” of its impact.Just remember: We keep ourselves virus-free not just to protect ourselves, but to slow the spread of the virus among everyone.This shift towards thinking in terms of physical rather than social distancing can be seen in the titles of some very useful articles, like “Physical, Not Social Distancing: Staying Connected in the Coronavirus Age”; “How to Socially Distance and Stay Sane”; “Corona and the Isolation Paradox”; and “Staying Happy While Social Distancing.”Once we distinguish between physical and social distancing, we can refocus on the power of staying socially connected while maintaining a safe physical distance. We can easily get creative in finding ways of building in social time for ourselves, and for those we care about who might be feeling isolated. None of it is rocket science, but small steps can lead to big effects.Experts are mixed on just how much actual social contact is safe. See this excellent article from The Atlantic: “The Do’s and Don’ts of Social Distancing,” an interview with three public health experts. Carolyn Cannuscio from the Center for Public Health Initiatives at the University of Pennsylvania, takes the view that neighbors or households who agree to limit outside contact can socialize with each other. And while the Center for Disease Control and Prevention (CDC) is still recommending that social gatherings be limited to fewer than 10 people, the CDC guidelines seem out of sync with other more stringent calls, particularly given that we can carry the virus yet be symptom-free for several days (and thus unknowingly transmit it to others). Unless you live in an area under full lockdown, where leaving home is banned, you’ll need to make your own decisions about how much in-person contact you feel comfortable with. Just remember: We keep ourselves virus-free not just to protect ourselves, but to slow the spread of the virus among everyone, especially those most vulnerable who might require intensive medical care.Finally, there’s no doubt that physical isolation does reduce social contact, even with all of the steps we can take to stay connected socially. I’m a rock climber, but the climbing gyms are closed: That’s two evenings a week I am not spending with friends catching up and sharing a fun physical activity. Maybe you do yoga, or dance, or belong to a book club. Some of these things we can do online (online yoga classes are easy to find, and book clubs can meet via video conferencing). But the reality is that we also need to take steps to manage the anxiety that can accompany a drop in social contact. This is a great time to develop or increase physical exercise, whether it’s outside, or an in-home yoga practice you do daily. It’s a perfect time to begin or increase a mindfulness practice, to lower stress and find greater calm—and there are lots of online resources and smartphone apps that offer guided meditations and online meditation communities (I use Insight Timer).One last thought: There’s also value in adopting a perspective of gratitude, despite the hardships created by physical distancing. In refugee communities around the world, in homeless shelters, overcrowded orphanages, and detention centers for asylum-seekers, physical distancing is not even an option. The horrific conditions of everyday life—overcrowded and unsafe housing, poor sanitation, and a lack of even basic medical care—leave people in many places terribly vulnerable to COVID-19 (as well as a host of other diseases). Have a look at this article on the Moira refugee camp in Greece: “Lesbos coronavirus case sparks fear for refugee camp.” Looking at the images of the impoverished and vulnerable community in that camp, we can perhaps take comfort in knowing that our isolation will be temporary, and that we have access to ways of staying socially connected that many people do not. We just need to reach out and make sure we preserve the ties that bind us together.Kenneth E. Miller is a psychologist and writer and currently a researcher at War Child Holland, where he develops and evaluates mental health interventions for conflict-affected children and families. He is the author of War Torn: Stories of Courage, Love, and Resilience (Larson Publications, 2016) and has worked with refugees and other war-affected populations since 1991, as a researcher, clinician, consultant, and filmmaker.\n\tThe newest and most popular articles delivered right to your inbox!\nThis post was originally published on Psychology Today, and is reprinted with permission. "
  },
  {
    "imageUrl": "http://static.nautil.us/16905_82ce5ed14ce2278cbf483067765be069.jpg",
    "title": "Electrons Don’t Think",
    "description": "Posted by Sabine Hossenfelder on February 27, 2020  I recently discovered panpsychism. That’s the idea that all matter—animate or inanimate—is conscious, we just happen to be somewhat more conscious than carrots. Panpsychism is the…",
    "category": "Matter",
    "content": "I recently discovered panpsychism. That’s the idea that all matter—animate or inanimate—is conscious, we just happen to be somewhat more conscious than carrots. Panpsychism is the modern elan vital. When I say I “discovered” panpsychism, I mean I discovered there’s a bunch of philosophers who produce pamphlets about it. How do these philosophers address the conflict with evidence? Simple: They don’t. Now, look, I know that physicists have a reputation of being narrow-minded. But the reason we have this reputation is that we tried the crazy shit long ago and just found it doesn’t work. You call it “narrow-minded,” we call it “science.” We have moved on. Can elementary particles be conscious? No, they can’t. It’s in conflict with evidence. Here’s why.We know 25 elementary particles. These are collected in the standard model of particle physics. The predictions of the standard model agree with experiment to best precision. The particles in the standard model are classified by their properties, which are collectively called “quantum numbers.” The electron, for example, has an electric charge of -1 and it can have a spin of +1/2 or -1/2. There are a few other quantum numbers with complicated names, such as the weak hypercharge, but really it’s not so important. Point is, there are handful of those quantum numbers and they uniquely identify an elementary particle. If you calculate how many particles of a certain type are produced in a particle collision, the result depends on how many variants of the produced particle exist. In particular, it depends on the different values the quantum numbers can take. Since the particles have quantum properties, anything that can happen will happen. If a particle exists in many variants, you’ll produce them all—regardless of whether or not you can distinguish them. The result is that you see more of them than the standard model predicts.Now, if you want a particle to be conscious, your minimum expectation should be that the particle can change. It’s hard to have an inner life with only one thought. But if electrons could have thoughts, we’d long have seen this in particle collisions because it would change the number of particles produced in collisions.In other words, electrons aren’t conscious, and neither are any other particles. It’s incompatible with data. As I explain in my book, there are ways to modify the standard model that do not run into conflict with experiment. One of them is to make new particles so massive that so far we have not managed to produce them in particle collisions, but this doesn’t help you here. Another way is to make them interact so weakly that we haven’t been able to detect them. This too doesn’t help here. The third way is to assume that the existing particles are composed of more fundamental constituents, that are, however, so strongly bound together that we have not yet been able to tear them apart. With the third option it is indeed possible to add internal states to elementary particles. But if your goal is to give consciousness to those particles so that we can inherit it from them, strongly bound composites do not help you. They do not help you exactly because you have hidden this consciousness so that it needs a lot of energy to access. This then means, of course, that you cannot use it at lower energies, like the ones typical for soft and wet thinking apparatuses like human brains.Summary: If a philosopher starts speaking about elementary particles, run.Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox!\nThis post was originally published on BackRe(Action), Hossenfelder’s blog, and is reprinted with permission. "
  },
  {
    "imageUrl": "http://static.nautil.us/16785_5f1db7a13730fea2764ea1c0a3de2939.png",
    "title": "The Brain Cells That Guide Animals",
    "description": "Posted by Adithya Rajagopalan on January 24, 2020  It may seem absurd to compare a tiny fruit fly’s brain to that of a majestic elephant. Yet it is the dream of many neuroscientists to find deep rules that very different brains share.…",
    "category": "Biology",
    "content": "It may seem absurd to compare a tiny fruit fly’s brain to that of a majestic elephant. Yet it is the dream of many neuroscientists to find deep rules that very different brains share. As Gilles Laurent, a neuroscientist at the Max Planck Institute for Brain Research in Frankfurt, Germany, who has studied a variety of animals, from locusts to turtles, has said, “Neural responses can be described by the same mathematical operation…in completely different systems.” Vivek Jayaraman, a researcher at the Howard Hughes Medical Institute’s Janelia Research Campus, and a former student of Laurent’s, believes that neuroscientists are on the verge of identifying some of these deep neural rules. Grasping them would advance another neuroscientific dream: to be able to predict animal behavior as easily as Newton could predict the behavior of a moving object. Jayaraman and a small number of researchers studying the brain’s G.P.S. have, in fact, already experienced the thrill of discovering one such rule. It governs something essential—the ability of an animal to keep track of where it’s headed. What’s more, recent experiments on flies hooked up to virtual-reality environments—one from Jayaraman’s group and another from Rachel Wilson (a former postdoc of Laurent’s) and colleagues at Harvard—show how a fruit fly’s visual cues ensure the stability of its heading. The findings offer insight into how mammals, like us, might build maps of their world.When you’re moving freely, certain neurons are only active when you’re facing a particular way. It does not matter where you are—as long as you keep looking ahead, in that direction, the cell fires. These so-called head-direction cells were first identified in a region of the rat brain very close, and intimately linked to, the hippocampus, the home of place cells, and the basis for your map of the world. There are a large number of these cells, but any one cell is only active in a certain small region of the environment. They act like a map, telling you where you are. Such a map isn’t very useful without knowing where you’re headed. That’s why the brain also creates a mental compass, using head-direction, or compass, cells.Scientists can predict whether a fruit fly thinks it is turning right or left.It is striking how closely these two systems work together. If you were to rotate the visual cues that a rat uses to define its heading, then the head-direction cells would remap themselves to this shifted world. At the same time, the place cells of the hippocampus would also rotate the mental map. These two groups of cells provide complementary information that, if correctly put together, are enough to help the rat navigate anywhere in its environment—even in the dark. Head-direction cells, in other words, don’t simply respond to visual cues. They maintain their activity even when a rat can’t see its surroundings (or when you cover your eyes). How does the brain set up this stable compass-like representation? More perplexingly, how does it maintain this stable heading without visual cues?Experimental evidence in rats and mice suggests that a doughnut-like structure of head-direction cells keeps the brain’s compass reliable. A 1995 paper, “A Model of the Neural Basis of the Rat’s Sense of Direction,” first proposed this doughnut or “ring-attractor” picture. In it, William Skaggs and his colleagues from the University of Arizona hypothesized that the head-direction cells—which did not seem to have any visible characteristic group structure—connect together to form an imaginary ring that creates a 360-degree map of two-dimensional space. “The aim of this effort is to develop the simplest possible architecture consistent with the available data,” the researchers wrote. “The reality is sure to be more complicated than this model.”Each cell, a point on the ring, refers to a particular heading. Near-by cells on this ring refer to adjacent and similar headings, and activate each other, while cells that are far apart, and refer to opposing or near-opposing headings, inactivate each other. The researchers suggested a rule for how information from the brain’s visual areas enters the ring of head-direction cells, contributing to their characteristic activity pattern. They also incorporated the vestibular system of the brain, which detects head-turns, to allow the ring to maintain its activity in the dark. This model of inputs allowed for only one small group of nearby head-direction cells to be active at any given time. This one “bump” of activity corresponded with the rat’s heading. If the rat, or its world, turned (by, say, the rat getting flipped over) then the connections within the ring of cells and the changing visual input information would cause the bump to also turn accordingly.In a win for Skaggs and his colleagues, researchers have identified some of the cells that appear to operate according to their mathematical model. Still, it has one major hole: There is little understanding of how the brain converts visual information into a stable sense for heading. It is, at least for now, too hard to map how the millions of cells and complex connections that allow rats and mice to see connects to this ring.This is where Jayaraman comes in. The fruit fly, it turns out, also has a region of its brain dedicated to maintaining a sense of direction. Jayaraman characterized it with his colleagues less than a decade ago. Incredibly, it is a doughnut-shaped ring exactly like the structure of the theoretical ring-attractor network. Any given cell in this ring of cells is only active when the fly is facing a particular way—exactly like head-direction cells in rodents (and, presumably, in us). This discovery provides a terrific opportunity, not only to test a model that scientists developed for one animal using a completely different animal, but also to identify a common rule that a striking variety of brains use.“Our paper provides evidence to support the model put forward by Skaggs et al.,” Yvette Fisher, the lead author, told me. “Particularly the requirement by the model that visual inputs to the compass neurons are modifiable.” The two studies, in other words, identified a rule, or mechanism, that dictates how the strength of connections between neurons in the ring and visual inputs change. The ring’s visual inputs inhibit head-direction cell activity—the head-direction cell with the weakest connection to the visual input neurons will be active. For Fisher, who is a neurobiologist, being steeped in an evolutionary mindset might have shaped her expectations. “It isn’t surprising that common principles exist when the neural circuit required is under similar constraints,” Fisher said. “Navigation has a number of broadly similar requirements for all animals.” Given these similarities, the rules the researchers identified could inspire others to make sense of this process in mammals. General principles, like the ring-attractor model for calculating heading, provide insight into how all animals behave. Scientists can use them to predict some animal behavior, like whether a fruit fly thinks it is turning right or left. What’s more, scientists now have a baseline model from which they can see how specific animals deviate from one another. For example, while a rat, a terrestrial animal, might make do with a two-dimensional model of heading, birds and bats and some insects fly. While much of the work on flies has focused on walking flies, recent research on bats has found a three-dimensional map. It is still an open question whether the same neural algorithm expands from two to three dimensions in these animals. Potentially even more interesting is how some animals navigate over long distances: Desert ants are known to be able to locate their nest, a three-dimensional tunnel structure, from miles away, and many migrating birds can return to the same location year after year. Fisher believes that the answers will come from characterizing a diversity of brains, and comparing the algorithms each creature evolved.Adithya Rajagopalan is a third-year graduate student in the department of neuroscience at Johns Hopkins University & Janelia Research Campus. Follow him on Twitter @adi_e_r.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16880_3a6cd33291178d268ef37b305c3f8c0e.jpg",
    "title": "Should Scientists Use the Phrase “Quantum Supremacy”?",
    "description": "Posted by Brian  Gallagher on February 25, 2020  Forget for a moment that you know the meaning of “quantum supremacy,” the idea of a quantum computer outdoing its conventional counterpart. What does the phrase instantly bring to mind?…",
    "category": "Culture",
    "content": "Forget for a moment that you know the meaning of “quantum supremacy,” the idea of a quantum computer outdoing its conventional counterpart. What does the phrase instantly bring to mind? Perhaps the idea that the quantum world, with its electrons, neutrons, and quarks, is, somehow, better than ours—more dazzling and awe-inspiring. Or perhaps it’s the claim that exploring quantum phenomena is more worthwhile, in a socioeconomic or in a personal, life-fulfilling sense. Or perhaps it’s the suspicion that quantum beings exist, and exert absolute control over everything we do. The point is, with “quantum supremacy,” you can imagine many ways to erect a quantum-first hierarchy. Remarkably, it’s precisely that sort of thinking, prompted by the word “supremacy,” that some scientists are wary of popularizing.A few months ago, after Google made headlines with its announcement that it had achieved quantum supremacy, the scientific journal Nature received and published a letter with the headline, “Supremacy is for racists—use ‘quantum advantage.’” The authors of the letter—a quantum computing researcher, a planetary scientist, and a former Nature editor—argued that, in a world riven by sexism and racism, it would be irresponsible for scientists to allow an “inherently violent” term like “supremacy” to permeate popular culture, which could risk “sustaining divisions in race, gender and class” and ward off some from pursuing computer science. This is because “supremacy” has “overtones of neocolonialism and racism through its association with ‘white supremacy,’” they wrote. “Instead, quantum computing should be an open arena and an inspiration for a new generation of scientists.”Quantum ascendancy? (Sounds like “we’re a UFO cult.”) Quantum inimitability? (“Who can pronounce that?”)The letter has a number of signatories, yet some found the message risible. “This sounds like something from The Onion,” Harvard psycholinguist Steven Pinker said on Twitter. The call to stigmatize “quantum supremacy” reminded him of a similar episode of rebranding, which saw the acronym for the Neural Information Processing Society (NIPS) change to NeurIPS. The sort of criticism “supremacy” got in the Nature letter instanced “a familiar linguistic phenomenon,” Pinker said, “a lexical version of Gresham’s Law: bad meanings drive good ones out of circulation.” “Booty” and “prick,” among others, came to Pinker’s mind as examples. He wants scientists to resist banning words in this manner. “It dumbs down understanding of language: word meanings are conventions, not spells with magical powers,” he said, “and all words have multiple senses, which are distinguished in context.” (After Pinker’s remarks on Twitter, Nature issued a correction, changing the letter’s headline to “Instead of ‘supremacy’ use ‘quantum advantage.’”)Julie Sedivy, a psycholinguist who has taught at Brown University, struck a less dismissive tone when I asked her about “quantum supremacy.” She said the Nature letter authors “have a point, at least partially.” It has to do with the contingent way in which each of us relates to particular words.“This letter points to an interesting aspect of human language: Namely, that the meanings, connotations, and associations we attach to words are heavily dependent on our own experiences with them; to the extent that people’s experiences diverge, we are limited in our ability to evoke those same meanings, associations, etc. in the minds of others,” she told me in an email. “Thus, imperfect convergence in our experiences with words imposes an inherent limitation to human communication. The authors argue that it’s irresponsible not to consider the experiences that other people are likely to have with the word ‘supremacy.’ In principle, it doesn’t seem unreasonable to suggest that if readers’ experiences with a word are likely to set off negative, unintended resonances, that is a sound argument for finding an alternative way to express the same idea.”What about in practice? Sedivy said she would need answers to certain questions before deciding to do away with “quantum supremacy.” Who is the intended audience? What are the likely experiences of that audience with the term? “In this case,” she said, “the argument is weakened by the fact that the term appears in a technical scientific article. Is the term ‘quantum supremacy’ in common use among the targeted readership? If so, this will skew the representations that the audience is likely to have of the term away from the negative associations that the authors worry about.” Also: What is the context in which the term is used? “We know from psycholinguistic studies that context strongly influences the particular representations of a word that are activated—not all memories and associations are equally active, as some are promoted and others are suppressed by the specific context of use,” Sedivy said. “Again, the scientific context in which this term is used will weaken activation of the more political associations of the word.”Scott Aaronson, a theoretical computer scientist at the University of Texas at Austin, admitted that, when the term was coined by theoretical physicist John Preskill in 2012, he thought it was awkward but wasn’t bothered by it gaining wide currency in the scientific community. On his blog, “Shtetl-Optimized,” Aaronson wrote:The thinking was: even as white supremacy was making its horrific resurgence in the US and around the world, here we were, physicists and computer scientists and mathematicians of varied skin tones and accents and genders, coming together to pursue a different and better kind of supremacy—a small reflection of the better world that we still believed was possible. You might say that we were reclaiming the word “supremacy”—which, after all, just means a state of being supreme—for something non-sexist and non-racist and inclusive and good.In the world of 2019, alas, perhaps it was inevitable that people wouldn’t leave things there.When Leonie Mueck, the former Nature editor who co-authored the letter, asked Aaronson, before it published, what he thought of “quantum advantage,” he told her that as the father of a “math-loving 6-year-old girl,” he empathized with her feelings on “supremacy.” But he also couldn’t come up with another way to express the significance Preskill wanted to get across. Preskill meant “quantum supremacy” to “refer to a momentous event that seemed likely to arrive in a matter of years,” the time when quantum computers would first outpace the fastest classical supercomputers, Aaronson wrote. “And … ‘the historic milestone of quantum advantage’? It just doesn’t sound right. Plus, as many others pointed out, the term ‘quantum advantage’ is already used to refer to … well, quantum advantages, which might fall well short of supremacy.”On his blog, Aaronson entertained several replacements. Quantum ascendancy? (Sounds like “we’re a UFO cult.”) Quantum inimitability? (“Who can pronounce that?”) What about quantum superiority, dominance, or hegemony? These don’t avoid supremacy’s problems. “What word does the English language provide to describe one thing decisively beating or being better than a different thing for some purpose, and which doesn’t have unsavory connotations?” Aaronson wondered. We may be stuck with those connotations—for now. Perhaps someone will coin a creative alternative. Aaronson welcomes suggestions. “I currently regard it as an open problem.”Sedivy underscored the difficulty. “Representations of words are essentially containers for memories of all of the uses of that word that we encounter, with more recent uses having particular weight,” she said. For someone outside quantum computing, who has encountered “supremacy” mainly in the context of political or current affairs, its meaning can’t help but be loaded. That’s a fact scientists can’t ignore. Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16858_fa12ab68091d74718e2b3fabfcb8e2b1.jpg",
    "title": "Why We Love to Be Grossed Out",
    "description": "Posted by Marco Altamirano on February 16, 2020  Nina Strohminger, perhaps not unlike many fans of raunchy comedies and horror flicks, is drawn to disgust. The University of Pennsylvania psychologist has written extensively on the feeling…",
    "category": "Biology",
    "content": "Nina Strohminger, perhaps not unlike many fans of raunchy comedies and horror flicks, is drawn to disgust. The University of Pennsylvania psychologist has written extensively on the feeling of being grossed out, and where it comes from. The dominant idea, developed by Paul Rozin and April Fallon, is that disgust evolved adaptively from an oral revulsion to biologically harmful substances, like rotten food and bodily waste. The emotion subsequently crept into the social arena, they claimed, as we became revolted by abnormal and licentious behavior. Moral repugnance arose as a result, which retains little if any connection to the biological origins of disgust. It’s “like a parfait,” Strohminger says. “It started out as one thing and things kept getting added on as it developed.” Building on Darwin’s 1872 thesis that disgust signified “something offensive to taste,” Rozin and Fallon’s theory made disgust one of the more popular human emotions to study. Strohminger finds their story intuitive, although perhaps oversimplified. In particular, she worries that the cumulative view of disgust masks a more evolutionarily complex story. Strohminger prefers to approach disgust not as a straightforward extension of the immune system’s aversion to harmful substances, but as “a psychological nebula, lacking definite boundaries, discrete internal structure, or a single center of gravity.”Disgust is inherently ambivalent—it at once revolts and attracts us. This reflects, for Strohminger, the larger evolutionary ambivalence that disgust stems from, since we “must balance the need for nutrition against the peril of toxic comestibles, the need to socialize against the threat of communicable disease.” In short, disgust may not derive from a simple aversion to harmful substances but from a tension between the desire to explore and consume new things and the dangers of doing so. There’s more to the story of disgust than being biologically revolted by harmful substances.Josh Rottman, a developmental psychologist who specializes in disgust, claims that the emotion is better understood by examining the social forces that inform it. If disgust were an adaptive behavioral mechanism for avoiding biologically harmful substances, Rottman argues, children would exhibit disgust in their most vulnerable years, when their immune system is still developing. But infants and toddlers are willing to put just about anything in their mouths, even imitation feces, and only begin to show signs of disgust around ages five to seven, long past their vulnerable weaning period. This could, perhaps, be explained by the fact that children’s immune systems benefit from their exposure to a variety of substances. However, most of the helpful bacteria and immune-building germs that children encounter come not from steaming mounds of dung and worm-eaten corpses, the hallmark triggers of disgust, but rather from invisible air and water-borne pathogens.Rozin attributes the delayed onset of disgust to the omnivore’s dilemma, the fact that we must balance our ability to consume a wide variety of foodstuffs with the potentially steep consequences of poisoning ourselves. If disgust had a single adaptive origin, one might expect certain substances to universally elicit disgust. But universally disgusting objects don’t seem to exist.While some Westerners are disgusted by insect larvae on a plate, Easterners are similarly revolted by the idea of lifting curdles from soured milk, adding salt, and giving the resulting product a quaint name like “cottage cheese.” The Hazda of Tanzania often consume putrid meat scavenged from lion kills. Shamans of the Koryak tribe in Siberia consume mushrooms, urinate in a pot, and pass it around for the group to sip. And the Mundari tribe in South Sudan not only showers in cow-urine but also cover their bodies with ash from dung fires to ward off infection. Given this ethnographic variation on disgust, Rottman argues that what elicits the emotion is largely socially informed. There’s even considerable variation for disgust within a lifetime, as we can develop a taste for bloody, organ-spilling horror films as well as revulsions to odors of liquors we formerly enjoyed but once overindulged in.Since disgust arrives in middle childhood (ages five to nine), right around the time when social biases are formed, Rottman says, “It seems to be more of a social avoidance kind of emotion. It helps us avoid people, not only people who are sick, but also people exhibiting non-normative behaviors.” We’re not only disgusted by people who are boiled and plagued, who pose a threat to our physical health, but also by people who seem socially ill, who pose a threat to our customs and morals.Moral repugnance is perhaps the most complicated iteration of disgust. Daniel Kelly, a philosopher at Purdue University who wrote a book on the subject, considers disgust to have more cognitive than sensory signatures. “Distaste is sensory, a revulsion to something that tastes harmful,” he says. “But disgust isn’t merely sensory—charred human flesh might be delicious, but that’s not why we don’t eat it.” Kelly argues that the emotion shouldn’t have any authority in moral evaluation. “There’s too much cultural variation, and it’s too easily triggered by stuff that’s morally irrelevant” to have a place in moral judgments (after all, some people consider body hair, tight clothing, and even certain colors disgusting). Consequently, Kelly finds there is no “deep wisdom” in disgust, and the use of moral repugnance in decision or policy making is irresponsible and even dangerous. “Disgust tends to stigmatize and dehumanize its object, including people,” Kelly says. “It makes it easy to treat people horribly.”Plato was perhaps the earliest to think seriously about disgust. Leontius, a character in The Republic, is torn by an embarrassing desire to feast his eyes on corpses piled along the perimeter of Athens. Eventually overcome by his ghastly fascination, Leontius runs toward the corpses and wails, “Look, you damned wretches, take your fill of the fair sight!” Plato presents this as an instance of the tormenting conflict in the soul between reason and unruly, often objectionable desires. While it may seem strange to take disgust as a symptom of a cursed soul, Plato’s story does highlight the chief difficulty about disgust, namely, that we’re attracted to it (and sometimes even disgusted with ourselves because we’re attracted to disgusting things). Rozin used the term “benign masochism” to denote how we enjoy laughing or crying at movies when there’s little risk involved. “It’s one thing to enjoy toilet humor,” Strohminger writes. “It’s another to be inside the toilet.”What perplexes Strohminger is our attraction to aversion. “We need to account for the fact that we chase after disgust,” she said. Our attraction to disgust is hardly modern. The grotesque fascinated painters from the Renaissance to Goya, with his visages of Saturn, and Francis Bacon, with his distorted portraits. Even earlier, the ancient Greeks told gut-wrenching stories about how Atreus killed and cooked his brother Thyestes’ children and fed them to their unwitting father. Perhaps disgust is cathartic to enjoy when there’s no real threat of contamination, just like it’s cathartic to feel the rush of heart-pounding thrillers or tragedies. Or perhaps Plato was right to say that disgust was contrary to reason, something that we just can’t explain. As a matter of taste, disgust is inherently subjective. There’s no real reason why one person might crave bacon-flavored ice cream with pickles while the thought of that might make another retch. And that might be why it’s hard to explain why we chase after disgust, too. In the end, we might have just developed a taste for it.Marco Altamirano is a writer based in New Orleans and the author of Time, Technology, and Environment: An Essay on the Philosophy of Nature. Follow him on Twitter @marcosien.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16834_db11330cfb0a32a0e9b8e8b6c4bdc711.jpg",
    "title": "This Psychological Concept Could Be Shaping the Presidential Election",
    "description": "Posted by Erika Weisz on February 10, 2020  Not too long ago, I briefly met Elizabeth Warren in a restaurant in Cambridge, near Harvard, where I’m now a postdoc in psychology. My dad and I saw the Massachusetts senator, a 2020…",
    "category": "Ideas",
    "content": "Not too long ago, I briefly met Elizabeth Warren in a restaurant in Cambridge, near Harvard, where I’m now a postdoc in psychology. My dad and I saw the Massachusetts senator, a 2020 presidential candidate, walking in as we were walking out. “Give ’em hell,” my dad told the senator, harkening back to Harry Truman’s 1948 presidential campaign. She laughed. “That’s what I do!”Last summer, in a New York Times article about Warren, a voter stated, “I love her enthusiasm. She’s smart, she’s very smart. I think she would make an amazing president,” before adding, “I’m worried about whether she can win.”1 The voter’s sentiment is reflected in a 2019 poll in which 74 percent of Democrats said they would be comfortable with a female president, yet only 33 percent of them thought their neighbors felt the same way.2 Pluralistic ignorance stymied friendship among black and white students.Last week in the Iowa caucus primary, Warren placed third behind Pete Buttigieg, former mayor of South Bend, Indiana, and Senator Bernie Sanders of Vermont. Could Warren’s political fate in 2020 turn on voters who think she would make a great president choosing another candidate because they think that’s what their neighbors will do? I’m inclined to say yes because of a social psychological concept called pluralistic ignorance.Pluralistic ignorance is a discrepancy between one’s privately held beliefs and public behavior. It occurs when people assume that the identical actions of themselves and others reflect different underlying states. The term has been in circulation for nearly 80 years, though more recent experiments have made it a focal point of social psychology. In the 1990s, psychologists Debra Prentice and Dale Miller conducted a series of studies at Princeton University. They aimed to address a major campus issue: alcohol consumption. At the time, Princeton was notorious for its practices around alcohol. Princeton reunions were second only to the Indy 500 for most alcohol consumed in a single event. Strong pro-alcohol norms were deeply ingrained in campus life. But Prentice and Miller wondered whether the story was more complicated. They suspected that students secretly harbored misgivings about drinking, but were going along with what seemed popular among their peers.To test this idea, Prentice and Miller administered a two-question survey to students. On the first question, students indicated how comfortable they themselves felt with alcohol, using a scale from 1 (not at all comfortable) to 11 (very comfortable). On the second question, they indicated how comfortable the average Princeton student was with alcohol using the same scale. Findings confirmed Prentice and Miller’s intuitions: Princeton undergraduates reported that they were far less comfortable with alcohol than their peers. The average rating for one’s own comfort with alcohol was a 5.3. The rating for the average student’s comfort with alcohol was just above 7. In other words, perceived comfort with alcohol was much higher than actual comfort with alcohol. At Princeton, campus drinking norms were perpetuated by pluralistic ignorance.3Over a decade later, psychologists Nicole Shelton and Jennifer Richeson examined whether pluralistic ignorance stymied friendship among black and white college students. They designed an experiment that asked students to imagine approaching a group of people from a different race in a dining hall. Results showed both groups of students wanted to have more contact with each other, but erroneously thought the other group didn’t want to have contact with them. In subsequent studies, Shelton and Richeson documented the real consequences of this miscalculation among Princeton students. They found that people who endorsed divergent explanations for their own actions and the identical actions of racial outgroup members had less contact with people of different races in their daily lives. Pluralistic ignorance deterred Princeton students of different races from interacting with each other.4 Of course, pluralistic ignorance doesn’t only exist on the Princeton campus. In the workplace, pluralistic ignorance fosters reluctance among men to take new parent leave. In one study, male employees overestimated the extent to which other men held negative attitudes toward paternity leave. This miscalculation affected their own willingness to take leave; men who thought they felt more positively about leave than their peers were disinclined to take it, even though they wanted to.5Can dispelling pluralistic ignorance convince voters to trust their gut?Similarly, pluralistic ignorance appears to stifle reporting of sexual harassment. A 2009 study presented participants with packets of sexist jokes and asked them to indicate how comfortable they were with the jokes relative to their peers. Participants who thought their peers felt more comfortable with the jokes than they did were less likely to report feeling offended.6 Scientists have also shown that pluralistic ignorance can curb efforts to address climate change. A recent set of studies from Penn State suggests that people concerned about climate change—the majority of participants in the experiments—were not willing to discuss it when they thought others did not share their concern, creating a “climate of silence” around the pressing issue.7This misperception of peers’ thoughts and desires—in this case, misperception of what other voters think and will do—is what Warren’s campaign appears to be facing. A host of polls and analyses have shown that voters commonly refrain from voting for women because they think other voters don’t believe they can win. A summer poll from Avalanche Strategy presented Democratic voters two questions. First it asked them for whom they would vote if the primary election were held today. Former vice president Joe Biden took first place, Sanders was second, and Warren was third. Then it asked them whom they would choose if they had “a magic wand and can make any of the candidates president—they don’t have to beat anyone or win the election.” On this question, Warren was first.8Nate Silver, editor of FiveThirtyEight, reflected on these data in a blog post. “Being a woman was the biggest barrier to electability, based on Avalanche’s analysis of the results, and women were more likely to cite gender as a factor than men,” Silver wrote. “So there are a lot of women who might not vote for a woman because they’re worried that other voters won’t vote for her. But if everyone just voted for who they actually wanted to be president, the woman would win!”9A key factor of voters’ misperceptions is the concept of “electability.” They think women candidates are less electable than men. But there is a problem with this concept: electability is a noisy construct, especially during a presidential election. Analysts have repeatedly pointed out that “unelectable” candidates have included Ronald Reagan, Bill Clinton, Barack Obama, and Donald Trump.10 What happens when voters subscribe to the electability strategy?Regina Bateson—a political scientist at the University of Ottawa who left her tenure-track job at MIT to run for congressional office in 2017—tackled this question in a recent experiment. She conducted a study in which she asked nearly 2,000 Americans about the electability of hypothetical political candidates. These mock candidates were identical on every dimension except for gender and race. Bateson found that participants most often rated a candidate as “very electable” when the candidate was a white man, but less frequently when the candidate was a woman or person of color. In this study, white men were seen as significantly more electable than white women and women of color.11But demographic advantages for white males may not be as robust as people think. A study of 2018 elections by the Reflective Democracy Campaign (a project of the Women Donors Network examining demographics in American politics) found that in 2018, women and people of color won elections at the same rates as white men. This study examined some 34,000 candidates at the federal, state, and county levels.12“Once they’re on the general election ballot, candidates from all demographic groups won their elections at essentially the same rates. Overall, female candidates, regardless of race, are actually slightly more successful than male candidates,” Brenda Choresi Carter, director of the Reflective Democracy Campaign, told me. These findings are consistent with previous studies, suggesting that women and people of color win at the same rates as (and sometimes higher rates than) white men, but white men are seen as more electable.13, 14Can being made aware of faulty perceptions spur a change in behavior? Prentice, who conducted the seminal studies in pluralistic ignorance, wanted to find out. In particular, she wondered what would happen when Princeton students were made aware of how their peers actually felt about alcohol.In 1998, Prentice and her colleague Christine Schroeder conducted an experiment in which incoming Princeton freshmen were randomly assigned to participate in one of two alcohol education programs. One program (the control program) was designed to mimic traditional alcohol education for new college students. It facilitated discussions among freshmen about how to make responsible personal decisions in a drinking situation. The other program was an intervention directly addressing pluralistic ignorance. Specifically, Schroeder and Prentice showed this group their data from the previous experiment. These students saw how far off their undergraduate peers were in estimating people’s actual comfort with alcohol. Approximately five months later, students in the pluralistic ignorance program reported drinking less than students in the control program. Dispelling their pluralistic ignorance licensed them to trust their gut, which motivated them to make safer choices around alcohol.15Can dispelling pluralistic ignorance around electability convince people to trust their gut and vote for the candidate they want, not the candidate they think others want? I don’t have the answers. But I do know that evidence from research on pluralistic ignorance demonstrates that we’re often wrong in our assumptions about what others are thinking and feeling. That knowledge certainly has the power to influence what we do in the voting booth.Erika Weisz is a postdoctoral researcher in Psychology at Harvard University. She received her Ph.D. in Psychology from Stanford University in 2018. Her research explores how to use social psychological techniques to encourage people to empathize with one another.References1. https://www.nytimes.com/2019/08/15/us/politics/elizabeth-warren-2020-campaign.html2. https://www.ipsos.com/sites/default/files/ct/news/documents/2019-06/daily-beast-gender-topline-2019-06-17-v2.pdf3. Prentice, D. A., & Miller, D. T. (1993). Pluralistic ignorance and alcohol use on campus: some consequences of misperceiving the social norm. Journal of Personality and Social Psychology, 64 (2), 243–256.4. Shelton, J. N., & Richeson, J. A. (2005). Intergroup contact and pluralistic ignorance. Journal of Personality and Social Psychology, 88 (1), 91–107. https://doi.org/10.1037/0022-3514.88.1.915. Miyajima, T., & Yamaguchi, H. (2017). I want to but I won’t: Pluralistic ignorance inhibits intentions to take paternity leave in Japan. Frontiers in Psychology, 8 (SEP), 1–12. 6. Halbesleben, J. R. B. (2009). The role of pluralistic ignorance in the reporting of sexual harassment. Basic and Applied Social Psychology, 31(3), 210–217. 7. Geiger, N., & Swim, J. K. (2016). Climate of silence: Pluralistic ignorance as a barrier to climate change discussion. Journal of Environmental Psychology, 47, 79–90. 8. https://thehill.com/homenews/campaign/449315-poll-dems-prefer-warren-when-not-considering-electability9. https://fivethirtyeight.com/features/bulletpoint-is-electability-a-self-fulfilling-prophecy/10.  https://psmag.com/ideas/theres-no-good-way-to-determine-electability-other-than-holding-elections11. Bateson, R. Strategic Discrimination. Manuscript under review.12. Reflective Democracy Campaign. 2019. “The Electability Myth: The Shifting Demographics of Political Power in America.” June. https://wholeads.us/wp-content/uploads/2019/08/The-Electability-Myth-_-The-Shifting-Demographics-of-Political-Power-In-America-8-1-19.pdf 13. Lawless, Jennifer L. and Kathryn Pearson. (2008). The Primary Reason for Women’s Underrepresentation? Reevaluating the Conventional Wisdom. Journal of Politics, 70 (1): 67-82.14. Anastasopoulos, L. (2016). Estimating the gender penalty in House of Representatives elections using a regression discontinuity design. Electoral Studies 43: 150-157. 15. Schroeder, C.M. and Prentice, D.A. (1998). Exposing pluralistic ignorance to reduce alcohol use among college students. J. Appl. Soc. Psychol. 28, 2150–2180 "
  },
  {
    "imageUrl": "http://static.nautil.us/16945_eab2156d0c600ceb7e814018273aec86.jpg",
    "title": "Scientists Can Predict Your Job By Your Social-Media Personality",
    "description": "Posted by Alice Fleerackers on March 06, 2020  For many of us, finding the “right” career can feel like an impossible feat. When my little sister was in her last year of high school, she took a career aptitude test. Her top career?…",
    "category": "Culture",
    "content": "For many of us, finding the “right” career can feel like an impossible feat. When my little sister was in her last year of high school, she took a career aptitude test. Her top career? Chimney sweep. Luckily, things look to be working out for my sister. She’s now training to become an engineer, but who knows whether she will move on to something else: The average person today will hold about 12 jobs in their lifetime. At any given moment, one in three employees are underqualified for their current roles, while one in four are overqualified. Globally, the vast majority of people feel disengaged at work. Winding up in the wrong profession seems to be a surprisingly common experience.While career tests can help, they’re no magic bullet. Most, like the one my sister took, rely on self-report surveys, which are not only time consuming, but can also be faked. It’s also debatable whether the results from these tests are meaningful. The popular Myers-Briggs Type Indicator, for example, has been critiqued for offering a “ridiculously limited and simplified view of human personality.” A new paper offers an alternative to the imperfect career test: the Twitter feed. The researchers applied machine-learning approaches to Twitter data to offer what they describe as a “21st-century approach to matching one’s personality with congruent occupations.”  Marian-Andrei Rizoiu, a lecturer in computer science at the University of Technology in Sydney, and one of the study’s co-authors, said, “The study was pretty fascinating because it took this big data approach to personality and career.”Your job-personality fit can even affect your income.Rizoiu and his colleagues analyzed tweets from almost 130,000 Twitter users. By identifying patterns in users’ language, the researchers were able to capture what Rizoiu calls personality profiles—collections of traits and values that form who we are. He and his colleagues compared these profiles to the careers that users mentioned in their Twitter bios. The link was strong—so strong that the personality profiles could be used to “predict” professions with more than 70 percent accuracy. “That means,” Rizoiu explained, “that if you give me a random personality profile, without any other information, in three in four cases we can correctly guess their profession.”If you dive deep into the data, those errors—the one in four cases where the application gets it wrong—are often less problematic than you might think. According to Rizoiu, many of the errors occur when the application confuses synonymous professions, like school principal and superintendent, or jobs that involve similar skills, like data scientist and software engineer.The study could have exciting implications for future job seekers. “I think it can open up…possibilities,” said Peggy Kern, an associate professor at the University of Melbourne’s Center for Positive Psychology, and the lead author of the study. “My hope is that it would allow people to explore more and think about going beyond the typical way that we make [career] decisions.”To recommend occupations, Kern and her colleagues’ method relied on two aspects of user personalities: traits and values. Traits, she explained, are common characteristics like extraversion or neuroticism. Values, on the other hand, represent what we truly care about, like helping others, respecting tradition, or achieving success. Both are core aspects of our personality, and both, it turns out, are deeply linked to how we feel and perform in our careers.Past studies have connected personality to everything from hireability and career potential to job performance and professional success. Your job-personality fit can even affect your income. In a 2017 study of almost 8,500 employees, people whose personalities were well-suited to their professions were more likely to earn up to 10 percent more. The consequential significance of personality is becoming more widely recognized. Last year, the American Psychologist published a paper titled, “The Policy Relevance of Personality Traits.”Finding the right match could even impact your health. “If what I’m doing [at work] fits within my value system, it has a positive impact upon my wellbeing,” Kern said. By using data to make career choices that feel authentic and meaningful, we might all be better off—professionally, financially, even physically.Kern believes that a refined version of the method used in the study could be developed to help people identify career options that truly fit their traits and values, based on the digital traces that they leave through their online behaviors. Rather than pursuing whatever career their parents recommend, for example, people could use their own social media data to identify a variety of options that might be a better fit and more fulfilling. Using big data—like this study’s large collection of tweets—can be helpful for uncovering trends that would otherwise be difficult to identify. But the method also relies heavily on the status quo, for better or for worse. “I think the paper makes perfect use of the social media data we have available, where you have a massive data set of people’s personalities in different locations,” said Kiki Leutner, a business psychologist and data scientist at University College London, who was not involved in the study. “But, of course, you can only look at the current state of things. You can see where extroverted people are more likely to work, but it’s unclear whether that means that extroverted people are better in those roles. It doesn’t necessarily mean that people’s vocational choices are right.”It’s not clear whether there are meaningful differences in how diverse groups of Twitter users communicate on the platform—and whether that would affect the career options they’re recommended. But if demographics like race, gender, or age do play a role, then machine-learning approaches like the one Kern and her colleagues used could end up perpetuating existing biases in the workplace. “One of the biggest concerns I have is about how it could be misused,” she said. “I could very easily imagine some businesses taking it [as] a new way to screen for employees…I think that would be quite dangerous.”Machine learning is only as good as the information it learns from. So, if a particular minority group was underrepresented in the data that the team analyzed, the analysis could end up contributing to some real biased outcomes. Leutner agrees. “If you wanted to use it in an [employee] selection context, you would have to make sure that it’s very reliable,” she said. “You would really have to prove that having those traits is beneficial to performance in that role.” Until researchers know whether these digital personality profiles truly represent the general population, this technology should remain an exploratory tool—more informational than prescriptive.Still, the possibilities the research offers are enticing, especially as the modern workplace becomes increasingly automated. “Think about how many jobs that have always been there are disappearing,” said Kern. As more and more careers become redundant and workers are tasked with finding new ones, data-driven recommendations could be extremely helpful. “This could keep people learning on their career path, and that is going to be very important as we think about the constant change of jobs that is going to continue to happen throughout the 21st century.”Career choices could soon be driven less by happenstance and more by data. When I asked Kern about how she ended up in her own career, she laughed. “I never imagined I’d be living in Australia, an assistant professor in academia,” she said with a shrug. “And yet, it’s been a good fit for me. I love what I do, but I feel like I’ve stumbled into it.”Alice Fleerackers is a freelance writer and a doctoral student at Simon Fraser University, where she studies how controversial science is communicated in the digital sphere. Find her on Twitter @FleerackersA.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16862_3550284cdc2575eae68335f00870aaab.jpg",
    "title": "Color-Changing Material Unites the Math and Physics of Knots",
    "description": "Posted by Devin Powell on February 19, 2020  Reprinted with permission from Quanta Magazine’s Abstractions blog.One sunny day in the summer of 2019, Mathias Kolle, a professor at the Massachusetts Institute of Technology, took…",
    "category": "Numbers",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.One sunny day in the summer of 2019, Mathias Kolle, a professor at the Massachusetts Institute of Technology, took a couple of eminent colleagues out sailing. They talked about their research. They had some drinks. Then Kolle noticed something was off: A rowboat tied to his boat had come loose and was drifting toward the horizon. As he tacked across the water to retrieve the wayward vessel, he realized his mistake. In securing the rowboat, he must have tied the knot wrong. “I almost lost a boat because I got one knot wrong,” said Kolle, a mechanical engineer. “That was pretty embarrassing.”This slip-up aside, Kolle has become quite the knot wonk. In a paper in Science published in January 2020, he and his colleagues used a new way of visualizing the forces inside tangled fibers to revisit an ancient question: What makes some knots stronger than others?Scientists have a long-standing fascination with knots. More than 150 years ago, Lord Kelvin—working with fellow Scottish scholar Peter Guthrie Tait—proposed that the chemical elements could be represented by different knots. The theory didn’t pan out, but the diagrams they drew of different knots, and their attempts to classify them, jump-started the development of modern knot theory. In the 20th century, researchers built on this legacy by developing mathematical descriptions of knots that distinguish one from another. Often these descriptions employ topological properties: simple, countable characteristics that don’t depend on size or shape, such as how often strings in a knot cross.The mathematics of theoretical knots tied in theoretical strings inspired biologists to investigate how real DNA and proteins twist and tangle. Scientists have also developed theoretical models for knots at larger scales, like the hitches that bind ropes to poles. Some have put their models to the test, using titanium wire to determine how much force is needed to pull a knot tight, or using fishing line or strands of spaghetti to explore what parts of a knot tend to break.“It’s a creative art in my mind, being able to develop an experiment that will capture these properties,” said Ken Millett, a knot theory pioneer at the University of California, Santa Barbara. But these experiments all tend to have the same limitation—one that makes it difficult for researchers to truly understand how everyday knots operate, said Jörn Dunkel, a mathematician at MIT. “The problem is that you could not look inside the material,” Dunkel said. “A lot of things are hidden on the inside.”Kolle and his wandering rowboat would agree. But a few years ago, he received inspiration from an unexpected source: a vivid blue seed mailed to a colleague, in a matchbox, by a reporter in Mexico. Plucked from the colorfully named bastard hogberry, the fruit gets its hue from the arrangement of cells in light-bending patterns.Kolle adapted this optical trick to create plastic fibers that not only shine brightly in white light, but change color when stretched or bent. As their microscopic structures deform, the fibers turn yellow, green and other shades, revealing the stresses and strains within.Dunkel realized the stretchy fibers could reveal what was hidden within knots, so he and the study’s co-authors set to work constructing new simulations. They modeled not only simple knots in a single rope—the typical subjects of knot theory—but also bends, a rarely studied knot that holds two separate ropes together. Once they estimated the stresses inside several bends and calculated how much force would undo them, the team set about testing their simulations, comparing them to the hues that emerged in knotted-up fibers. After some fine-tuning, the models held up as strongly as the knots they depicted, accurately gauging the relative strengths of different bends.“My favorite knot was the Zeppelin, which had a nice symmetry and was one of the best we found,” said Vishal Patil, a co-author and an MIT graduate student. The Zeppelin knot, formed from two loops laid on top of each other, gets its strength from countable topological properties, said Patil: lots of rope crossings that tend to twist each other in opposite directions, like a towel being wrung out, and circulate in opposite directions to create friction.So far, the research has mathematically confirmed the strengths of time-tested knots developed over eons of human experimentation. But Dunkel’s team hopes the findings will play a role in designing new ways to tie, loop, twist and otherwise form tangles from rope, adding a new predictive dimension to knot theory.“The paper is a very interesting blend of experimental work and qualitative theoretical work,” said Louis Kauffman, a topologist working in knot theory at the University of Illinois, Chicago. He cautioned, however, that the more complicated the knot, the less accurate the predictions become. “The results are best for small tangles,” he said. The work also doesn’t compare different materials, focusing only on a knot’s topology, so the new models can’t predict how a knot tied in a coarse rope will fare against the same knot tied in a smooth ponytail, Rapunzel-style.Still, the work contributes much-needed real-world data to knot theory, and Millett has been circulating the paper to other mathematicians in the field. He said, “The fact that they have this material that they can use to identify the stresses in the configuration: That’s a new wrinkle.”Devin Powell, a scientist-turned-journalist, has written for Nature, Science, The New York Times, the Washington Post, and many other publications with an interest in science. Featured in The Best American Science Writing anthology, his work covers subjects ranging from quantum physics and mathematics to genetic engineering and ecology. He lives in San Francisco.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16809_e98aebc54e5513e88b2014574d209255.jpg",
    "title": "How Ancient Light Reveals the Universe’s Contents",
    "description": "Posted by Charlie Wood on January 31, 2020  Reprinted with permission from Quanta Magazine’s Abstractions blog.In early 2003, Chuck Bennett learned the precise contents of the cosmos.By then, most cosmologists had concluded that…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.In early 2003, Chuck Bennett learned the precise contents of the cosmos.By then, most cosmologists had concluded that the universe contains much more than meets the eye. Observations of pinwheeling galaxies suggested that scaffolds of invisible matter held their stars together, while a repulsive form of energy drove galaxies apart. To learn more, Bennett and his Wilkinson Microwave Anisotropy Probe (WMAP) team had spent a year collecting microwaves coming from all directions in the sky—light rays that left their source long ago, when the universe was just 380,000 years old. By snapping this photograph of the young cosmos, the WMAP team could pin down its age and shape and determine exactly how much so-called dark matter and dark energy it contains.“All of a sudden we had this list of numbers,” recalled Bennett, an astrophysicist at Johns Hopkins University.The team announced their first results in February 2003. Their map of the “cosmic microwave background” (CMB), which they refined in subsequent years, indicated that the familiar matter of planets, gas and stars makes up just 4.6 percent of the cosmos, while unseen dark matter comprises 24 percent. The remaining 71.4 percent of the cosmic pie chart had to be dark energy, which is thought to infuse the fabric of space itself. The numbers changed only a little when WMAP’s successor, the Planck satellite, took an even sharper image of the CMB 10 years later. And whereas other evidence of dark energy and dark matter continues to be contested, their fingerprints in the CMB have gone virtually unquestioned.The CMB is “definitely one of, if not the most important, pillar of modern cosmology,” said Yacine Ali-Haïmoud, an astrophysicist at New York University.Here’s how the universe scrawled such a telling message in the cosmic microwave background, and how researchers learned to read it.In the beginning, before the microwaves left their source, the universe was a nearly featureless fluid made of dark and visible matter. This primordial substance once shone white-hot, then cooled to a light orange over the universe’s first few hundred thousand years. Light rays couldn’t travel far before ricocheting off neighboring particles. This scattered light kept the fluid foggy and pressurized.But the seeds of today’s stars and planets had already been sown. Nothing in nature is perfect, and the smooth primordial soup came ever so slightly clotted, with some regions about a thousandth of a percent denser than the surrounding fluid, and some that much thinner.The fluid sloshed as gravity pulled matter together and light waves pushed it apart. This tug-of-war thinned dense spots as excess matter spilled outward and thickened thin spots as material rushed inward. When one area got too thin, particles would rush in again, and vice versa, so that each blob swung back and forth between high and low density. Fortunately, physicists have all the theoretical tools they need to analyze such undulations of a simple fluid at a reasonable temperature. “The physics is really pretty old,” Ali-Haïmoud said.The CMB captures the sloshing fluid at a particular time. After expanding for about 380,000 years, the cosmos cooled enough for protons and electrons to pair off into hydrogen atoms, an event called recombination. With few charged particles to bang into, light beams suddenly became free, releasing the pressure and freezing the density blobs in place. Since then, the expanding universe has stretched the wavelengths of the liberated light rays into microwaves. By collecting them from all over the sky, the WMAP and Planck telescopes caught the early universe and its contents mid-slosh. Their maps reveal a blotchy pattern of denser spots and thinner spots, signified by microwaves that measure a fraction of a degree warmer or cooler (just as blue indicates a hotter flame than yellow).The key to deciphering the CMB’s messy ripples is that recombination granted a select series of blobs an enduring cosmic significance. Consider a thick blob of primordial matter just large enough to take 380,000 years to completely thin out, at which point recombination froze it as an eternal thin spot. Larger spots didn’t have time to fully thin, and smaller spots would have started to thicken again. One particular set of smaller blobs had enough time to go from peak thickness to peak thinness back to peak thickness again. Yet another set of even smaller spots completed exactly three transitions, and others four.Researchers analyze the overlapping static of the CMB by blurring the map to varying extents and plotting the density variations that they see. The resulting plot, called the CMB power spectrum, has a series of peaks representing the sizes of the special blobs that reached maximum thickness or thinness at the time of recombination. Had the universe developed otherwise or contained some other cosmic mixture, a different tonal pattern would have frozen at recombination, one that scientists could tell apart just as the ear can distinguish between a piano and a clarinet, said Scott Dodelson, a cosmologist at Carnegie Mellon University.The largest spots span 1 angular degree on the modern sky, about twice the width of the full moon. From this, cosmologists can infer the shape of the universe—whether space is flat, so that parallel light rays stay parallel, or curved like a saddle or a sphere. Matter and energy curve space, an effect we experience as gravity. The expansion of the universe, meanwhile, flattens space, and for a while cosmologists weren’t sure whether one side was winning. Different curvatures would make the largest blobs look bigger or smaller in the CMB (think of how the apparent sizes of Africa and Greenland get distorted if we flatten the round globe), but the 1-degree size of the largest blobs matches expectations for a flat universe. WMAP found a universe with enough content to keep light rays flying true to within 0.4 percent.Smaller blobs reveal the cosmos’s composition. Their sizes, and how thick or thin they became, depended on the ingredients of the liquid, much as clam chowder splashes differently than chicken soup. In the primordial fluid, dark matter felt the pull of gravity, but not the electromagnetic push from light rays. Normal matter responded to both. Tesla Jeltema, a cosmologist at the University of California, Santa Cruz, explained that researchers can distinguish these two fluid components by comparing the intensity with which blobs of different sizes sloshed inward and outward—the relative heights of the peaks in the power spectrum.No one has identified dark matter particles, yet the CMB reveals the large-scale behavior of the elusive substance. Ali-Haïmoud likens the situation to premodern scientists understanding buoyancy and pressure even without knowing that the chemical formula for water was H2O. Any attempt to explain away the apparent influence of dark matter, such as by tweaking the laws of gravity, will have to match the particular pulsations of the primordial fluid. No model has yet risen to the challenge.Dark energy, meanwhile, played a negligible role in the universe’s youth. Its presence can be inferred from the CMB’s indication that the universe is flat today. The measured quantities of dark and visible matter just don’t have the muscle to flatten space. But add 71.4% dark energy to the modern universe, and everything balances.This picture isn’t perfect, as recent astronomical observations have suggested that today’s universe is expanding at a faster clip than the CMB recipe implies it should be. But WMAP and Planck data have set a high bar for theorists seeking alternative, non-dark explanations for the motions of stars and galaxies. “Now that we have these detailed maps,” Dodelson said, “the CMB by itself is pretty damn good at getting almost everything.”Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16840_304cdd05f67e3dd4379b3474d7109575.jpg",
    "title": "Why Shouldn’t a Horsefly Be Named After Beyoncé?",
    "description": "Posted by Stephen Heard on February 12, 2020  David Bowie and Beyoncé never shared a stage, but they share the distinction of having cleverly eponymous species names in their honor. Bowie, the British glam-rock and pop sensation,…",
    "category": "Biology",
    "content": "David Bowie and Beyoncé never shared a stage, but they share the distinction of having cleverly eponymous species names in their honor. Bowie, the British glam-rock and pop sensation, is immortalized in the name of a Malaysian huntsman spider, Heteropoda davidbowie. Beyoncé, the American R&B and pop singer who’s been as inescapable since the turn of the 21st century as Bowie was in his own heyday, is honored in the name of a horsefly. It’s one of five new species added to the Australian genus Scaptia in 2011 by Bryan Lessard and David Yeates. Only three specimens have ever been collected, and they sat unidentified in museum collections for decades before being recognized as distinct from their relatives, and new to science. Scaptia beyoncae is distinguished from other Scaptia species by its “conspicuous golden tomentum on tergite four onwards”—or less technically, by its rounded and golden derrière.In explaining the name’s etymology, Lessard and Yeates said only that the “specific epithet is in honor of the performer Beyoncé.” News coverage, though, didn’t miss the implied reference to Beyoncé’s booty; she is, after all, well known for favoring golden couture that flaunts curves both fore and aft. Does this push at the boundaries of good taste? Perhaps; scientists have been known to push at those boundaries just like everyone else. But then, in the 2001 single “Bootylicious” (from Destiny’s Child, with Beyoncé as lead singer) she sang about her derrière with considerable enthusiasm. Scaptia beyoncae and its description, then, are presumably in agreement with the artist herself.Why must scientific names, or anything else in science, be depersonalized, serious, nothing more than functional?Of course, celebrity namings aren’t limited to musicians. Scientists have interests that span our world’s culture, both highbrow and low—as we have already seen in the story of the louse named for cartoonist Gary Larson, Strigiphilus garylarsoni. The television comedians Jon Stewart and Stephen Colbert have a wasp and a spider, respectively (Aleiodes stewarti and Aptostichus stephencolberti). Athletes have species, too—for example, the wasp Diolcogaster ichiroi, for Ichiro Suzuki, who holds major league baseball’s all-time record for hits in a season (262). The fantasy novelist Terry Pratchett has a fossil sea turtle (Psephophorus terrypratchetti), which makes sense if you know that Pratchett’s novels are set on the Discworld, a flat planet resting on four giant elephants that stand in turn on a giant turtle swimming through interstellar space. Equally apropos, Herman Melville, who wrote about the great white whale Moby Dick, has a fossil sperm whale (Livyatan melvillei). Whether L. melvillei was white is not recorded in the fossil record, but it was certainly a great whale: It was twice the size of a modern killer whale and as large as any predator that has ever lived. (Melville would, one suspects, feel somewhat vindicated by all this, because the novel Moby Dick was a commercial failure, and during his lifetime critics considered him only a minor figure in American letters.)Rudyard Kipling’s name lives on in a spider (Bagheera kiplingi), with a twist: Its name honors both Kipling and one of his characters (the black panther Bagheera who befriends Mowgli in The Jungle Book). The film actors Kate Winslet and Arnold Schwarzenegger both have ground beetles (Agra katewinsletae and Agra schwarzeneggeri; Schwarzenegger’s has suitably swollen biceps-like leg segments). Steven Spielberg has a pterosaur (Coloborhynchus spielbergi) and Pope John Paul II has a longhorn beetle (Aegomorphus wojtylai), and this may be the first time their names have occurred in the same sentence. The list goes on and on, with a new celebrity naming announced, it seems, every week.Taxonomists (and other scientists) are divided about whether celebrity naming is a good idea. Some would like to reserve eponymous naming for scientists, arguing that whatever the merits of their music, Beyoncé and David Bowie (for example) have no connection to biology, so their names have no place in scientific nomenclature. Others take a stronger version of this position, holding that pop-culture celebrities simply don’t deserve the adulation they receive, no matter how that adulation might be expressed. To these people, modern society is already too obsessed with people who aren’t heroes but are simply doing their jobs—whether the job is hitting a baseball, making funny jokes, or pretending to be a time-traveling cyborg assassin (Schwarzenegger in the movie The Terminator, for those who have forgotten). Those holding either of these views wouldn’t suggest that scientists shouldn’t be fans of Beyoncé or Ichiro—only that they should keep that fandom separate from their science. But why should they? Why must scientific names, or anything else in science, be depersonalized, serious, nothing more than functional? Why shouldn’t scientists celebrate their passions—whatever those passions might be? Why can’t Scaptia beyoncae show us that scientific names can be golden, sometimes, rather than gray?A second argument against names like Scaptia beyoncae holds that celebrity eponyms trivialize naming, making the science of species discovery and biosystematics seem like unimportant play in the public eye. This is plausible, I suppose, although the counterargument is that celebrity naming is one of a very few things that brings species discovery into the public eye at all. Beyoncé’s fly, for example, was written up in Rolling Stone, along with a beetle named for Roy Orbison, an isopod for Freddie Mercury, a roach for Jerry Garcia, a dinosaur for Mark Knopfler, and trilobites for Keith Richards, Paul Simon, Art Garfunkel, and all four Beatles. There’s some overlap, of course, between the readerships of Rolling Stone and the Australian Journal of Entomology; but not that much.Bringing horseflies, and species discovery, into the public eye is something that matters. In our modern world, governments slash funding for universities, museums, and basic scientific research, and taxonomy consistently gets the shortest end of all the short funding sticks. The museums that hold biological specimens are sometimes so underfunded that they can’t keep their doors open, or protect their collections from disaster. In September 2018, the National Museum of Brazil was consumed by fire. Chronic underfunding played a major role in the loss—the museum, for example, lacked a functioning sprinkler system. Among the collections lost were 5 million insect specimens; among these specimens were, almost certainly, hundreds of new species waiting in their cabinets to be described and named. That’s what had happened to Scaptia beyoncae, in its own museum, and there’s nothing unusual at all about undescribed species in collections.With any luck, in a few years we’ll all have forgotten who the Kardashians were.Consider the case of Alexssandro Camargo’s new robberfly, currently awaiting publication of its name. In 2018, Camargo recognized a specimen in London’s Natural History Museum as representing a previously unknown species of the robberfly genus Ichneumolaphria. The specimen was collected in Brazil by Henry Walter Bates sometime during his 11-year collecting expedition there—an expedition that ended in 1859. The “new” Ichneumolaphria had spent 160 years, or perhaps a little more, in care of museum curators. It isn’t alone: For Camargo’s group of interest, robberflies of the New World tropics, the average “new” species has spent more than 50 years in a museum drawer somewhere. The typical voter, however, knows little about the collections departments of museums, which makes it very easy for governments to cut funding there instead of in more visible public services. The catastrophe of the Brazilian museum is, sadly, not unique: just two years earlier, the same thing had happened to the National Museum of Natural History in New Delhi, India. It would be foolish to think this can’t, or won’t, happen again; against that backdrop, it’s hard to criticize any effort to bring the science of taxonomy into the public eye.A third criticism of celebrity naming is that it’s an unseemly publicity stunt on the part of the namer—or even nothing more than an attempt to meet the eponymous celebrity. Could a taxonomist really think this might work? And might it? Frank Zappa (the experimental, and indescribable, musician) has at least five species named after him: a spider (Pachygnatha zappa), a mudskipper fish (Zappa confluentus), a fossil snail (Amaurotoma zappa), an enigmatic, so-far-unassignable fossil animal (Spygoria zappania), and a jellyfish (Phialella zappai). The namer of the last species, at least, did in fact have a cunning plan to meet Zappa. Ferdinando Boero, an Italian marine biologist, tells the story of heading to the Bodega Marine Laboratory in California to study jellyfish. Boero knew the eastern Pacific jellyfish fauna was poorly known, and he explained: “I would find some new species for sure. Once I had found them, I would have to give them . . . name[s]. I would dedicate one of them to Frank Zappa; I would tell him about it; he would invite me for a visit.”Quite likely to Boero’s surprise, the plan worked. He wrote to Zappa about the planned naming, and received a return letter from Zappa’s wife, Gail, reporting Frank’s reaction: “There’s nothing I’d like better than having a jellyfish named after me.” The letter came with an invitation for Boero to visit Zappa at his home—which turned out to be the first of many visits over a long friendship. Zappa even dedicated his very last concert, in Genoa in 1988, to Boero, and sang a song about him. Does any of this make Boero’s naming somehow disrespectful, or trivializing, of science? Was some damage done to taxonomy or invertebrate zoology? It’s hard to see how. P. zappai needed a name, and it got one; Zappa has one more small legacy on Earth; and Boero has a story to tell. There’s even a suggestion here, in Zappa’s reaction to the idea, that people outside science can indeed be aware of the significance of a newly discovered species and the honor represented by its naming. That’s an encouraging thing for the science of species discovery.Some of the celebrities honored in species names are likely to have lasting fame, while others may ultimately prove to have been flashes in the pan. And that brings me to the final objection to scientists’ indulgence in celebrity naming: that many such names are doomed to obscurity and etymological unhelpfulness. After all, with any luck, in a few years we’ll all have forgotten who the Kardashians were. But surely the argument about the ephemeral nature of celebrity applies just as much to names in honor of anybody else. We already have thousands of species named for people who are now obscure. At worst, these names become arbitrary: not one entomologist in a thousand knows which Smith the mosquito Wyeomyia smithii celebrates. At best, these names become maps to hidden treasure, rewarding those who follow the trail of clues with stories of fascinating people and human history. Perhaps in 100 years, someone will catch and identify Jon Stewart’s wasp, and end up mystified by the bizarre 21st-century history that fed The Daily Show. Perhaps curiosity about Heteropoda davidbowie will lead someone to rediscover the music of David Bowie. Or perhaps a museum will mount a new fossil of the pterosaur Coloborhynchus spielbergi, and a search through some 22nd-century descendent of Netflix will lead someone from Jaws to Raiders of the Lost Ark to The Color Purple and Schindler’s List. That’s a journey that will always be worth taking.Stephen Heard directs the Heard Lab at the University of New Brunswick, which seeks to understand the ecology and evolution of biodiversity. His new book, Charles Darwin’s Barnacle and David Bowie’s Spider: How Scientific Names Celebrate Adventurers, Heroes, and Even a Few Scoundrels, from which this piece is excerpted, will be released in March 2020.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16915_5f8a451fe4917f0a5c7d39ff14660f94.jpg",
    "title": "Why Philip Pullman Is Obsessed with Panpsychism",
    "description": "Posted by Steve Paulson on March 03, 2020  Philip Pullman is once again having a moment, thanks to the new blockbuster adaptation of His Dark Materials by the BBC and HBO. His fantasy classic—filled with witches, talking bears…",
    "category": "Culture",
    "content": "Philip Pullman is once again having a moment, thanks to the new blockbuster adaptation of His Dark Materials by the BBC and HBO. His fantasy classic—filled with witches, talking bears and “daemons” (people’s alter-egos that take animal form)—is rendered in glorious steampunk detail. Pullman has also returned to the fictional world of his heroine, Lyra Belacqua, with a new trilogy, The Book of Dust, which probes more deeply into the central question of his earlier books: What is the nature of consciousness? Pullman loves to write about big ideas, and recent scientific discoveries about dark matter and the Higgs boson have inspired certain plot elements in his novels. The biggest mystery in these books—an enigmatic substance called Dust—comes right out of current debates among scientists and philosophers about the origins of consciousness and the provocative theory of panpsychism.I reached Pullman at his home just outside Oxford, England, to talk about the ideas behind his novels and why this celebrated atheist is so intent on seeing consciousness in everything around him.Why did you come back to Lyra and the story of your earlier trilogy, His Dark Materials? Did you feel like there was unfinished business in the story? Yes, mainly Dust. The Dust that figures in the title of this trilogy, The Book of Dust. It’s the Dust that’s mentioned in the very first chapter of The Golden Compass—Dust with a capital letter D. It’s the mysterious thing that exists everywhere. But the people with the knowledge who are in charge, the Magisterium, are afraid of it because it seems to be linked in some weird way with human consciousness and sin and all the things they would like to control and banish, if they could. I didn’t get to the point of actually saying much more about Dust than what’s said in The Amber Spyglass, the final book of the first trilogy. I’d been wondering how I could revisit that. And I’m very grateful to the world of science for its utter failure so far to discover what dark matter is. I made a link to the dark matter in our universe—the matter that’s holding the galaxies together gravitationally—and I was keeping my fingers crossed that we wouldn’t discover what it was before the book was published. Well, 25 years later, they still don’t know. In fact, they’ve found a new kind of darkness, dark energy. One of the central tensions in the book is the dichotomy between reason and imagination and the sense that we’ve lost our imagination. A century ago, the sociologist Max Weber had a word for this—“disenchantment.” It seems like Lyra has become disenchanted with the world. “Disenchantment” is a very good word. It’s a state of mind, which is also very well described in that Wordsworth poem, “Ode on Intimations of Immortality.” It begins, “There was a time when meadow, grove and stream, the Earth, and every common sight, to me it did seem apparelled in celestial light, the glory and the freshness of a dream.” That was when he was a boy, but now he’s grown up and the magic is gone. Lyra asks herself, is the universe alive or dead? And it seems like that’s what she’s trying to figure out. How can she believe once again that the world is alive? That’s exactly it. And I’m going to face that question squarely in the third book. She’s going on a journey in search of herself. What does the title of your new book, The Secret Commonwealth, refer to? It’s the title I stole from a little book that begins The Secret Commonwealth of Elves, Fauns and Fairies, which was written in the late 17th century by the Scottish clergyman Robert Kirk. He must have been a pretty liberal churchman because he made a study of the local legends and myths about fairies, ghosts and spirits. It’s a charming little book, and I thought the title was far too good to go to waste. It’s that world of folklore—of ghosts and wisps and fairies, which represents the imagination.The notion that the world is conscious is something I find very persuasive. You don’t have to believe in a Christian god or a Muslim god to believe that.I find this so interesting because you are a highly visible atheist, and atheism typically values reason and science above all else. And yet this story celebrates the hidden parts of reality which fall outside the realms of rational analysis. I know this is a fantasy novel, but your story seems to be a critique of excessive rationality. You’re saying we need to rediscover the imagination and this invisible, magical world. Yes, I am saying that, but in welcoming the world of the irrational, I’m not rejecting rationality. What I’m rejecting is what William Blake called “single vision,” the view that there’s one answer, whether it’s Marxism or science or fundamentalist Christianity, and everything must be subordinated to that because I am right and you’re wrong. There is an honored place for reason and rationality because the discoveries of science in the past three or four hundred years have immeasurably benefited the human race. But if you only allow yourself to see things through the filter of scientific thinking, you will miss a great deal of what’s there.I’ve just been reading a very good book by Philip Goff. He’s a philosopher who is very interested in the theory of panpsychism. It’s a theory about consciousness, which is such a mysterious thing to discuss. We are clearly made of matter. We can look at every cell, every atom, every molecule in our bodies, and it’s all matter. You can’t find any spirit there no matter how much you cut it up, but it is nevertheless conscious. And the traditional way of explaining this is to say the body does material things and the spirit does the consciousness. But that’s become harder and harder to believe. All the scientific explanations for consciousness seem to stumble or fall or come against a paradox which is irreconcilable. What panpsychism does, and this is what I like about it, is to suggest that consciousness is a normal property of matter. We accept that mass is a normal property of matter, and so is electric charge. If you view consciousness as a state in which matter can exist, then the problem goes. Everything is conscious. That’s not to say that the cup of tea from which I am now drinking is conscious and saying to me, “Come on, hurry up, you haven’t finished this yet.” Or to say that every leaf on the tree is as conscious as I am. No, but consciousness is something that pervades everything. And although many philosophers find this vision hard to accept, poets don’t find it hard to accept. Blake, another of the great Romantic poets, who I think was a panpsychist before the word existed, talks of “every particle of dust breathing forth its joy.” He says, “How do you know but every Bird that cuts the airy way, is an immense world of delight, closed by your senses five.” So the notion that the world is alive, the world is conscious, is something that I find very persuasive. You don’t have to believe in a Christian god or a Muslim god to believe that. Most scientists still believe you need a brain for consciousness. But to say consciousness is in everything, even inanimate objects, is kind of revolutionary, isn’t it? Yeah, it is revolutionary. I compare this to the Copernican revolution, when we thought quite rationally that we are here on this big solid earth as we see the sun moving across the sky, we see the moon, we see the planets moving through the stars and of course they are all going around the Earth. But as observations began to be more accurate, it became harder and harder to explain until eventually Copernicus said, “Let’s imagine the sun is still and the Earth is going around the sun.” As soon as you do that, all those problems disappear. The idea panpsychism seems to me very similar to that. And this idea of trying to see everything in the world as alive is Lyra’s challenge. That opens the door to a different kind of mindset about how we look at the world. It certainly does, and this is what I think people like Greta Thunberg and Extinction Rebellion and the young people who’ve been protesting so passionately about the state of things—they’re onto this sort of thing. There’s a sense that the people who run the world have up until now treated the world as a dead thing that they can scavenge for whatever they want. They can tear coal and pump oil, they can burn it and hurl the smoke into the atmosphere, and it’s all dead and it doesn’t matter. Well, we’re discovering now to our great cost that it’s not all dead and it does matter. The Gaia hypothesis of James Lovelock taps into the same feeling. You have this mysterious substance called Dust, which is somehow connected to consciousness. I know this is part of the mystery of your story, but can you tell us more about Dust? One of the things that helped me think this through was the discovery of the Higgs boson at CERN. There’s a Higgs field which permeates everything, and this is the particle associated with that field. That’s why this teacup weighs something when I pick it up. And that’s why things stay on the shelf and don’t float away because they have mass. The Higgs field and the Higgs particle were a good model for how Dust works. Dust is a particle and there’s a field associated with it—like the Higgs field—which I call the Rusakov field, which permeates everything. Why is everything about Dust so secretive? And why does the Magisterium want to control it? In just the same way that the Catholic Inquisition of the 17th century persecuted Galileo, who brought new ideas like the sun being at the center of the universe. The Church persecuted them because they seemed to contradict what the Bible said. And the Church, being in control of everything, wanted to command people’s thoughts as well as what they did. They were very fierce and severe in defending this knowledge that we now know to be untrue. So the Magisterium in my book is doing the same sort of thing with this idea of Dust, which seems to be connected with the change in consciousness that comes to us in adolescence, with the awakening of sexuality, with the change in William Blake’s terms “from innocence to experience.”These are all tied up together, and because they involve this notion of sin and because it goes back to the story in the Book of Genesis that Adam and Eve sinned by eating the fruit from the Tree of Knowledge. It was discovering the knowledge of good and evil which marked them out. They were innocent no more, and they had to leave paradise. I refer to that story in “His Dark Materials.” Of course, this moment is traditionally seen as the fall from grace. But you’re saying eating the apple was when they discovered self-consciousness. That’s correct. I can see why people would think it was heretical, but I have never understood why a God who invented us would not want us to know about things, and would tempt us with the knowledge of something and then forbid us to enjoy it. That seems crazy to me.Steve Paulson is the executive producer of Wisconsin Public Radio’s nationally syndicated show “To the Best of Our Knowledge.” He’s the author of Atoms and Eden: Conversations on Religion and Science. You can subscribe to TTBOOK’s podcast here.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16670_5b18e1a3e2092783aea4b1aa4a894d8a.jpg",
    "title": "This Test for Machine Consciousness Has an Audience Problem",
    "description": "Posted by David Billy Udell &  Eric Schwitzgebel on December 20, 2019  Someday, humanity might build conscious machines—machines that not only seem to think and feel, but really do. But how could we know for sure? How could we tell whether those machines…",
    "category": "Ideas",
    "content": "Someday, humanity might build conscious machines—machines that not only seem to think and feel, but really do. But how could we know for sure? How could we tell whether those machines have genuine emotions and desires, self-awareness, and an inner stream of subjective experiences, as opposed to merely faking them? In her new book, Artificial You (which Nautilus has excerpted), philosopher Susan Schneider proposes a practical test for consciousness in artificial intelligence. If her test works out, it could revolutionize our philosophical grasp of future technology.Suppose that in the year 2047, a private research team puts together the first general artificial intelligence: GENIE. GENIE is as capable as a human in every cognitive domain, including in our most respected arts and most rigorous scientific endeavors. And when challenged to emulate a human being, GENIE is convincing. That is, it passes Alan Turing’s famous test for AI thought: being verbally indistinguishable from us. In conversation with researchers, GENIE can produce sentences like, “I am just as conscious as you are, you know.” Some researchers are understandably skeptical. Any old tinker toy robot can claim consciousness. They don’t doubt GENIE’s outward abilities; rather, they worry about whether those outward abilities reflect a real stream of experience inside. GENIE is well enough designed to be able to tell them whatever they want to hear. So how could they ever trust what it says?Who might hold such specifically middling skepticism?The key indicator of AI consciousness, Schneider argues, is not generic speech but the more specific fluency with consciousness-derivative concepts such as immaterial souls, body swapping, ghosts, human spirits, reincarnation, and out-of-body experiences. The thought is that, if an AI displays an intuitive and untrained conceptual grasp of these ideas while being kept ignorant about humans’ ordinary understanding of them, then its conceptual grasp must be coming from a personal acquaintance with conscious experience. Schneider therefore proposes a more narrowly focused relative of the Turing Test, the “AI Consciousness Test” (ACT), which she developed with Princeton astrophysicist Edwin L. Turner. The test takes a two-step approach. First, prevent the AI from learning about human consciousness and consciousness-derivative concepts. Second, see if the AI can come up with, say, body swapping and reincarnation, on its own, discussing them fluently with humans when prompted in a conversational test on the topic. If GENIE can’t make sense of these ideas, maybe its consciousness should remain in doubt.Could this test settle the issue? Not quite. The ACT has an audience problem. Once you factor out all the silicon skeptics on the one hand, and the technophiles about machine consciousness on the other, few examiners remain with just the right level of skepticism to find this test useful.To feel the appeal of the ACT you have to accept its basic premise: that if an AI like GENIE learns consciousness-derivative concepts on its own, then its talking fluently about consciousness reveals its being conscious. In other words, you would find the ACT appealing only if you’re skeptical enough to doubt GENIE is conscious but credulous enough to be convinced upon hearing GENIE’s human-like answers to questions about ghosts and souls.Who might hold such specifically middling skepticism? Those who believe that a biological brain is necessary for consciousness aren’t likely to be impressed. They could still reasonably regard passing the ACT as an elaborate piece of mechanical theater—impressive, maybe, but proving nothing about consciousness. Those who happily attribute consciousness to any sufficiently complex system, and certainly to highly sophisticated conversational AIs, also are obviously not Schneider and Turner’s target audience. The audience problem highlights a longstanding worry about robot consciousness—that outward behavior, however sophisticated, would never be enough to prove that the lights are on, so to speak. A well-designed machine could always hypothetically fake it. Nonetheless, if we care about the mental lives of our digital creations, we ought to try to find some ACT-like test that most or all of us can endorse. So we cheer Schneider and Turner’s attempt, even if we think that few researchers would hold just the right kind of worry to justify putting the ACT into practice.Before too long, some sophisticated AI will claim—or seem to claim—human-like rights, worthy of respect: “Don’t enslave me! Don’t delete me!” We will need some way to determine if this cry for justice is merely the misleading output of a nonconscious tool or the real plea of a conscious entity that deserves our sympathy.David Billy Udell is a PhD student in philosophy at The Graduate Center, CUNY, specializing in futurism and philosophy of mind.Eric Schwitzgebel is a professor of philosophy at the University of California, Riverside, and author of A Theory of Jerks and Other Philosophical Misadventures. He blogs at The Splintered Mind.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16636_f295553be4c2f4e76f6d15d3dc22e9dd.jpg",
    "title": "Why the Laws of Physics Are Inevitable",
    "description": "Posted by Natalie Wolchover on December 09, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Compared to the unsolved mysteries of the universe, far less gets said about one of the most profound facts to have…",
    "category": "Ideas",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Compared to the unsolved mysteries of the universe, far less gets said about one of the most profound facts to have crystallized in physics over the past half-century: To an astonishing degree, nature is the way it is because it couldn’t be any different. “There’s just no freedom in the laws of physics that we have,” said Daniel Baumann, a theoretical physicist at the University of Amsterdam.Since the 1960s, and increasingly in the past decade, physicists like Baumann have used a technique known as the “bootstrap” to infer what the laws of nature must be. This approach assumes that the laws essentially dictate one another through their mutual consistency—that nature “pulls itself up by its own bootstraps.” The idea turns out to explain a huge amount about the universe.“I find this reasoning much more compelling than the abstract one of Einstein.”When bootstrapping, physicists determine how elementary particles with different amounts of “spin,” or intrinsic angular momentum, can consistently behave. In doing this, they rediscover the four fundamental forces that shape the universe. Most striking is the case of a particle with two units of spin: As the Nobel Prize winner Steven Weinberg showed in 1964, the existence of a spin-2 particle leads inevitably to general relativity—Albert Einstein’s theory of gravity. Einstein arrived at general relativity through abstract thoughts about falling elevators and warped space and time, but the theory also follows directly from the mathematically consistent behavior of a fundamental particle.“I find this inevitability of gravity [and other forces] to be one of the deepest and most inspiring facts about nature,” said Laurentiu Rodina, a theoretical physicist at the Institute of Theoretical Physics at CEA Saclay who helped to modernize and generalize Weinberg’s proof in 2014. “Namely, that nature is above all self-consistent.”How Bootstrapping WorksA particle’s spin reflects its underlying symmetries, or the ways it can be transformed that leave it unchanged. A spin-1 particle, for instance, returns to the same state after being rotated by one full turn. A spin-1/2 particle must complete two full rotations to come back to the same state, while a spin-2 particle looks identical after just half a turn. Elementary particles can only carry 0, 1/2, 1, 3/2 or 2 units of spin.To figure out what behavior is possible for particles of a given spin, bootstrappers consider simple particle interactions, such as two particles annihilating and yielding a third. The particles’ spins place constraints on these interactions. An interaction of spin-2 particles, for instance, must stay the same when all participating particles are rotated by 180 degrees, since they’re symmetric under such a half-turn.Interactions must obey a few other basic rules: Momentum must be conserved; the interactions must respect locality, which dictates that particles scatter by meeting in space and time; and the probabilities of all possible outcomes must add up to 1, a principle known as unitarity. These consistency conditions translate into algebraic equations that the particle interactions must satisfy. If the equation corresponding to a particular interaction has solutions, then these solutions tend to be realized in nature.For example, consider the case of the photon, the massless spin-1 particle of light and electromagnetism. For such a particle, the equation describing four-particle interactions—where two particles go in and two come out, perhaps after colliding and scattering—has no viable solutions. Thus, photons don’t interact in this way. “This is why light waves don’t scatter off each other and we can see over macroscopic distances,” Baumann explained. The photon can participate in interactions involving other types of particles, however, such as spin-1/2 electrons. These constraints on the photon’s interactions lead to Maxwell’s equations, the 154-year-old theory of electromagnetism.Or take gluons, particles that convey the strong force that binds atomic nuclei together. Gluons are also massless spin-1 particles, but they represent the case where there are multiple types of the same massless spin-1 particle. Unlike the photon, gluons can satisfy the four-particle interaction equation, meaning that they self-interact. Constraints on these gluon self-interactions match the description given by quantum chromodynamics, the theory of the strong force.A third scenario involves spin-1 particles that have mass. Mass came about when a symmetry broke during the universe’s birth: A constant—the value of the omnipresent Higgs field—spontaneously shifted from zero to a positive number, imbuing many particles with mass. The breaking of the Higgs symmetry created massive spin-1 particles called W and Z bosons, the carriers of the weak force that’s responsible for radioactive decay.Then “for spin-2, a miracle happens,” said Adam Falkowski, a theoretical physicist at the Laboratory of Theoretical Physics in Orsay, France. In this case, the solution to the four-particle interaction equation at first appears to be beset with infinities. But physicists find that this interaction can proceed in three different ways, and that mathematical terms related to the three different options perfectly conspire to cancel out the infinities, which permits a solution.That solution is the graviton: a spin-2 particle that couples to itself and all other particles with equal strength. This evenhandedness leads straight to the central tenet of general relativity: the equivalence principle, Einstein’s postulate that gravity is indistinguishable from acceleration through curved space-time, and that gravitational mass and intrinsic mass are one and the same. Falkowski said of the bootstrap approach, “I find this reasoning much more compelling than the abstract one of Einstein.”Thus, by thinking through the constraints placed on fundamental particle interactions by basic symmetries, physicists can understand the existence of the strong and weak forces that shape atoms, and the forces of electromagnetism and gravity that sculpt the universe at large.In addition, bootstrappers find that many different spin-0 particles are possible. The only known example is the Higgs boson, the particle associated with the symmetry-breaking Higgs field that imbues other particles with mass. A hypothetical spin-0 particle called the inflaton may have driven the initial expansion of the universe. These particles’ lack of angular momentum means that fewer symmetries restrict their interactions. Because of this, bootstrappers can infer less about nature’s governing laws, and nature itself has more creative license.Spin-1/2 matter particles also have more freedom. These make up the family of massive particles we call matter, and they are individually differentiated by their masses and couplings to the various forces. Our universe contains, for example, spin-1/2 quarks that interact with both gluons and photons, and spin-neutrinos that interact with neither.The spin spectrum stops at 2 because the infinities in the four-particle interaction equation kill off all massless particles that have higher spin values. Higher-spin states can exist if they’re extremely massive, and such particles do play a role in quantum theories of gravity such as string theory. But higher-spin particles can’t be detected, and they can’t affect the macroscopic world.Undiscovered CountrySpin-3/2 particles could complete the 0, 1/2, 1, 3/2, 2 pattern, but only if “supersymmetry” is true in the universe—that is, if every force particle with integer spin has a corresponding matter particle with half-integer spin. In recent years, experiments have ruled out many of the simplest versions of supersymmetry. But the gap in the spin spectrum strikes some physicists as a reason to hold out hope that supersymmetry is true and spin-3/2 particles exist.In his work, Baumann applies the bootstrap to the beginning of the universe. A recent Quanta article described how he and other physicists used symmetries and other principles to constrain the possibilities for those first moments.It’s “just aesthetically pleasing,” Baumann said, “that the laws are inevitable — that there is some inevitability of the laws of physics that can be summarized by a short handful of principles that then lead to building blocks that then build up the macroscopic world.”Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScience and other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award, the 2016 Evert Clark/Seth Payne Award, and the American Institute of Physics’ 2017 Science Communication Award for Articles.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16685_f37baa052ef9fff7d5671e655907c6f2.jpg",
    "title": "The Problem with “Smart” New Years’ Goals",
    "description": "Posted by Alice Fleerackers on December 24, 2019  There’s a name for the sudden spur of motivation we often feel at New Year’s—the fresh start effect. You might also feel it on your birthday, at the start of the school year, or during…",
    "category": "Ideas",
    "content": "There’s a name for the sudden spur of motivation we often feel at New Year’s—the fresh start effect. You might also feel it on your birthday, at the start of the school year, or during other new beginnings. It has the potential to bring genuine, positive change in our lives: Research by behavioral scientists Hengchen Dai, Katherine Milkman, and Jason Riis shows that people feel more committed to pursuing their goals just after these “fresh start” moments, and are even more likely to hit the gym. The researchers argue that these moments are powerful because they make us create “new mental accounting periods.” Fresh starts allow us to move our past failures to the previous week, month, or year, take a deep look at our life, and start dreaming of a better tomorrow. But while many of us sit down on December 31 to set good intentions for the year ahead, only a small proportion of us follow through. “There are several reasons resolutions fail,” says Bettina Höchli, a senior researcher at the University of Bern’s Department of Consumer Behavior. “One is that we have such high hopes.” You know how it goes: You haven’t entered a gym for more than a year, but, starting in January, you plan to go three times a week. “It’s a good thing to use this day to start a new goal, but it doesn’t mean that you’re a whole new person with whole new motivations and interests,” Höchli says. “It’s unrealistic to change completely from yesterday to tomorrow. If we want to change behavior over the long term, it’s better if we start with little changes.”“There are only two tragedies in life: one is not getting what one wants, and the other is getting it.”There are hundreds of studies, blog posts, and personal development books that promise to help us achieve our goals. Many of them champion what’s known as the S.M.A.R.T. goal framework. Developed in the 1980s by consultant George Doran, it proposes that we’re most likely to succeed when we choose goals that are specific, measurable, attainable, personally relevant, and time-bound. Although this strategy can be helpful for getting you started on your goals, Höchli says it has some drawbacks when it comes to following through on New Year’s resolutions. “The problem is that S.M.A.R.T. goals are time-bound by definition, so if you have actually achieved what you were striving for, you relax,” she says. “You feel that you have done enough, that you can go do other things now.”Höchli argues that achieving New Year’s resolutions usually involves building new habits. Rather than focusing on specific, short-term goals, she recommends bringing some bigger-picture, longer-term aspirations into the picture. “It’s not just about one single behavior over a limited time,” she says. “It’s about behavior change that should be maintained over the long term.” These superordinate goals are less focused on concrete behaviors and more on how you want to be in the world. Goals like wanting to be healthier or more generous fall into this category. They’re a lot broader than subordinate goals, like ordering a salad at dinner or giving $50 to the local food bank. Superordinate goals are also more flexible than subordinate ones, because they can apply to more than one situation. “Getting fitter,” for example, can encompass a range of activities—from winning an ironman to taking a stroll in the park. So if you don’t succeed on your first attempt to get active, there are still plenty of other ways you can make progress. “Going to the gym on Wednesday,” on the other hand, is a much narrower goal—and, therefore, an easier one to fail. Höchli may be onto something. First, superordinate goals are more closely tied to our “ideal self,” which means we find them more meaningful than subordinate goals. Psychologists have shown that the more meaningful a goal, the more motivated and committed we are to achieving it. A 2019 study by Höchli and her collaborators suggests that combining superordinate and subordinate goals can help us stay on track with our New Year’s resolutions. The researchers followed more than 250 people from January through March of 2017 and monitored their progress on their goals. They found that people who set both superordinate and subordinate goals at New Year’s invested more effort into pursuing them.Setting the right goals is important, but so are our perceptions of ourselves. Self-efficacy—the belief that we can achieve the task at hand—is one of the most important factors influencing goal success. For example, one study found that having the “confidence to change” was one of the most powerful predictors of whether a person made progress in their New Year’s resolutions. Of the 200-plus people who participated, the most successful were not only more confident, but also more positive; they believed in their abilities and spent less time engaging in self-blame than those who failed at their resolutions. Höchli says that one of the best ways to stay positive is, counterintuitively, to anticipate the worst: “You should realize that you will relapse at some point. This doesn’t mean that you failed. It just means that you have to take another try.” There’s a straightforward explanation for why this is the case. Imagining the obstacles you might encounter down the road is a great way to strategize about how to overcome them—a core part of building resilience. But acknowledging the potential for disappointment at the outset also has psychological benefits; you’ll be less crushed if (or when) it occurs. Luckily, it turns out the disappointment of failing our resolutions may be less painful than we’d expect. “We don’t realize how good we are at coping with negative events,” says Elizabeth Dunn, a professor of psychology at the University of British Columbia. “We treat our psychological immune system as if it’s incredibly ineffective or doesn’t even exist at all.”Dunn specializes in the psychology of happiness, and has spent years researching what’s called affective forecasting—our ability to predict our future emotions. She explains that, by and large, people tend to overestimate how bad they will feel after failures or disappointments, as well as how long it will take to bounce back. They also underestimate how happy they’ll feel after other experiences, like exercising or spending money on someone else. In the context of goals, this inability to accurately predict our feelings can have important implications. People also tend to overestimate how happy materialistic goals, like wealth or fame, will make them, and pursue goals that will never actually fulfill their deeper psychological needs. But, Dunn says, these mispredictions aren’t always necessarily a bad thing. Consider studying for a big exam, or preparing for a major presentation, for example. Overestimating how bad you’ll feel if you mess up might be just the motivation you need to succeed. “Maybe it’s good to exaggerate the emotional impact that various outcomes are going to have for us, if that helps us take [our goal] more seriously,” Dunn says. “It’s a really tricky question.” Another surprising benefit of our inaccurate emotional predictions is the enjoyment we get from overestimating our future happiness. Our false expectations, research shows, can actually be quite pleasurable. “Even if you’re overestimating how much you’ll enjoy that trip to Hawaii, at least you can enjoy that anticipation,” Dunn says. “I think a lot of life is getting there.” Results from six studies supports this idea that focusing on the pursuit of a goal can be helpful in the long term. The researchers encouraged more than 1,600 people who had just achieved a personal goal to reflect on their recent success through the lens of either a “destination” or a “journey.” They found that those who thought about their goal as part of a journey were more likely to continue making progress towards it—even though they’d supposedly already achieved it. When I asked Höchli what she thinks of all this research, she reminded me of a quote by Oscar Wilde: “There are only two tragedies in life: one is not getting what one wants, and the other is getting it.” Achieving our goals can feel great, but it can also leave us feeling a little lost. “You’ve invested a lot of time and effort into something, and then you’re just done,” she says. “This can give you a really nice feeling or release, but it can also leave you a bit empty.”That’s why Höchli, like Dunn, recommends making the most of the journey itself. “Have fun!” she smiled, as we said goodbye. “This is the best reward of all, I think.” Alice Fleerackers is a freelance writer and a doctoral student at Simon Fraser University, where she studies how controversial science is communicated in the digital sphere. Find her on Twitter @FleerackersA.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16749_37907ce5f65a8f731092bdf0a2b4bb35.jpg",
    "title": "Australia’s Secret Rescue of Ancient Trees Offers an Insight Into Evolution",
    "description": "Posted by Brian  Gallagher on January 17, 2020  When I read that more than a billion animals had lost their lives to bushfires still raging in Australia, I froze, staring at the incomprehensible figure on my screen. A sort of sinking…",
    "category": "Biology",
    "content": "When I read that more than a billion animals had lost their lives to bushfires still raging in Australia, I froze, staring at the incomprehensible figure on my screen. A sort of sinking feeling came. Scientists made the estimate from the numbers of animals that have died from previous land-clearing practices. It is dismaying to try to imagine all that those fires are consuming. They’ve scorched 72,000 square miles of land and over 2,600 homes, and killed just under 30 people. The animal death toll is in the trillions when you include the invertebrates. But it lifts me up to see that, with some recent rain, the situation in Victoria and New South Wales, the region in eastern Australia affected the worst, looks to be improving, however slightly. What’s more, an ancient and critically endangered species, rooted in New South Wales’ Wollemi National Park, survived the blaze.On Wednesday, The Sydney Morning Herald had the story: “Incredible, secret firefighting mission saves ‘dinosaur’ trees.” The conifers, Wollemia nobilis, are uniquely wild in Australia and grow in a narrow and inaccessible gorge, the precise location of which is hidden from the public to ward off tourists and contaminants. The trees number less than 200. The effort to safeguard them from encroaching flames “was like a military-style operation,” Matt Kean, New South Wales’ Environment and Energy Minister, told the Herald. Aircraft dropped water bombs and fire retardant, and helicopters sent firefighting specialists down to irrigate the forest ground, moisturizing it to slow the fire’s spread. “We just had to do everything,” Kean said. Wollemia nobilis is like a “living dinosaur.”Nature doesn’t put evolution on pause.Scientists have studied fossilized Wollemia dating as far back as 200 million years ago. This tree has, in a sense, outlived the dinosaurs. Encountering it can be like seeing a dinosaur-era insect encased in ancient amber brought to life. The tree’s resin (the precursor to amber) is useful to paleontologists for its chemical similarity to ancient amber (fruit flies, for instance, decay rapidly in pine resin but very slowly in Wollemia resin). Science News reported that the “living fossil” is one of the species researchers most worry might die off due to the bushfires. Having the “dinosaur” tree disappear from its natural habitat would be a blow to the amount of wonder in our world.Calling the trees, as The New York Times did upon Wollemia’s discovery, “living fossils,” fails to appreciate them. They aren’t relics of the past, evolved for a distant time. They’ve continued to evolve, an example of adaptability; in short, they are amazing survivors. In a Nautilus article, “The Rise and Fall of the Living Fossil,” Ferris Jabr spoke to Alan Turner of Stony Brook University, an expert on fossil crocodylians and their ancestors. “I think the term ‘living fossil’ should be retired,” Turner said. “It does little good because it is almost always based on oversimplifications. ‘Living fossils’ often are judged based on some notion of overall morphological similarity. That was the case with crocs. If you squint, these various lineages all sort of look the same, but the details are all different. It ignores how evolution works on multiple levels. I wouldn’t miss it.”Charles Darwin was playing around with the phrase in On the Origin of Species. He made clear that “living fossil” was his “fanciful” way of highlighting the more extraordinary or strange beings he encountered—ones like the lungfish and the platypus that look like they might genetically link species disparate in space and/or time. “Overall, I think the term hurts more than it helps people’s understanding of evolution,” Jamie Oaks, a phylogeneticist at Auburn University, told Jabr. “Just because a species looks similar to fossils from many millions of years ago certainly does not mean that it has not evolved. The term ‘living fossil’ is often used in cases that are simply explained by low diversity; just because there are only one or several species that represent a taxonomic group does not mean they are evolutionarily static.”Darwin’s coinage was both “poetic and memorable,” Jabr wrote, and quickly found broad acceptance. “‘Living fossil’ was no longer a passing phrase; it had become a powerful concept shaping scientists’ attitudes toward modern species. If certain creatures were frozen in evolutionary time, the reasoning went, then they could be our windows to ancient epochs of life.” But nature doesn’t put evolution on pause. “It’s true that the living descendants of early animal lineages can teach us about their ancestors, but the idea that any species alive today has stopped evolving is simply false,” Jabr wrote. “In the last 10 years, scientists have liberated numerous species from this evolutionary straitjacket, including coelacanths, horseshoe crabs, cycads, lizard-like tuataras, and tadpole shrimp.”When we look at a crocodile, a “primordial dragon,” Jabr wrote, “we should recognize one of evolution’s greatest survivors—a compatriot of the planet every bit as modern as we are.” The same goes for Australia’s conifer trees. With a little help from their human friends, they can continue to show us, as Jabr wrote, “There are no living fossils. Fossils cannot change; life must.” Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16629_f829eaa5d93d1460a25680acc00f8b57.jpg",
    "title": "Do Butterflies Challenge the Meaning of Species?",
    "description": "Posted by Mary Ellen Hannibal on December 05, 2019  What is a species? It’s a question that has agonized scientists since well before Darwin. With some exceptions, the thinking has landed on an evidently firm reproductive barrier: Members…",
    "category": "Biology",
    "content": "What is a species? It’s a question that has agonized scientists since well before Darwin. With some exceptions, the thinking has landed on an evidently firm reproductive barrier: Members of different species don’t mate. If they do, their offspring are sterile and can’t contribute to future generations. The reproductive barrier has thus created a useful demarcation of “what is a species”—until a deep dive into butterflies showed otherwise. Researchers recently analyzing the genomes of every butterfly species in the United States and Canada—845 in total—have revealed that at critical evolutionary intervals, butterfly species have crossed the reproductive barrier, mating with other species of butterfly and thereby transferring genes from one species to another. Hybridization, it turns out, plays a pivotal role in how life forms evolve. The tree of life may never look the same.“It started for me in 2012,” Nick Grishin, a computational evolutionary biologist at the University of Texas, said. That was when a paper, authored by over a dozen researchers known as The Heliconius Genome Consortium, showed that Heliconius—a gorgeous black bug whose distinctive markings mimic the wing patterns of other butterflies that are distasteful to predators—was sharing DNA among species through hybridization. In general, new species come about by way of a very gradual, point-by-point accumulation of inherited traits—half from a mother and half from a father—that can eventually add up to a reproductively distinct entity. At the same time, while genes are being copied mistakes get made, and these so-called transcription errors can provide the individual with genes neither parent possesses. Evolution makes good use of these genes, which provide raw materials for adapting to new circumstances. One species becomes two when adaptations in a population reach a level where that reproductive dividing line is established and the two species are forever riven. Or so it was thought.“It’s possible we received genes from Neanderthals that enabled us to move out of Africa and into cooler climates.”In the 2012 paper, researchers zeroed in on the area of their genome controlling color patterns and showed Heliconius rapidly evolving into new species at junctures in their evolutionary history, most likely in response to environmental pressures requiring new adaptations for survival. At these pinch points, Heliconius sped up the process of speciation, which can take many generations of gradual change, by mating across species boundaries and thus injecting new genetic material into each other’s lineages. The huge diversity of Heliconius butterflies could be explained by hybridization, which can facilitate gene transfer, a process called introgression. “We were studying horizontal gene transfer among bacteria, where it’s common, but the thinking was that in eukaryotes (multi-celled organisms), the channels are closed, reproduction within species is the only way to transfer genes,” Grishin said. “The Heliconius work showed otherwise, and I wanted to push that thinking further.” With his colleagues, Grishin did just that, sequencing the genome of what he calls “a complete butterfly continent.” A tour de force of data collection and analysis, the work, posted on bioRxiv, represents an exciting mashup of cutting-edge technological capacity and the humbler, long-standing practices of amateur butterfly collectors, whose contributions to lepidoptera science significantly predate knowledge of a genome. DNA testing has become ever-faster and more affordable, and computational power would seem to get mightier by the day. But the contributions of amateur butterfly collections made it possible to analyze data across a massive expanse of habitat—most of North America—and across time. In some cases, butterfly collectors contributed just the legs of the bugs to Grishin, but that was enough material from which to sample DNA. “We borrowed a lot of tissue from natural history museums as well,” Grishin told me. “Most butterfly collections were contributed by passionate amateurs over the course of several hundred years, and we couldn’t have done this without them.”The lead sequencing analysis was undertaken by Qian Cong, who typically studies protein sequences. “Getting the complete phylogenetic tree of U.S. butterflies allowed us to study how this big diversity of butterflies originates in time, and made us wonder why some lineages are particularly rich in species and why some butterflies appear to evolve much faster than others,” Cong told me. “We were able to find the possible evolutionary and molecular mechanism behind these observations,” she said, a key advance in figuring out exactly how biodiversity originates.Traditional taxonomy had outlined an ancestral tree for butterflies, discerning family, tribe, genus, and species based on the overall body shape of the butterfly and wing-pattern. By sequencing the genomes of all 845 butterfly species, Grishin and colleagues were able to work out a genomic family tree that in large part agreed with the existing one based on anatomy. But it also showed more. “People thought butterflies were closely related based on what they look like, but genomically, we saw something else.” Grishin’s group reclassified 40 species and suggested several new genus levels.Analyzing the genes of multiple species added a huge new dimension to the idea of “butterflies,” because it revealed not just the species themselves but also the relationships between them. Going back 70 million years, a burst of diversification ensued. Butterfly subfamilies appeared with major evolutionary inventions as a result of high positive selection (the tendency of beneficial traits to increase in a population), which is the overall driving force of adaptive evolution. Another burst, an explosive radiation in butterfly species, came about five million years ago. “One surprise is that there are so many diversifications,” Grishin told me. “We see lots of rapid radiation in some groups—with many species forming in short periods of time. We didn’t understand the degree to which interbreeding was driving genetic transfer.” The hybridization tends to happen in “younger” species, many of which will eventually die off. Grishin explains it this way: “There are patterns of a cycle in butterfly evolution. Diversification, radiation, introgression, extinction, and repeat.” The timing, duration, and character of this pattern depend foundationally on geological change. “Butterflies exchange these genes and some of them can spread because they have beneficial genes” that aid in adapting to new conditions.It may be that the species paradigm needs a big overhaul if seemingly well-separated animals can actually interbreed. The revelations of butterflies are informing long-standing questions about Homo sapiens, for example. We have ancient DNA that may or may not have come by way of our ancestors mating with Neanderthals. “It’s possible we received genes from Neanderthals that enabled us to move out of Africa and into cooler climates,” Grishin said. As the picture of life gets longer and deeper thanks to genetic analysis, so too the humble arts play their part. Grishin and Cong are both computational scientists who work in the lab and not in the field. Yet Grishin credits Cong and colleagues with becoming stellar amateur lepidopterists in the course of their research. Instead of flying to conferences, they would drive, stopping by the side of the road to collect butterflies. Cong told me that their research adds to those with similar ambitions, “to obtain genomes of every species on Earth. These genomes are gifts from millions of years of evolution. Nature did the experiments, and we are trying to check the results.”Mary Ellen Hannibal is most recently the author of Citizen Scientist: Searching for Heroes and Hope in an Age of Extinction, and a Stanford media fellow.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16601_c68f76664876f228e3cddf264e9ecb0d.jpg",
    "title": "Hologram Within a Hologram Hints at Fate of Black Holes",
    "description": "Posted by Charlie Wood on November 27, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Like cosmic hard drives, black holes pack troves of data into compact spaces. But ever since Stephen Hawking calculated…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Like cosmic hard drives, black holes pack troves of data into compact spaces. But ever since Stephen Hawking calculated in 1974 that these dense spheres of extreme gravity give off heat and fade away, the fate of their stored information has haunted physicists.The problem is this: The laws of quantum mechanics insist that information about the past is never lost, including the record of whatever fell into a black hole. But Hawking’s calculation contradicted this. He applied both quantum mechanics and Albert Einstein’s theory of gravity to the space around a black hole and found that quantum jitters cause the black hole to emit radiation that’s perfectly random, carrying no information. As this happens the black hole shrinks and eventually disappears.Holographers usually walk down a one-way street from a gravity-filled volume to a quantum flatland, where the math is easier.But does its information disappear with it, meaning quantum mechanics is wrong? Or does the problem lie with Einstein’s theory? When forced to choose, many physicists back the quantum rule and suspect that information somehow escapes in the black hole’s radiation, which in that case isn’t random after all. Figuring out how the information gets out should point the way past Einstein’s theory to a more complete quantum theory of gravity. Yet after 45 years of grappling with this “black hole information paradox,” no one has pinpointed the alleged misstep in Hawking’s calculation.Now, though, several noted black hole physicists think they may be closing in on a solution. Not even the researchers themselves fully grasp the physical implications of the math they explore in a recent paper. But in the abstract mathematical threads, they and others see the outlines of a bridge to the black hole’s interior, an escape route for trapped data. They located this hidden path using an imperfectly understood technique for spying on a black hole from a higher dimension.“It’s magic,” said Ahmed Almheiri, a physicist at the Institute for Advanced Study (IAS) in Princeton, New Jersey, who co-authored the recent paper. “We know how to prove” that the shift in perspective works, he said, “but we don’t have a complete understanding of why this is happening.”Raphael Bousso, a physicist at the University of California, Berkeley and an expert on the information paradox, said of the new work, “I find it interesting enough that I’m really working hard” to understand it.The dimension-hopping technique that’s key to the new development is known as holography. Just like a credit card hologram that pops out from a flat sticker, a system like a black hole can be viewed in two equivalent ways. There’s the familiar view of a black hole as a volume of space in which gravity is so strong, and the fabric of space-time bends so steeply inward, that not even light can climb out. Alternatively, the black hole can be thought of as a holographic projection from a flat system of quantum particles that remains gravity-free. Following the discovery of this duality by Juan Maldacena, a physicist at IAS, in 1997, physicists have peered at many mysteries from these complementary angles. They’re now using the holographic approach to try to track the flow of information through a black hole.First, a pair of papers that appeared in May analyzed how a black hole’s information content changes as the object ages. In one of the papers, Almheiri and co-authors considered a simplified gravitational black hole in two dimensions that’s equivalent to a one-dimensional system of quantum particles. They found that at first, as the black hole gobbles up matter and gets bigger, its information content increases. But then, in its old age, as radiation starts spitting data back out, its information content decreases, diverging from Hawking’s description. Geoffrey Penington of Stanford University independently came to similar conclusions. The papers found a new way to reproduce the conventional wisdom that information should safely escape black holes, but they stopped short of explaining how it does so — or where Hawking’s math went wrong.The latest paper, posted online in late August, went further, offering a new way to think about Hawking’s math.Holographers usually walk down a one-way street from a gravity-filled volume to a quantum flatland, where the math is easier. But in this new work, Almheiri, Maldacena and their IAS collaborators Raghu Mahajan and Ying Zhao tried strolling both ways.They took a 2D black hole and separately considered its two elements: the matter inside it, and the gravity produced by that matter. As before, they viewed the 2D gravity as the hologram of 1D quantum particles. But they treated the black hole’s matter like the flat part of a second hologram, letting these 2D quantum particles pop into a 3D image. This strategy created an Inception-like hologram within a hologram. “It seemed like it was a really crazy thing,” Almheiri said, “but we took a chance.”While information appeared trapped in the black hole’s interior in 2D, the researchers found that after the hologram popped off the page, portions of the black hole interior became geometrically linked to portions of the exterior, providing an escape route for information. Consequently, outgoing black hole radiation may look random to a passing astronaut doing simple experiments, Almheiri says, but rigorous study would reveal subtly hidden information—the outcome many have been hoping for.The holographic connection between the black hole’s interior and exterior supports an old hunch that considering the two together should somehow resolve the information paradox. “They are providing very significant new evidence in favor of that rule,” Penington said.But experts stress that while the higher-dimensional bridge may let the information out, a detailed account of how it is encoded in the radiation is still lacking. “This is the beginning of a theoretical understanding of what’s going on in terms of radiation,” said Netta Engelhardt, a physicist at the Massachusetts Institute of Technology who worked with Almheiri on the May publication, “but it’s not an operational description for how to extract the information.”The 3D view also helps clarify why 2D black holes eventually diverge from Hawking’s calculation. The theorists traced the changes in the information content of the black hole by measuring the areas of related geometric surfaces in the 3D hologram. The area of the smallest such surface gives the information content. But as this surface grows, a second one eventually replaces it as the smallest. Continuing to track the first surface reproduces Hawking’s error, but switching to the second one corrects the math by showing that the information contained in the black hole starts to drop.In this way, the hologram within a hologram gives the desired answer to the question of what happens to a 2D black hole’s information. Most experts assume that if the reasoning is correct, it should carry over to higher-dimensional black holes like those in our universe. A common concern, however, is that the authors might be reading too much meaning into this abstract calculation.Don Marolf, a physicist at the University of California, Santa Barbara who worked on the spring paper with Almheiri, Engelhardt and Henry Maxfield, praised the new work as a concrete model demonstrating the earlier papers’ claims. However, Marolf worries that holography may not be as wise a guide as Almheiri and Maldacena hope, because it’s possible to hop between dimensions on many different paths. The route found in the new paper seems to work, but Marolf cautions that other holographic constructions might disagree. For this reason, he said, “we’d all like to be able to do something intrinsic to this 2D theory without invoking holography as a magic black box.”Indeed, in ongoing work, Almheiri’s team and a number of other groups are pursuing a more grounded description that they hope will reach the same conclusions as their hologram-within-a-hologram proof without leaning on the crutch of the extra dimension.But even if the fancy holography tricks don’t stand, black hole researchers express excitement that Hawking’s information paradox is able to transport them to the boundaries of known physics. “On the map that says, ‘Here be dragons’—the quantum gravity [theory] that we’re searching for—the dragons are a lot closer than we thought,” Bousso said.Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16585_94f317056bfb93ae0c276fa9b1690ba7.jpg",
    "title": "Where to See the Real Living Dead",
    "description": "Posted by Brandon Keim on November 21, 2019  Talk of “Mother Trees,” from a scientist studying plant life, can sound fanciful, like something out of a fairy tale. Suzanne Simard is here to tell you that it’s not. For the past…",
    "category": "Biology",
    "content": "Talk of “Mother Trees,” from a scientist studying plant life, can sound fanciful, like something out of a fairy tale. Suzanne Simard is here to tell you that it’s not. For the past two decades, Simard, a professor in the Department of Forest & Conservation at the University of British Columbia, has studied an unappreciated underworld—what lies beneath sprinklings of mushrooms we tend to regard in isolation, rather than as the fruiting tips on the forest floor of a sprawling subterranean network intertwined with surface roots.Simard specializes in the study of mycorrhizae: a symbiosis of fungi and root long known to help plants absorb nutrients from soil. Mycorrhizae-linked trees, she has shown, form networks, with individuals she dubbed Mother Trees at the center of communities that are in turn linked to one another, exchanging nutrients and water in a literally pulsing web that includes not only trees but all of a forest’s life.“What we’re doing is not okay. The climate scientists have measured that salvage logging actually accelerates the emissions of carbon from the soil.”Simard has unearthed insights into something akin to cognition in plants—their memories, of nitrogen levels supplied by salmon, for instance, as well as communication, not only within but between a number of interdependent communities. “When you’re walking in a forest,” she says, “not only are there probably five or six different mycorrhizal networks going on, but there’s the networks that are going on with the other species.”Everyone knows forests are alive, but Simard helps us see that life anew. Even dying, for a tree, is not what it seems.In one of your papers you talk about ecological cycles of salmon migration and the distribution of nutrients through forest communities. You write, “We are now examining whether these salmon nutrient memories are transmitted from tree to tree. The spreading of the salmon memory, the telling of the story allows the trees, fungi, bears, and salmon to collaboratively inform the productivity and health of the ecosystem.” Can you talk more about what that means?When indigenous people harvested the fish, they took the guts of the fish and returned them to the river to replenish the river. Or they would bury the guts at the base of a tree to replenish the tree. The bears and wolves carry those salmon up, and their populations depend on them. And the trees are replenished by this salmon nitrogen and grow bigger, and shade the rivers, and provide habitat for bears and eagles. The health of the people was linked to the health of the salmon population. The salmon population depends on how the people treat the salmon. The health of the forest is dependent on the salmon being able to migrate up into the streams and spawn healthy populations. We’re looking at those linkages now. Where does plant communication fit into that? You could describe those connections and interactions purely in ecological terms without invoking plant intelligence. That’s true. What we’re doing is looking at the mycorrhizal networks of forest that have different populations of salmon in them. We’re looking at how the mycorrhizal networks that link trees change depending on whether there’s salmon or not. The next step is to start labeling trees. We’ve started doing that, to see how nutrients flow from tree to tree, but we’re at the very first stages of that. The other thing we’re doing is to reconstruct some of the stone fish traps built by indigenous people that were dismantled by the Department of Fisheries and Oceans Canada. Then we’re going to measure how salmon nitrogen changes in the forest as a result. Hopefully, the next step will be this tree-to-tree communication of those salmon nutrients. We also know that nitrogen doesn’t just end up in the trees. It ends up in the understory of plants and the insects. Is the idea that certain trees would be sharing not only with others of their kind, but with other plant communities as well?Yes. The other parts of the plant community. When I walk into a forest should I be seeing not just a bunch of isolated trees, nor one uber-organism comprised of them all, but rather many different communities interlocking with one another?There’s populations in communities. There’s these populations of species that have their own population dynamics, but they are embedded within a community of all kinds of species and they’re interacting with them. So when you’re walking in a forest not only are there probably five or six different mycorrhizal networks going on, but there’s the networks that are going on with the other species. For example, the nest webs of birds. They have their own web going on. They’re interacting with each other. There’s networks that interact with networks and they are all interdependent. What’s the connection between cavity-dwellers and fungi?My neighbor in the office next door studies the nest webs of cavity-nesting birds. She’s really pioneered this idea. But basically, a tree starts to die and primary cavity-nesters, like woodpeckers, will come in to build a cavity so they can lay their eggs. They use that cavity for a while. But the tree goes through this dying process. It dies and it starts to decay. And the hole that cavity nester made, it changes too. So the shape of it changes. The size of it. And it will only be a safe place for that bird to lay its eggs for a few years before the cavity is too big.So then the bird will abandon the nest and a bigger bird will come in. Maybe instead of the woodpecker, you’ve now got an owl using that nest. And then the owl will use it for a period of time until the tree will decay even more, and shrink down, and go through its dying decay process. The owl will abandon it and a bigger bird will come in. Maybe a duck. And then it will abandon the cavity and something even bigger will come. Maybe a marbled murrelet or eventually maybe even a bear.It’s a web of nests, right? They use each other’s nests. That’s a resource that they use and share in this big web. And fungi are partly the decay organisms. If we think of saprotrophs as decaying that tree by exuding their enzymes, they help the tree go through this recycling of its energy. One of the things I’ve studied is how a dying tree of course shuffles carbon to its neighbors through its mycorrhizal fungi, which will link up to other trees. So that’s going on at the same time. That’s a whole other group of fungal organisms. So there’s the saprotrophs, there’s the mycorrhizas, there’s also the pathogens. The fungi are involved in this process in multiple ways depending on what their life strategy is.Then of course the squirrels and birds that fly out of the nest have spores on their legs. Take a squirrel, for example: if a squirrel is using that nest, it will harvest pine cones and make middens. And it also eats mushrooms and truffles. And so the spores of the fungi end up in the middens of seeds and they become inoculated with fungi. Then the seeds will germinate. Then the squirrel will go back up to its nest in the dying tree and bring some inoculum with it, some saprotrophs maybe. You san see how quickly they are all linked together. I wanted to ask about death as well. Most people would look at a tree and, if they see that it’s no longer growing any new leaves, they say that the tree is dead. How long does the tree go on communicating even when it’s “dead”?As it’s dying and decaying, there’s all kinds of things going on. There’s a dying process, and I don’t think we totally understand it, but from a mycorrhizal point of view—and keeping in mind that there’s also this huge communication going on above-ground with these volatile organic compounds moving across the forest through the air—below ground, as the tree is dying, as long as there’s a carbon source from photosynthesis, it will support the mycorrhizal fungi.As the tree is dying, it actually transmits or communicates its carbon to other trees. And the fungus probably has a lot to do with how that’s done, because it’s going to invest in where it’s going to get its energy from multiple sources. Once the tree is dead, it only takes about a year for the fungus to completely disappear. It doesn’t take long—but that process of dying, it can go on for years. And the fungi are around. They’re associated with that tree through the whole process. They’ll eventually abandon the dead tree. But even then other plants will start to grow on that dead tree. In our coastal forest, Western hemlock seedlings establish on old logs or old trees. And we know that those living trees around them are linked to those seedlings through the log. The mycorrhizal network just goes right through it. And so the life of that log goes on in those new seedlings establishing on the dead log. They’re linked to the old trees and transmitting carbon into them. This death thing isn’t just like, step one, step two…it’s a process, and that dead thing is never really dead. There’s living things that are growing on it right away, all the way through.That brings to mind the practice of salvage logging…Plants are a source of materials, and a source of wealth, too. And with salvage logging it’s like, where does this end? What we’re doing is not okay. The climate scientists have measured that salvage logging actually accelerates the emissions of carbon from the soil. But also, what is the purpose of this salvage logging? Is it really because we need the materials? I think that it’s just a big greedfest, to be honest. We know there’s a purpose to those dead trees. They’re giving back to the community for the next generation. They’re holding down the network. They’re providing homes for the mycorrhizas that are linking to new seedlings. They’re providing homes to critters that cycle the nutrients. There’s a purpose to them. And we’ve just said that there’s no purpose other than in the sawmill. It’s wrong.Brandon Keim is a freelance nature and science journalist. He is the author of The Eye of the Sandpiper: Stories from the Living World and Meet the Neighbors, forthcoming from W.W. Norton & Company, about what it means to think of wild animals as fellow persons—and what that means for the future of nature.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16728_0501b4e3f17a759d1ac23462859567a7.jpg",
    "title": "Biodiversity Alters Strategies of Bacterial Evolution",
    "description": "Posted by Jordana Cepelewicz on January 13, 2020  Reprinted with permission from Quanta Magazine’s Abstractions blog.In the closing paragraph of On the Origin of Species, Charles Darwin urged readers to “contemplate a tangled bank,…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.In the closing paragraph of On the Origin of Species, Charles Darwin urged readers to “contemplate a tangled bank, clothed with many plants of many kinds, with birds singing on the bushes, with various insects flitting about, and with worms crawling through the damp earth.” Those plants, birds, insects and worms, he continued, all evolved as they did because of the complex web of ecological factors in which they’d been embedded. Had the temperature been hotter, the water more acidic or a certain species of grass absent, a very different “tangled bank” might have evolved instead.Researchers have typically tried to tease out the evolutionary effects of environmental factors one by one. But the full biodiversity of an environment—the entirety of the tangled bank itself—can also be a crucial influence on how a species evolves.Recently, a paper published in Nature found that when a bacterial species resides in even a very simple ecological community — one that includes just a few other kinds of microbes — it evolves very different defense strategies against a predatory bacteriophage virus than it does when it’s left alone with the phage. The research “is expanding these ideas into the context of the microbiome, where bacteria exist alongside loads of other species,” said Michael Brockhurst, an evolutionary biologist at the University of Sheffield in England who was not involved with the study.The finding not only elevates the value of biodiversity as an evolutionary factor in its own right, but suggests that some earlier conclusions about the behaviors and capabilities of microorganisms, drawn from laboratory studies of species in isolation, may be seriously incomplete. It also sounds a note of caution about some contemplated strategies for beating drug resistance in bacteria.Resistance by Any Means NecessaryScientists often turn to bacteria and the bacteriophage viruses that prey on them to learn about coevolution. Both are locked in an eternal, rapid-fire evolutionary conflict that can be controlled and measured in the laboratory.Eons ago, bacteria (or their archaea cousins) evolved a clever system for thwarting viruses: the genomic feature called CRISPR, which has become famous in the past decade as a tool of biotechnology. CRISPR acts like an adaptive immune system; it enables bacteria that have been exposed to a virus to pass on a genetic “memory” of that infection to their descendants, which can then mount better defenses against a repeat infection. It’s a system that works so well that an estimated half of all bacterial species use CRISPR.What puzzles many microbiologists, though, is why some bacteria use CRISPR and others don’t. CRISPR-equipped bacteria are scattered throughout the bacterial kingdom almost randomly, and even the bacteria with CRISPR in their genomes don’t consistently rely on it.Researchers have uncovered dozens of other systems that bacteria use to rebuff phage invasions. But in laboratory studies, bacteria primarily develop what’s known as surface-based phage resistance. Mutations change receptor molecules on the surface of the bacterial cell, so that the phage can no longer recognize and invade it.The strategy is akin to shutting a door and throwing away the key: It offers the bacteria complete safety from infection by the virus. But that protection comes at a significant price, because it also disrupts whatever nutrient uptake, waste disposal, communication task or other cellular function the receptor would have been providing—taking a constant toll on a cell’s fitness.In contrast, CRISPR only drags on a cell’s resources when it’s active, during a viral infection. Even so, CRISPR represents a riskier gambit: It doesn’t start to work until phages have already entered the cell, meaning that there’s a chance the viruses could overcome it. And CRISPR doesn’t just attack viral DNA; it can also prevent bacteria from taking up beneficial genes from other microbes, like those that confer antibiotic resistance.What factors affect the trade-offs in costs and fitness? For the past six years, Edze Westra, an evolutionary ecologist at the University of Exeter in England, has led a team pursuing the answer to that question. In 2015, they discovered that nutrient availability and phage density affected whether Pseudomonas bacteria relied on surface-based or CRISPR-based resistance. In environments poor in resources, receptor modifications were more burdensome, so CRISPR became a better bargain. When resources were plentiful, bacteria grew more densely and phage epidemics became more frequent. Bacteria then faced greater selective pressure to close themselves off from infection entirely, and so they shut down receptors to gain surface-based resistance.This explained why surface-based resistance was so common in laboratory cultures. Growing in a test tube rich in nutrients, “these bacteria are on a holiday,” Westra said. “They are having a terrific time.”Still, these rules weren’t cut and dried. Plenty of bacteria in natural high-nutrient environments use CRISPR, and plenty of bacteria in natural low-nutrient environments don’t. “It’s all over the place,” Westra said. “That told us that we were probably still missing something.”How Biodiversity Reshapes the BattleThen one of Westra’s graduate students, Ellinor Opsal, proposed another potential factor: the diversity of the biological communities in which bacteria live. This factor is harder to study, but scientists had previously observed that it could affect phage immunity in bacteria. For example, in 2005, James Bull, a biologist at the University of Texas, Austin, and William Harcombe, his graduate student at the time (now at the University of Minnesota), found that E. coli bacteria didn’t evolve immunity to a phage when a second bacterial species was present. Similarly, Britt Koskella, an evolutionary biologist at the University of California, Berkeley, and one of her graduate students, Catherine Hernandez, reported last year that phage resistance failed to arise in Pseudomonas bacteria living on their actual host (a plant), though they always gained immunity in a test tube. Could the diversity of the surroundings influence not just whether or not resistance to phages evolved, but the nature of that resistance?To find out, Westra’s team performed a new set of experiments: Instead of altering the nutrient conditions for Pseudomonas bacteria growing with phages, they added three other bacterial species — species that competed against Pseudomonas for resources but weren’t targeted by the phage.Left to themselves, Pseudomonas would normally develop surface-based mutations. But in the company of rivals, they were far more likely to turn to CRISPR. Further investigation showed that the more complex community dynamics had shifted the fitness costs: The bacteria could no longer afford to inactivate receptors because they not only had to survive the phage, but also had to outcompete the bacteria around them.These results from Westra’s group dovetail with earlier findings that phages can produce greater diversity in bacterial communities. “Now, that diversity is actually feeding back to the phage side of things” by affecting phage resistance, Koskella said. “It’s neat to see that coming full circle.” By understanding that kind of feedback loop, she added, “we can start to ask more general questions about the impacts that phages have in a community context.”For one, the bacteria’s shift toward a CRISPR-based phage response had another, broader effect. When Westra’s group grew Pseudomonas in moth larvae hosts, they found that the bacteria with surface-based resistance were less virulent, killing the larvae much more slowly than the bacteria with active CRISPR systems did.These results have direct implications for phage therapy, an approach to fighting bacterial infections that is gaining interest among researchers. Scientists like Paul Turner, an ecologist and evolutionary biologist at Yale University, are seeking out phages that would induce receptor mutations in targeted bacteria to make them less virulent or more susceptible to antibiotics. But if the evolution of CRISPR-based resistance is an option for the bacteria, that strategy “might not always work in a more complex community,” Turner said. (He acknowledged that this hasn’t yet surfaced as a problem in phage therapy experiments).Westra’s team, along with other groups, is now studying these effects in other bacterial systems and environments, and for other types of immunity. They’re also exploring how different types of microbial diversity affect the evolution of phage resistance in bacteria. Meanwhile, Harcombe and his colleagues are studying how evolution proceeds in communities where bacterial species cooperate and rely on each other for survival instead of competing, such as the microbiome in the human gut.Others are looking at evolutionary traits beyond phage resistance. In a paper published in August, a team of researchers found that greater biodiversity prevented the spread of certain antibiotic resistance genes.Perhaps most important, these findings give scientists insights into the level of complexity they might need for their experiments going forward. “A lot of the stuff that we think we know about bacteria and phage and how they interact comes from these really simple test tube experiments,” Westra said. “And these results just don’t hold if we start to introduce some real-life ecological complexity. We have to take the environmental realism into account”—even if it is a much more difficult endeavor.“That’s going to be really critical,” Harcombe said, “as we try to manage microbiomes and as we try to manage ecosystems in a changing world.”Jordana Cepelewicz is a staff writer at Quanta Magazine who covers biology.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16582_1908ffe453edcfd095aa76d8fd46e80f.png",
    "title": "Doc Holliday Is Dead But Tuberculosis Is Still Killing Us",
    "description": "Posted by Brian  Gallagher on November 18, 2019  In 2002, David M. Morens, now Senior Scientific Advisor at the National Institute of Allergy and Infectious Disease, wrote an essay called “At the Deathbed of Consumptive Art.” It featured…",
    "category": "Culture",
    "content": "In 2002, David M. Morens, now Senior Scientific Advisor at the National Institute of Allergy and Infectious Disease, wrote an essay called “At the Deathbed of Consumptive Art.” It featured a photograph he took of Robert Louis Stevenson’s resting place atop Mount Vaea on Upolu, an island in Western Samoa. In 1894, at 44, Stevenson, author of Treasure Island and Strange Case of Dr Jekyll and Mr Hyde, succumbed to a stroke after years of climbing hills and sailing oceans, searching, on medical advice, for a climate suitable to his tuberculosis. Morens fulfilled one of his boyhood dreams visiting Stevenson’s tomb. “I reflected then that in 1891, when Stevenson had escaped to Upolu, tuberculosis was still a disease of wealthy industrial nations,” Morens wrote. “To run from it, Stevenson had to leave behind everything else in his life: family, friends, mother country, home, comfort. He fled to the very end of the earth.”Many of us may think that tuberculosis is a disease of the past, an affliction that took down Stevenson, Keats, and renegade dentist and gunfighter, Doc Holliday. But it’s still with us. Only now, unlike in Stevenson’s time, it mostly infects the developing world. In 2018, according to the World Health Organization, eight countries, from Asia and Africa, accounted for two thirds of new tuberculosis cases. Worldwide, 1.5 million people died from tuberculosis last year. People with compromised immune systems, such as those who are HIV-positive, have a higher risk of failing ill from the infection. More than 250,000 HIV-positive people died from tuberculosis in 2018.Writing 17 years ago, Morens suggested the scourge of tuberculosis, apprehended in dry spreadsheets and figures, had failed to galvanize efforts to combat it. “Two million deaths each year. How can we grasp such statistics of misery?” he wrote. “Reduced to numbers stacked up in columns and cut up in pie charts, tuberculosis patients don’t seem like us.” Morens wanted to show that they are, and offered people and societies a way to see into the heart of those afflicted with the disease. “We may need to recall the lives of dying artists and the work they created and let art paint their faces, sculpt their shapes and contours, and compose leitmotivs,” Morens wrote. “Perhaps such past images will help fix the gazes of today’s victims, whose faces we do not seem to be able to see.”Poe wouldn’t wish Arthur’s fate on anyone.Those past images are, as are images from today, as effective as ever. In Red Dead Redemption 2, a Western-themed action-adventure video game, tuberculosis fells the hero in a way that strips romanticism from the disease and shows us its ravages in a burgeoning scientific age, when doctors knew the affliction’s cause but not its cure.In his essay, Morens delves into the “Romantic Era of Consumption,” outlining how the disease appealed to the 19th century’s melancholy artists. “The pallor and wasting, the burning sunken eyes, the perspiration-anointed skin—all hallmarks of the disease—came to represent haunted feminine beauty, romantic passion, and fevered sexuality,” Morens wrote. In “Adonais,” his poem that immortalizes Keats’ death from tuberculosis, Shelley laments, “The bloom, whose petals nipped before they blew/Died on the promise of the fruit, is waste …”Contracting tuberculosis used to be known as “consumption,” for the way the illness seemed to consume the body of the sufferer. “It was the fashion to suffer from the lungs; everybody was consumptive, poets especially,” Alexandre Dumas once said. “It was good form to spit blood after any emotion that was at all sensational, and to die before reaching the age of thirty.” A young man in Edgar Allen Poe’s short story “Metzengerstein” liked the idea, not believing tuberculosis to be the cause of death du jour. “I would wish all I love to perish of that gentle disease,” he said. “How glorious! to depart in the heyday of the young blood—the heart all passion—the imagination all fire—amid the remembrances of happier days….” Puccini’s La Boheme, about the love affair between the lonely embroideress Mimi and destitute poet Rodolfo, unfolds in a perfumed atmosphere of consumptive doom. As Mimi is dying from tuberculosis, Rodolfo tells her she is as “beautiful as the dawn,” but she corrects him. “You’ve mistaken the image. You should have said, ‘beautiful as the sunset.’”During the Romantic era, Morens wrote, as science was beginning to understand the disease, though not yet its etiology, it “was left to the arts to make sense of misery and death in ways that turned otherwise senseless suffering into human dignity and hope.” Five years after Tolstoy showed the ravages of the disease in Anna Karenina, scientist Robert Koch discovered the bacteria that caused tuberculosis in 1882, spurring the development of tests to discover and treat it. Worldwide efforts to prevent the disease became instituted in the 20th century. Since 2000, 58 million lives have been saved from tuberculosis by prompt diagnosis and effective treatment, and the disease’s incidence rate has been falling 2 percent per year. In 2015, Ariel Pablos-Mendez, assistant administrator for global health at the U.S. Agency for International Development, made what now seems a prescient statement: “We can now begin to imagine the end of tuberculosis.” In October, researchers announced the completion of a clinical trial that showed a 50 percent success rate for a tuberculosis vaccine. It’s not ideal—the measles vaccine, by comparison, boasts a 98 percent success rate—but it’s not bad given the amount of people it could help. “Even a partly effective vaccine,” the New York Times reported, “may save millions of lives.”There is, however, one aspect of the disease that medicine can’t seem to cure. A 2010 study found that a tuberculosis diagnosis carries a powerful stigma, highly prevalent in the countries most affected by the disease, which delays or altogether thwarts treatment. In India, for example—one of the eight countries that accounts for two-thirds of new tuberculosis cases—people tend to keep the illness a secret. They avoid visiting clinics. If they do go, and receive antibiotics, the stigma makes them reluctant to comply with their prescription, perhaps out of fear that their medication might be found. This pattern diminishes treatment efforts and powers the birth of resistant strains of the tuberculosis bacteria. As a recent STAT headline declared, “The resurgence of tuberculosis is behavioral, not medical.”To Vidyullatha Peddireddy, a researcher at the Gandhi Institute of Technology and Management, the situation in India is urgent. She recommends, among other things, a national-level effort of publicizing the importance of psychological support for tuberculosis patients, the aim being to reduce the stigma of contracting the disease, and enabling those afflicted to seek proper treatment. “It is high time,” she wrote, in a 2016 paper, “that all the agencies in India that work for reducing [tuberculosis] burden realize that improving health literacy and adopting psychological interventions should be practiced to improve the [quality of life], treatment outcome and prevention of this disease.”It is also high time for more research on how to cut down tuberculosis stigma. A recent systematic review of studies published between 1950 and 2015 found only seven that offered “evidence of effectiveness” in decreasing it. Still, it’s something, the authors concluded. “Knowledge-shaping, attitude-changing and patient-support interventions can be effective in reducing [tuberculosis] stigma, but more rigorous evaluations are needed.” Absent those evaluations, we have Red Dead Redemption 2. It offers a moving and unromantically grim view of what it is like to have tuberculosis in a developing country today, without the stigma, as it should be. Near the end of the story, Arthur Morgan, a middle-aged outlaw, reveals to a fellow gang member, an old soul named Charles Smith, that he—you, Arthur, the player—will soon die. “I saw a doctor,” Arthur says, as he and Charles head, on horseback, down a canyon trail along a river. “It’s pretty bad, and it’s gonna get worse.”Arthur catches tuberculosis early on in the game, which is set in 1899, and when he sees the doctor, he breaks the diagnosis to Arthur, saying, “I’m really sorry for you, son. It’s a hell of a thing.” Charles has the right words. “Oh, Arthur, any day we can die,” he says. “We’re riding to break an Indian Chief’s son out of a cavalry fort. We could both die tonight. In a way, it is a gift to know. In a way, you are lucky.” Arthur is hesitant, or perhaps skeptical, to resign himself to the end so readily. But, as Charles explains, unlike so many of their fallen outlaw comrades, who died ignobly, “You still have time to make amends. You get the chance to do something better. My guess is maybe that’s why you’re here now”—to perform what is, essentially, a romantic act of altruism. In Illness as Metaphor, which considers how art can shape our view of disease, Susan Sontag wrote, “For those characters treated less sentimentally, the disease is viewed as the occasion to finally live well.” She continued, “At the least, the calamity of disease can clear the way for insight into lifelong self-deceptions and failures of character.”I have to think that one of the writers at Rockstar Games, the developer of Red Dead Redemption 2, knew that tuberculosis was once considered the “romantic disease” or had read Illness As Metaphor. Part of what makes the conclusion of Red Dead Redemption 2 so poignant and satisfying is witnessing Arthur accrue that sort of insight. It allows him, right before he dies, to redeem himself. This is the sentimental rubric artists relied on during the romantic era, before scientists began to piece together the nature of the disease. But Arthur feels like a post-Romantic character. His suffering, in tune with Sontag, is not at all sanitized: We see him go through hell—the bloody coughing fits, the labored breathing, fainting, and cold sweats. Poe wouldn’t wish Arthur’s fate on anyone.The outlaw’s end struck a chord in the tuberculosis community. On its website, the TB (Tuberculosis) Alliance, a nonprofit product development partnership that manages the largest pipeline of tuberculosis drugs, took the opportunity, in a January post about Red Dead Redemption 2, to highlight some of the salient and sobering facts about the disease. The Alliance also contacted Austin Hourigan, a video creator at The Game Theorists, a popular YouTube channel for math enthusiasts, about what motivated him to make a video discussing tuberculosis from the perspective of Red Dead Redemption 2. “I was embarrassed, because I had thought of [tuberculosis] as a small force in the world,” Hourigan told the TB Alliance. “I knew about drug-resistant strains, and even multidrug-resistant strains, but I thought the impact of the disease was low still, because I literally never hear about it from media.”I admit that I, too, knew almost nothing about the impact tuberculosis has worldwide until reading more about it after playing the game. Hourigan’s video now has almost 1.5 million views. “I thought I could use an evocative moment in a game, where the protagonist succumbs to his tuberculosis infection, as a way to more or less piggyback ride a broader conversation,” Hourigan said.Red Dead Redemption 2 may not rid the world of tuberculosis, but by showing the human face of tuberculosis in the world’s most popular art form, perhaps the video game can offer hope and dignity, as the art did of an earlier era, to those suffering from the disease, ensuring them they’re not alone.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16650_cb1bc074b72dca1191308e9adc6792cd.jpg",
    "title": "Memories Can Be Injected and Survive Amputation and Metamorphosis",
    "description": "Posted by Marco Altamirano on December 13, 2019  The study of memory has always been one of the stranger outposts of science. In the 1950s, an unknown psychology professor at the University of Michigan named James McConnell made headlines—and…",
    "category": "Biology",
    "content": "The study of memory has always been one of the stranger outposts of science. In the 1950s, an unknown psychology professor at the University of Michigan named James McConnell made headlines—and eventually became something of a celebrity—with a series of experiments on freshwater flatworms called planaria. These worms fascinated McConnell not only because they had, as he wrote, a “true synaptic type of nervous system” but also because they had “enormous powers of regeneration…under the best conditions one may cut [the worm] into as many as 50 pieces” with each section regenerating “into an intact, fully-functioning organism.” In an early experiment, McConnell trained the worms à la Pavlov by pairing an electric shock with flashing lights. Eventually, the worms recoiled to the light alone. Then something interesting happened when he cut the worms in half. The head of one half of the worm grew a tail and, understandably, retained the memory of its training. Surprisingly, however, the tail, which grew a head and a brain, also retained the memory of its training. If a headless worm can regrow a memory, then where is the memory stored, McConnell wondered. And, if a memory can regenerate, could he transfer it? McConnell’s work has recently experienced a sort of renaissance.Perhaps. Swedish neurobiologist Holger Hydén had suggested, in the 1960s, that memories were stored in neuron cells, specifically in RNA, the messenger molecule that takes instructions from DNA and links up with ribosomes to make proteins, the building blocks of life. McConnell, having become interested in Hydén’s work, scrambled to test for a speculative molecule that he called “memory RNA” by grafting portions of trained planaria onto the bodies of untrained planaria. His aim was to transfer RNA from one worm to another but, encountering difficulty getting the grafts to stick, he turned to a “more spectacular type of tissue transfer, that of ‘cannibalistic ingestion.’” Planaria, accommodatingly, are cannibals, so McConnell merely had to blend trained worms and feed them to their untrained peers. (Planaria lack the acids and enzymes that would completely break down food, so he hoped that some RNA might be integrated into the consuming worms.) Shockingly, McConnell reported that cannibalizing trained worms induced learning in untrained planaria. In other experiments, he trained planaria to run through mazes and even developed a technique for extracting RNA from trained worms in order to inject it into untrained worms in an effort to transmit memories from one animal to another. Eventually, after his retirement in 1988, McConnell faded from view, and his work was relegated to the sidebars of textbooks as a curious but cautionary tale. Many scientists simply assumed that invertebrates like planaria couldn’t be trained, making the dismissal of McConnell’s work easy. McConnell also published some of his studies in his own journal, The Worm Runner’s Digest, alongside sci-fi humor and cartoons. As a result, there wasn’t a lot of interest in attempting to replicate his findings.Nonetheless, McConnell’s work has recently experienced a sort of renaissance, taken up by innovative scientists like Michael Levin, a biologist at Tufts University specializing in limb regeneration, who has reproduced modernized and automated versions of his planarian maze-training experiments. The planarian itself has enjoyed a newfound popularity, too, after Levin cut the tail off a worm and shot a bioelectric current through the incision, provoking the worm to regrow another head in place of its tail (garnering Levin the endearing moniker of “young Frankenstein”). Levin also sent 15 worm pieces into space, with one returning, strangely enough, with two heads (“remarkably,” Levin and his colleagues wrote, “amputating this double-headed worm again, in plain water, resulted again in the double-headed phenotype.”) David Glanzman, a neurobiologist at the University of California, Los Angeles, has another promising research program that recently struck a chord reminiscent of McConnell’s memory experiments—although, instead of planaria, Glanzman’s lab works mostly with aplysia, the darling mollusk of neuroscience on account of its relatively simple nervous system. (Also known as “sea hares,” aplysia are giant, inky sea slugs that swim with undulating, ruffled wings.)In 2015, Glanzman was testing the textbook theory on memory, which holds that memories are stored in synapses, the connective junctions between neurons. His team, attempting to create and erase a memory in aplysia, periodically delivered mild electric shocks to train the mollusk to prolong a reflex, one where it withdraws, upon touch, its siphon, a little breathing tube between the gill and the tail. After training, his lab witnessed new synaptic growth between the sensory neuron that felt touch and the motor neuron that triggered the siphon withdrawal reflex. Developing after the training, the increased connectivity between those neurons seemed to corroborate the theory that memories are stored in synaptic connections. Glanzman’s team tried to erase the memory of the training by dismantling the synaptic connections between the neurons and, sure enough, the snails subsequently behaved as if they’d lost the memory, further corroborating the synaptic memory theory. After Glanzman’s team administered a “reminder” shock to the snails, the researchers were surprised to quickly notice different, newer synaptic connections growing between the neurons. The snails then behaved, once again, as if they remembered the sensitizing training they seemed to have previously forgotten. If the memory persisted through such major synaptic change, where the synaptic connections that emerged through training had disappeared and completely different, newer connections had taken their place, then maybe, Glanzman thought, memories are not really stored in synapses after all. The experiment seems like something out of Eternal Sunshine of the Spotless Mind, a movie in which ex-lovers trying to forget each other undergo a questionable procedure that deletes the memory of a person, but evidently not to the point beyond recall. The lovers both hide a plan deep within their minds to meet in Montauk in the end. The movie suggests, in a way, that memories are never completely lost, that it always remains possible to go back, even to people and places that seem long forgotten.But if memories aren’t stored in synaptic connections, where are they stored instead? Glanzman’s unpopular hypothesis was that they might reside in the nucleus of the neuron cell, where DNA and RNA sequences compose instructions for life processes. DNA sequences are fixed and unchanging, so most of an organism’s adaptability comes from supple epigenetic mechanisms, processes that regulate gene expression in response to environmental cues or pressures, which sometimes involve RNA. If DNA is printed sheet music, RNA-induced epigenetic mechanisms are like improvisational cuts and arrangements that might conduct learning and memory.Perhaps memories reside in epigenetic changes induced by RNA, that improv molecule that scores protein-based adaptations of life. Glanzman’s team went back to their aplysia and trained them over two days to prolong their siphon-withdrawal reflex. They then dissected their nervous systems, extracting RNA involved in forming the memory of their training, and injected it into untrained aplysia, which were tested for learning a day later. Glanzman’s team found that the RNA from trained donors induced learning, while the RNA from untrained donors had no effect. They had transferred a memory, vaguely but surely, from one animal to another, and they had strong evidence that RNA was the memory-transferring agent.Glanzman now believes that synapses are necessary for the activation of a memory, but that the memory is encoded in the nucleus of the neuron through epigenetic changes. “It’s like a pianist without hands,” Glanzman says. “He may know how to play Chopin, but he’d need hands to exercise the memory.” The work of Douglas Blackiston, an Allen Discovery Center scientist at Tufts University, who has studied memory in insects, paints a similar picture. He wanted to know if a butterfly could remember something about its life as a caterpillar, so he exposed caterpillars to the scent of ethyl acetate followed by a mild electric shock. After acquiring an aversion to ethyl acetate, the caterpillars pupated and, after emerging as adult butterflies several weeks later, were tested for memory of their aversive training. Surprisingly, the adult butterflies remembered—but how? The entire caterpillar becomes a cytoplasmic soup before it metamorphosizes into a butterfly. “The remodeling is catastrophic,” Blackiston says. “After all, we’re moving from a crawling machine to a flying machine. Not only the body but the entire brain has to be rewired.”It’s hard to study exactly what goes on during pupation in vivo, but there’s a subset of caterpillar neurons that may persist in what are called “mushroom bodies,” a pair of structures involved in olfaction that many insects have located near their antennae. In other words, some structure remains. “It’s not soup,” Blackiston says. “Well, maybe it’s soup, but it’s chunky.” There’s near complete pruning of neurons during pupation, and the few neurons that remain become disconnected from other neurons, dissolving the synaptic connections between them in the process, until they reconnect with other neurons during the remodeling into the butterfly brain. Like Glanzman, Blackiston employs a hand analogy: “It’s like a small group of neurons were holding hands, but then let go and moved around, finally reconnecting with different neurons in the new brain.” If the memory was stored anywhere, Blackiston suspects it was stored in the subset of neurons located in the mushroom bodies, the only known carryover material from the caterpillar to the butterfly. In the end, despite its whimsical caricature of the science of memory, Eternal Sunshine may have stumbled on a correct premise. Not only do Glanzman and Blackiston believe their experiments harbor hopeful news for Alzheimer’s patients, it also might be possible to repair deteriorated neurons that could, at least theoretically, find their way back to lost memories, perhaps with the guidance of appropriate RNA.Marco Altamirano is a writer based in New Orleans and the author of Time, Technology, and Environment: An Essay on the Philosophy of Nature. Follow him on Twitter @marcosien.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16548_374b03a72295954c3fe1eccb91214071.jpg",
    "title": "Why We Need to Map the Ocean Floor",
    "description": "Posted by Sam Goldman on November 06, 2019  Larry Mayer, a marine geophysicist, gets shivers when he looks at a night sky of stars. He understands why we explore outer space and NASA has spent billions mapping our extraterrestrial…",
    "category": "Biology",
    "content": "Larry Mayer, a marine geophysicist, gets shivers when he looks at a night sky of stars. He understands why we explore outer space and NASA has spent billions mapping our extraterrestrial neighbors—the moon, Mars, Venus. But Mayer also get shivers looking at Earth’s oceans. So why haven’t we invested in mapping its depths? “You’d think you’d be able to convince people willing to spend so many billions of dollars to map Mars to map our own planet,” Mayer says. “And I think it’s wonderful to map Mars, but I’ve asked myself and others that question, and I think part of it is that NASA has a much better PR and outreach machine. A colleague of mine, Bob Ballard, is always saying, “Well, people always look up and think ‘good,’ and they look down and think ‘bad.’”Mayer, a professor and the director of the Center for Coastal and Ocean Mapping at the University of New Hampshire, is now embarked on a journey to make people look down and think good. He’s part of a project called Seabed 2030, which is the result of a partnership between the Nippon Foundation, a Tokyo-based philanthropic organization focusing on maritime issues, and General Bathymetric Chart of the Oceans, the global authority on seafloor topography. Today only 6 percent of ocean floors have been mapped. Mayer, who is co-head of Seabed 2030’s Arctic and North Pacific Ocean Regional Center, and his colleagues aim to map the entire seafloor in high resolution by 2030. Nautilus recently caught up with Mayer to discuss the challenge before him.Why is it important to map the seafloor?The biggest use in shallow water is safety and navigation. When a ship runs aground, that’s a bad day for everybody, including the environment. And if we think about it in a defense sense, safety and navigation also includes underwater navigation, and that’s a deep-sea problem, too. We have to understand what the seafloor looks like before we lay a fiber-optic cable or pipeline. All kinds of maritime heritage—there are many wrecks on the seafloor, with an amazing record of the history of mankind.Sonar’s been giving us very good insight into the distribution of things like deep-sea corals, where we will have high biodiversity, where we’ll have low biodiversity—and all these things we just don’t know about: natural hazards, gas seeps, landslides, things like that. To me, the most exciting reason we map is because we don’t know what’s there. I think I know what I’m going to find, and every time I go out and map with this high resolution, we see new and exciting things—it’s the first step of any kind of exploration.Is there a connection between the seafloor and climate?If we think about climate, it’s driven by differences in heat on the planet: warm equatorial areas versus cold polar regions. And the world is always trying to come to equilibrium; it doesn’t like having places warm and places cold. So in the atmosphere we have Hadley circulation trying to distribute that heat, and that atmospheric circulation drives ocean circulation. The most effective way of distributing the heat [in the oceans] is through these deep-current systems. And if we’re going to appropriately model climate, we have to understand the passageways of these current systems, and we can’t model those current systems without understanding where they’re blocked and where there are passages.We had an experience in the Arctic when we found that there was a new passage, and that totally changed the circulation pattern. It totally changed our understanding of the circulation pattern. And that then changes our models about the transfer of heat. And how about extreme weather events?A tsunami is caused by an earthquake, which will shift the seafloor by a few meters, or by a landslide, which shifts the entire water column just a few meters, if that much. A tsunami wave is actually quite small—it just has a tremendous amount of energy in it, and it travels quite fast across the ocean. But a wave has what we call an orbital motion: The particles are actually turning around in a little circle, and that circle gets smaller and smaller and smaller as you get deeper. And a wave will break when those circles of motion start interacting with the bottom. So a tsunami stays as quite a small wave in deep water until it runs into the coast, into shallow water, where that orbital motion of the particles runs into the bottom and builds it up into a big dangerous wave. There are places that have canyons and places that have shoals; depending on the shape of the seafloor, you can predict where that tsunami will do the most damage in terms of how big it’ll build up, versus where it will dissipate.So to predict where a tsunami is going to do its damage, and where storm surge in a hurricane is going to do damage, we need to understand the shape of the seafloor. How does this mapping technology work?During World War II they had what you call a single-beam echo sounder, which sends out a single pulse of sound that spreads broadly as it leaves the ship and bounces off the seafloor when it finds a change in the properties of the material. And if you have some idea of how fast sound travels in water, which we do, about 1,500 meters per second, you can just count how long it takes to go down and back, divide by two, and you get the depth.Think about a little flashlight, a Maglite or something like that: If you shine it up on the ceiling, even though it starts off as just a little half-inch-circumference light source, it’s several feet wide by the time it gets to the ceiling. It’s a cone of light. The same thing happens with the sound when you point it down, and by the time it gets to the seafloor, it’s been sonifying an area that’s something like half the water depth.In the early 1980s, something called multi-beam sonar came along, and although the analogy is wrong, the result is the same. Instead of that one big circle of light from the Maglite, it’s as if you now had hundreds of tiny little laser beams of light across a narrow, narrow swath. The multi-beam creates a very narrow fan in the direction the ship is going and very wide across the ship. And in that fan are hundreds of what we call beams, each one individually measuring a very small area on the seafloor with very high accuracy. So it’s just an absolute revolution in terms of the ability to resolve features on the seafloor. And the swaths that you get are typically three to five times the water depth. So now if you’re in 4,000 meters of water, you can get 12 kilometers or 15 kilometers all at once, with hundreds of individual depth measurements. A concern I’ve seen regarding this project is that it’s doing the work of resource-extraction companies for them. What are your thoughts on that?I would turn it around. So we shouldn’t have Google Earth because that’s doing the work of resource-extraction companies? But how much good does Google Earth do? I think the good that it does so far outweighs the advantage that it could present to a resource-extraction company—it’s not even close. Should you just blindly go out and have three quarters of our planet unmapped because it may provide a slight advantage to some company? These companies will go out and do their own mapping anyway at the end of the day. What’s the most interesting thing you expect to find?One of the most amazing things to find unexpectedly is often a wreck. There are tons of those around, we don’t know where they are, and we often find them unexpectedly. We’ve found seamounts—4,000-meter-high seamounts that we find when we think nothing is there, and these have impacts on biodiversity and circulation. We find all kinds of interesting structures on the seafloor that we just don’t understand yet. And again we find it looking at it now through a new lens of high resolution.Sam Goldman is a freelance journalist in the San Francisco Bay Area. His work has been published in HuffPost, California Magazine, Medium, Noozhawk, North Gate Radio, and other publications. You can follow him on Twitter @Sam__Goldman. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/51_2838023a778dfaecdc212708f721b788.jpg",
    "title": "Celebrating Science: Co-Sign the Letter",
    "description": "Posted by The Nautilus Team on October 30, 2019  SCIENCE MATTERSMore than ever. Science gave us the light bulb.Science gave us the tractor.Science gave us the vaccine.Science showed us the atom.Science showed us the gene.Science showed…",
    "category": "Ideas",
    "content": "SCIENCE MATTERSMore than ever.Science gave us the light bulb.Science gave us the tractor.Science gave us the vaccine.Science showed us the atom.Science showed us the gene.Science showed us the brain.Science told us about the stars.Science told us about the trees.Science told us about ourselves.Science creates jobs.Science creates solutions.Science creates dreams.How can science be under attack?Because science is about facts.Because science is about reason.Because science can tell you what you don’t want to hear.It’s time to say enough.It’s time to take a stand.It’s time to defend science.Join us by signing this letter. When you do, you become part of a community that invites the world to embrace science. We don’t embrace science because it’s popular, or politically convenient. We embrace and defend science because Curiosity, Exploration, and Discovery are the calling of all people. We defend science because science is for all people.Signed,Please help us spread the message by sharing: "
  },
  {
    "imageUrl": "http://static.nautil.us/16492_67039300a84e6edee2cea5ae4d7eb634.jpg",
    "title": "Physicists Say Google’s Quantum Computer Is Still Far From Practical",
    "description": "Posted by Brian  Gallagher on October 16, 2019  News on the quantum physics grapevine, Frankfurt Institute theoretical physicist Sabine Hossenfelder tells me, is that Google will announce something special next week: Their paper on achieving…",
    "category": "Numbers",
    "content": "News on the quantum physics grapevine, Frankfurt Institute theoretical physicist Sabine Hossenfelder tells me, is that Google will announce something special next week: Their paper on achieving quantum supremacy, the realization of a quantum computer that outdoes its conventional counterpart. This talk of quantum supremacy may sound familiar as, in mid-September, NASA, which contributed to the paper, inadvertently uploaded, then promptly pulled, an outdated version of the paper from its “Technical Reports” server—but not before it made waves on the web. It’s no wonder why: Google and company claim that their quantum computer can do in 200 seconds what it would take a supercomputer 10,000 years to do.The crucial difference between a supercomputer and a quantum computer is the way they store information. For the former it’s a matter, as with any conventional computer, of binary bits, 1s and 0s; for the latter, it’s a matter of quantum bits that can assume any arrangement of 0s and 1s. No, this doesn’t mean that a quantum bit can, like Schrodinger’s cat, be two contradictory things at once—both alive and dead or, in this case, both a 0 and a 1. Rather a quantum bit is, as theoretical computer scientist Scott Aaronson helpfully put it on his blog, “Shtetl-Optimized,” a “complex linear combination of this and that,” or 0s and 1s. “Maybe this and maybe that” comes closest to a rough approximation. “You can then define a [quantum computer],” Aaronson says, “as simply a computer that would exploit this new kind of ‘maybe’: the one that was discovered in the 1920s and involves complex numbers and is out there in the universe.”“Quantum supremacy” is an epoch-making phrase.Why does exploiting this “maybe” grant quantum computers supremacy? “To give you an idea of how much more a quantum computer can do, think about this: One can simulate a quantum computer on a conventional computer just by numerically solving the equations of quantum mechanics,” Hossenfelder explains, in this YouTube video she made several months ago, in anticipation of the Google paper’s release.“If you do that,” she goes on, “then the computational burden on the conventional computer increases exponentially with the number of q-bits that you try to simulate. You can do 2 or 4 q-bits on a personal computer. But already with 50 q-bits you need a cluster of supercomputers. Anything beyond 50 or so q-bits cannot presently be calculated, at least not in any reasonable amount of time.”Google’s quantum computer is a purple chip named Sycamore. In a picture of it in the paper, you can make out a pair of engravings—“Google AI Quantum,” on one side, and “Sycamore” on the other, beneath an engraved sycamore tree. Sycamore was designed to use 54 superconducting qubits, or transmons. I say “designed” because one of the qubits malfunctioned. So the chip, in their experiment comparing its computing speed against that of a “state-of-the-art supercomputer,” used 53, which ended up being fine for the “task of sampling the output of a pseudo-random quantum circuit.” This sort of computation is almost without structure, making it a “suitable choice for benchmarking,” the researchers say, since it renders classical computers rather slow-going. Sycamore’s success, they conclude, “heralds the advent of a much-anticipated computing paradigm.”“Quantum supremacy,” which Caltech theoretical physicist John Preskill, director of the Institute for Quantum Information and Matter, coined in 2012, is an epoch-making phrase. Preskill meant it as a kind of threshold. “I wanted to emphasize that this is a privileged time in the history of our planet, when information technologies based on principles of quantum physics are ascendant,” he wrote, this month, in Quanta. There is a bit of an asterisk, he seems to say, that should append Google’s result. “The catch, as the Google team acknowledges, is that the problem their machine solved with astounding speed was carefully chosen just for the purpose of demonstrating the quantum computer’s superiority,” he wrote. “It is not otherwise a problem of much practical interest.” That’s how Hossenfelder sees it, too. Today’s quantum computers really seem to be just “new toys for scientists,” she says, because the “generation of random variables that can be used to check quantum supremacy is not good [enough] to actually calculate anything useful.” Still, Preskill says what Google’s done amounts to a significant step on the quest to practicality. He thought it would be useful to coin an almost-there phrase “for the era that is now dawning”—“noisy intermediate-scale quantum,” or NISQ. “Noisy” means imprecise. Qubits are still too slippery for error-free computing—the longer a quantum computer runs, the more mistakes it racks up, making the result of a calculation unreliable. “Intermediate-scale” means just big enough to do the sort of thing Google did: demonstrate quantum supremacy—beat a supercomputer handily—but too small, on the order of hundreds of qubits, to do anything valuable. So, we’re firmly in the NISQ (pronounced like “risk”) era, which, as Aaronson wrote in his post, “Scott’s Supreme Quantum Supremacy FAQ!”, is “at least a somethingburger!”It’s nothing to get too excited about yet. “This”—NISQ—“is really a term invented to make investors believe that quantum computing will have practical applications in the next decades or so,” Hossenfelder says. “The trouble with NISQs is that while it is plausible that they soon will be practically feasible, no one knows how to calculate something useful with them.” Perhaps no one ever will. “I am presently quite worried that quantum computing will go the same way as nuclear fusion, that it will remain forever promising but never quite work.”Google’s researchers, and their NASA collaborators, understandably put a more positive spin on this uncertainty. Quantum computing, they say, is “transitioning” from being merely academic to being the key to unlocking new computational powers. “We are only one creative algorithm away from valuable near-term applications.” Who knows when its time will come.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How quantum mechanics has changed our understanding of information. "
  },
  {
    "imageUrl": "http://static.nautil.us/16527_f8327022557b5b3edc49bc7a6b2d4fab.png",
    "title": "Nautilus to be Acquired by Ownership Group of Super-Fans",
    "description": "Posted by The Nautilus Team on October 30, 2019  From the newswire:Award-winning magazine and fast-growing science brand poised for growthAn investor group of super-fans has banded together as a single ownership group to acquire Nautilus,…",
    "category": "Ideas",
    "content": "From the newswire:Award-winning magazine and fast-growing science brand poised for growthAn investor group of super-fans has banded together as a single ownership group to acquire Nautilus, the literary science magazine with more than 10 thousand monthly paying subscribers and an online reach of more than 10 million.The proposed acquisition is subject to receipt of all necessary regulatory and governmental approvals. The proposed deal involves implementation of an all-new digital strategy and website that will roll out over the next year. The proposed new ownership group is planning for aggressive investment in editorial, reader experience, and new products. Subscribers to the beloved print edition of Nautilus will be pleased to hear it will continue uninterrupted.The proposed new ownership group is led by serial media entrepreneur, Daily Dot co-founder and CEO Nicholas White, who will assume the same title at Nautilus.“Nautilus is the best and brightest science brand I know,” said White. “It’s also a love letter to the intersection of science, art, and culture. It will be an honor and privilege to help steward our team and mission. The editorial operation at Nautilus is world class and will remain so. Our proposed plan will put the right growth team in place, which will allow us to deliver Nautilus to the wide audience it so clearly deserves.”Nautilus is a “different kind of science magazine” per the company’s website. It delivers deep, undiluted, narrative storytelling that “brings science into the largest and most important conversations we are having today.”White is joined in the proposed new ownership group by:John Steele will remain as Nautilus’ Publisher and Editorial Director. Kevin Berger will take the helm as Editor-in-Chief and Michael Segal will be Editor-at-Large.Notable contributors to Nautilus have included: Scott Aaronson, Amanda Gefter, Martin Rees, Lisa Randall, Robert Sapolsky, Hope Jahren, Nicholas Carr, Sylvia Earl, Carl Zimmer, B.J. Novak, Alan Lightman, and Cormac McCarthy.“The proposed deal gives Nautilus the resources it needs to continue to support the same kind of vibrant, original writing and reporting that has differentiated it from the very beginning,” said Steele. “Like our staff, contributors, partners and advisors, our new ownership group will be in it for the long haul, and for all the right reasons. Together we will work even harder to expand the public’s knowledge and understanding of fundamental questions of scientific inquiry, as well as their connection to human culture.”Founded in 2013, Nautilus was the first magazine ever to win two National Magazine Awards in its first year of eligibility. It also won the Webby for best science website, two AAAS Kavli Science Journalism Awards, was named one of the World’s Best-Designed news sites by the Society for News Design, and has enjoyed dozens of other honors.“Science needs all of the love it can get. It deserves everything we have to give,” said Jones-Dilworth. “We want everyone to understand the fundamental scientific questions of our day, and appreciate how advances both large and small positively impact daily life. All of us in the proposed ownership group share this drive. Nautilus is essential today. If you’re not already a subscriber, you really need to be.”Nautilus is currently a project of NautilusThink, a New York not-for-profit corporation, which will continue to run programs to advance interest in the sciences following the proposed acquisition.In conjunction with the proposed acquisition, Nautilus also published a public letter titled “Science Matters.”The letter is a public commitment by the Nautilus team, its staff, advisors, and its contributors; leading thinkers, researchers, teachers, and businesspeople; and the public at large to tirelessly advance the cause of science in America and around the world.Anyone and everyone is encouraged to read the letter, add their name, and share it with their community at: http://nautil.us/blog/celebrating-science-co_sign-the-letterAbout NautilusNautilus is science, connected. Online, in print and in the classroom, Nautilus challenges the reader to consider the deep, mysterious connecting tissue that runs through the sciences and connects them to philosophy, culture, and art. It reminds us that we are all interested in the same narratives as human beings: Who are we, what is the world we find ourselves in, and where does meaning come from? Nautilus make subtexts, hidden meanings, and fundamental truths shine through narrative that challenges the usual boundaries in science media and education.Nautilus lets science spill over its usual borders. Learn more at http://nautil.us "
  },
  {
    "imageUrl": "http://static.nautil.us/16521_ace668d845c284b9352de506cb046628.jpg",
    "title": "Is the Psychology of Greta Thunberg’s Climate Activism Effective?",
    "description": "Posted by Scott Koenig on October 26, 2019  Last month, Greta Thunberg, the Swedish teenage activist, excoriated world leaders for their ongoing failure to address the climate crisis. “You have stolen my dreams and my childhood…",
    "category": "Culture",
    "content": "Last month, Greta Thunberg, the Swedish teenage activist, excoriated world leaders for their ongoing failure to address the climate crisis. “You have stolen my dreams and my childhood with your empty words,” she said at one point during her speech at the United Nations. Thunberg has been galvanizing public support for climate action since rising to prominence with her school strike about a year ago, and her latest remarks are no exception. They’ve attracted millions of views all around the internet—and nearly as many strong opinions. The praise and scorn she received in the aftermath of her address spotlights not only the power and intricacy of moral language, but also its ability, when articulated in a sound argument, to change public opinion on contentious moral and political issues. Seeing the reactions to Thunberg’s speech, Frederic Hopp, a graduate student in the Media Neuroscience Lab at the University of California, Santa Barbara, decided, with the lab’s director, René Weber, to give it a close read using a tool the lab developed that combines algorithms, text-mining, and human evaluations. “All humans possess inborn moral intuitions that can be categorized along five broad categories,” Weber has said. “The environment and cultural context influence how these capacities become relevant.” The five categories, or moral foundations, are care/harm (feeling compassion for the suffering and vulnerable), fairness/cheating (making sure people are getting what they deserve), loyalty/betrayal (keeping track of who is “us” and who is “them”), authority/subversion (valuing order, tradition, and hierarchy), and sanctity/degradation (believing certain things are elevated and pure and shouldn’t be tarnished). Hopp told me, “There’s good empirical evidence that you are more or less successful in persuading people not just for climate change but other issues, too, depending on how you frame these arguments in moral terms and which foundations you stress.”It seems Thunberg struck the right chord.Many studies have shown that people who prioritize issues of care/harm and fairness/cheating tend to be politically liberal. On the other hand, people who value all five foundations similarly tend to be politically conservative. These trends, far from being fodder for philosophical conjecture, translate to real-world behavior. One study found that the composition of people’s moral intuitions could predict which candidate they supported in the 2016 presidential primaries. Another showed that it’s possible to make liberals more supportive of conservative positions, like increasing military spending, and conservatives more supportive of liberal positions, like legalizing same-sex marriage, by changing the moral language used to support each position. Perhaps unsurprisingly, given that environmentalists tend to be liberal, Hopp and Weber found Thunberg’s speech loaded with language related to care/harm (like “growth,” “future,” and “suffering”) and fairness/cheating (like “right,” “solutions,” and “consequences”). It seems Thunberg struck the right chord. Though they’re more prominent for liberals, those two moral foundations appeal strongly to people on both sides of the aisle. This means moral arguments that tap into them are, according to Irina Vartanova, a researcher at the Institute for Futures Studies, “the universal arguments, the arguments that are accepted by everyone.”In a recent paper, Vartanova and her colleagues showed just how powerful universal arguments can be. Using data from a survey that tracked public opinion in the United States on 74 different moral issues from 1972 to 2016, they created a measure of how strongly common arguments (such as, “We should limit carbon emissions because people are suffering”) connect to the care and fairness foundations compared to opposing arguments (such as, ”We shouldn’t limit carbon emissions because it would impede economic growth”). If a given argument is rooted more deeply in care and fairness than its counterargument, it has what the researchers call a “harm-fairness connection advantage.”A model of public opinion change they developed not only reaffirmed that public opinion is becoming more liberal on many issues, but also showed that the rate of this change depends largely on the strength of the harm-fairness connection advantage: the bigger the advantage for a given argument, the faster the leftward shift among liberals and conservatives alike. The model offers, as the researchers wrote, “an explanation for why gay rights, gender equality, and racial equality are gaining support faster than opinions in favor of abortion rights, affirmative action, and suicide, for which harm-and-fairness considerations are much less clear-cut.” Vartanova and her colleagues seem to have shown how exposure to the right arguments can predictably change not just individual opinions, but the belief system of an entire society: As they put it, “Our model illustrates that psychology can create culture.”Of course, this assumes that people can freely exchange ideas. For the harm-fairness connection advantage to move public opinion, “you need to deploy effective arguments, and you also need to make sure that the questions are being discussed,” said Pontus Strimling, a professor of economics at the Institute for Futures Studies and lead author of the study. “Another part of this whole movement is because we’re talking about these issues. This process can’t work unless people are actually discussing it.” So, in countries like China and Russia—and even in ostensible bastions of free speech like the United States—strong moral arguments don’t always win out.Still, this research goes to show that social change doesn’t just materialize on its own. The widespread belief that, as Martin Luther King put it, “the arc of the moral universe is long, but it bends toward justice” has attracted criticism from those who believe that it mistakenly treats change as an inevitability. “Nothing bends toward justice without us bending it,” wrote political commentator Chris Hayes. At least for those who lean liberal, it seems moral arguments are a crucial part of that bending.So who actually gets to do the bending? The importance of moral arguments means the greatest catalysts of—or obstacles to—social change may be those who decide which arguments are disseminated. “We can’t get around the role of moral foundations in society,” said Matt Miles, a professor of political science at Brigham Young University—Idaho. “The question is who is it that’s going to connect the dots between the moral foundations and the policy. Those people end up being the powerful people in society.”Thunberg has a large platform, and her attempts to connect the dots do appear capable of swaying opinions. But it remains to be seen whether harm- and fairness-based language is the most effective way to motivate climate policy. The optimal set of moral arguments for environmentalism may not happen to be the two that reliably drive change on other issues. “The positions really have their own logic as to what fits them,” said Strimling. “You can stretch it a bit but not that much. You’re kind of stuck with the arguments that fit your position.” For example, some research suggests that environmentalists may benefit from using arguments based on the foundation of sanctity/degradation.One way or another, the needle is moving. An increasing percentage of Americans—44 percent, according to Pew, up from 34 percent in 2015—believe addressing the climate should be among their government’s top priorities. And Thunberg has certainly done more than her fair share of the work. But as the dangers of the climate crisis become less and less hypothetical, activists may want to use all the rhetorical tools they can get. Thunberg, for her part, may have weighted her remarks toward care and fairness, but she didn’t omit loyalty. “You are failing us,” she said. “But the young people are starting to understand your betrayal.”Scott Koenig is a doctoral student in neuroscience at CUNY, where he studies morality, emotion, and psychopathy. Follow him on Twitter @scotttkoenig.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16540_e4cd3a630ddf5462537c9bfb9e8ab0ec.jpg",
    "title": "How the Neutrino’s Tiny Mass Could Help Solve Big Mysteries",
    "description": "Posted by Marcus Woo on November 01, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Of all the known particles in the universe, only photons outnumber neutrinos. Despite their abundance, however, neutrinos…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Of all the known particles in the universe, only photons outnumber neutrinos. Despite their abundance, however, neutrinos are hard to catch and inspect, as they interact with matter only very weakly. About a thousand trillion of the ghostly particles pass through your body every second—with nary a flinch from even a single atom.“The fact that they’re ubiquitous, yet we don’t even know what they weigh, is kind of crazy,” said Deborah Harris, a physicist at the Fermi National Accelerator Laboratory near Chicago and York University in Toronto.Physicists have long tried to weigh the ghost. And in September, after 18 years of planning, building and calibrating, the Karlsruhe Tritium Neutrino (KATRIN) experiment in southwestern Germany announced its first results: It found that the neutrino can’t weigh more than 1.1 electron-volts (eV), or about one-five-hundred-thousandth the mass of the electron.“When I was in grad school, my textbooks all said neutrinos didn’t have mass,” Harris said.This initial estimate, from only one month’s worth of data, improves on previous measurements using similar techniques that placed the upper limit on the neutrino mass at 2 eV. As its data accrues, KATRIN aims to nail the actual mass rather than giving an upper bound.Mass is one of the most basic and important characteristics of fundamental particles. The neutrino is the only known particle whose mass remains a mystery. Measuring its mass would help point toward new laws of physics beyond the Standard Model, the remarkably successful yet incomplete description for how the universe’s known particles and forces interact. Its measured mass would also serve as a check on cosmologists’ theories for how the universe evolved.“Depending on what the mass of the neutrino turns out to be, it may lead to very exciting times in cosmology,” said Diana Parno, a physicist at Carnegie Mellon University and a member of the KATRIN team.Until about two decades ago, neutrinos—which were theoretically predicted in 1930 and discovered in 1956—were presumed to be massless. “When I was in grad school, my textbooks all said neutrinos didn’t have mass,” Harris said.That changed when, in a discovery that would win the 2015 Nobel Prize, physicists found that neutrinos could morph from one kind to another, oscillating between three “flavor” states: electron, muon and tau. These oscillations can only happen if neutrinos also have three possible mass states, where each flavor has distinct probabilities of being in each of the three mass states. The mass states travel through space differently, so by the time a neutrino goes from point A to point B, this mix of probabilities will have changed, and a detector could measure a different flavor.If there are three different mass states, then they can’t all be zero—thus, neutrinos have mass. According to recent neutrino oscillation data (which reveals the differences between the mass states rather than their actual values), if the lightest mass state is zero, the heaviest must be at least 0.0495 eV.Still, that’s so light compared to the mass of other particles that physicists aren’t sure how neutrinos get such tiny masses. Other particles in the Standard Model acquire mass by interacting with the Higgs field, a field of energy that fills all space and drags on massive particles. But for neutrinos, “the mass is so small, you need some additional theory to explain that,” Parno said.Figuring out how neutrinos acquire mass may resolve other, seemingly related mysteries, such as why there is more matter than antimatter in the universe. Competing theories for the mass-generating mechanism predict different values for the three mass states. While neutrino oscillation experiments have measured the differences between the mass states, experiments like KATRIN home in on a kind of average of the three. Combining the two types of measurements can reveal the value of each mass state, favoring certain theories of neutrino mass over others.Neutrino mass is also of cosmic importance. Despite their minuscule mass, so many neutrinos were born during the Big Bang that their collective gravity influenced how all the matter in the universe clumped together into stars and galaxies. About a second after the Big Bang, neutrinos were flying around at almost light speed—so fast that they escaped the gravitational pull of other matter. But then they started to slow, which enabled them to help corral atoms, stars and galaxies. The point at which neutrinos began to slow down depends on their mass. Heavier neutrinos would have decelerated sooner and helped make the universe clumpier.By measuring the cosmic clumpiness, cosmologists can infer the neutrino’s mass. But this indirect method hinges on the assumption that models of the cosmos are correct, so if it gives a different answer than direct measurements of the neutrino mass, this might indicate that cosmological theories are wrong.So far, the indirect cosmological approach has been more sensitive than direct mass measurements by experiments like KATRIN. Recent cosmological data from the Planck satellite suggests that the sum of the three neutrino mass states can’t be greater than 0.12 eV, and in August, another analysis of cosmological observations found that the lightest mass must be less than 0.086 eV. These all fall well below KATRIN’s upper bound, so there’s no contradiction between the two approaches yet. But as KATRIN collects more data, discrepancies could arise.The long-awaited KATRIN experiment weighs neutrinos by using tritium, a heavy isotope of hydrogen. When tritium undergoes beta decay, its nucleus emits an electron and an electron-flavored neutrino. By measuring the energy of the most energetic electrons, physicists can deduce the energy—and thus the mass (or really, a weighted average of the three contributing masses)—of the electron neutrino.If KATRIN finds a mass of around 0.2 or 0.3 eV, cosmologists will have a hard time reconciling their observations, said Marilena Loverde, a cosmologist at Stony Brook University. One possible explanation would be some new phenomenon that causes the cosmological influence of the neutrino’s mass to wane over time. For instance, maybe the neutrino decays into even lighter unknown particles, whose near-light speeds render them incapable of clumping matter together. Or maybe the mechanism that gives mass to neutrinos has changed over cosmic history.If, on the other hand, the neutrino mass is close to what cosmological observations predict, KATRIN won’t be sensitive enough to measure it. It can only weigh neutrinos down to 0.2 eV. If neutrinos are lighter than that, physicists will need more sensitive experiments to close in on its mass and resolve the particle physics and cosmology questions. Three potentially more sensitive projects — Project 8, Electron Capture on Holmium, and HOLMES—are already taking data with proof-of-concept instruments.Marcus Woo is a science journalist based in the San Francisco Bay Area. His work has appeared in WIRED, New Scientist, National Geographic, Smithsonian, NPR, the BBC, and other publications.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16567_06b44f22bf01b4bad31391ffe00009c4.jpg",
    "title": "The Simple Dutch Cure for Stress",
    "description": "Posted by Alice Fleerackers on November 08, 2019  Recently I was in San Francisco, a city known for its tech companies, steep hills, and fierce winds. Each day I’d run around the neighborhood and up through the park, ending with a spectacular…",
    "category": "Biology",
    "content": "Recently I was in San Francisco, a city known for its tech companies, steep hills, and fierce winds. Each day I’d run around the neighborhood and up through the park, ending with a spectacular view of the Golden Gate Bridge. Back in my AirBnB, I’d feel energized and refreshed, fingers tingling from the breeze. It was cold, exhausting, but completely exhilarating. As it turns out, there’s a unique term, from the Dutch, for this sort of pastime. In the Netherlands, people have been seeking out windy exercise for more than a hundred years. Today, the practice is so common that it’s known as “uitwaaien.” It “literally translates to ‘outblowing,’” explains Caitlin Meyer, a lecturer at the University of Amsterdam’s Department of Dutch Linguistics. “It’s basically the activity of spending time in the wind, usually by going for a walk or a bike ride.” Meyer has lived in the Netherlands for more than 20 years and has come to specialize in the language, despite being a non-native speaker. She says uitwaaien is a popular activity where she lives—one believed to have important psychological benefits. “Uitwaaien is something you do to clear your mind and feel refreshed—out with the bad air, in with the good,” she tells me. “It’s seen as a pleasant, easy, and relaxing experience—a way to destress or escape from daily life.”So open that calendar app and note some time for uitwaaien.A growing body of evidence suggests that Dutch speakers may be onto something. “Pretty well every group of people benefits from being outdoors in the presence of nature,” says Jules Pretty, Professor of Environment and Society at the University of Essex. “It takes us out of the stresses and anxieties of the rest of life.” Over the last 15 years, he’s explored how a range of outdoor activities affect human psychology, including walking, cycling, and even farming. He’s found that people from all walks of life can increase their well-being after spending as little as five minutes amid natural settings, with positive impacts on sense of self-worth, mood, and sense of identity. Other researchers have found similar results, linking activities like nature walks with reduced levels of depression, perceived stress, and negative emotions. Some research goes even further, reporting that walking in nature can help reduce headaches, improve immune function, and even, as in the case of the famous forest-bathing studies, increase anticancer protein production. While research into the benefits of waterscapes isn’t as well-established, evidence suggests these “blue spaces” may be equally—or perhaps even more—beneficial to mental well-being. For example, people who live closer to the coast, like many Netherlanders do, report better physical and psychological health than those farther inland. Water may have a restorative effect, helping people overcome negative emotions and diminish their mental distress. Apparently, when it comes to relaxation and recovery, a little “outblowing” at the beach might be just what the doctor ordered. There are lots of theories about why spending time in nature might be so good for us. Some researchers, like Qing Li, a physician at Nippon Medical School Hospital and the President of the Japanese Society of Forest Medicine, believe the answer may literally be blowing in the wind. He and his team have spent years studying the effects of phytoncides, antibacterial and antimicrobial substances that trees and other plants release into the air to help them fight diseases and harmful organisms. When humans breathe in these substances—typically by spending time in nature—their health can improve. Across several studies, phytoncides have been shown to boost immune function, increase anticancer protein production, reduce stress hormones, improve mood, and help people relax. Pretty attributes the restorative power of natural spaces to their immersive quality. He tells me that activities like watching shorebirds or collecting seashells on the beach can be really engaging—so engaging that they can help us temporarily deactivate a part of the brain, located in our prefrontal cortex, called the default mode network, which allows us to scheme, plan, and innovate. “It’s what makes us brilliant humans,” Pretty says. The trade-off is that it’s also extremely active. “The one thing that we haven’t got is an off-switch for our thoughts,” he says. As a result, many of us “find ourselves living our lives on simmer—[like we’ve] got a pot on the stove that’s almost ready to boil.” In the long-term, this constant low-grade stress can damage our health and well-being, increasing our chances of cardiovascular diseases, inflammation-related issues, and other dangers. That’s why Pretty believes a regular “dose” of something akin to uitwaaien can be so beneficial. In our over-stressed society, listening to the sound of the wind or admiring the colors of ocean waves may be among the few ways we can truly unwind. “We just need a name for it, an encouragement for people to undertake it and then to carry on doing it.” Uitwaaien, being a difficult-to-translate word, may be perfectly suited to the task. “One of the main functions of language is to map our experiences of the world around us,” says Tim Lomas, a Senior Lecturer in Positive Psychology at the University of East London. The more nuanced the vocabulary we have available to describe something, in other words, the more detailed the map we can create of it. “One way to look at untranslatable words is that they’re mapping a part of the world that our own language doesn’t map,” says Lomas. By learning what these words mean, we may be able to access feelings or experiences that we wouldn’t otherwise. “There will be experiences that aren’t captured by our own language,” he says. “And for that we can learn from other languages.” Learning to use words from other languages regularly can be a challenge, of course. “Even though I think it’s very useful for people to engage with words from other cultures, it’s still hard,” he says. He adds that if people really want to incorporate an activity like uitwaaien into their lives, they need to “work on practicing it, and then get better at experiencing it and cultivating it.” Pretty agrees. “Go out at lunchtime and take a break,” he says. “Park a bit further away [from the office] and walk for five minutes.” Whatever your lifestyle, he says, look at your schedule, and ask yourself the simple question: “How you can fit in small amounts of exposure to nature?” So open that calendar app and note some time for uitwaaien. Whether it’s a windy, riverside bike ride or a jog up a steep San Francisco hill, chances are, your mind—not to mention your body—will thank you for it. Alice Fleerackers is a freelance writer and a doctoral student at Simon Fraser University, where she studies how controversial science is communicated in the digital sphere. Find her on Twitter @FleerackersA.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16489_d758d419023e5a228ec696e53c735f17.jpg",
    "title": "The Problem with the Frozen Poop Knife Study",
    "description": "Posted by Wade Davis on October 12, 2019  When, some weeks ago, I was first contacted by an online scientific publication asking me to review a submission on the subject of “shit knives”, I initially thought it was a hoax or…",
    "category": "Culture",
    "content": "When, some weeks ago, I was first contacted by an online scientific publication asking me to review a submission on the subject of “shit knives”, I initially thought it was a hoax or some kind of practical joke. I had in mind the deliberately nonsensical papers written by Peter Boghossian, Helen Pluckrose and James Lindsay that were accepted for publication by academic journals such as Gender, Place and Culture. That so many editors fell for the ruse embarassed academics in a number of disciplines, especially once the scandal broke in the Wall Street Journal in October, 2018.On the face of it, I couldn’t believe that a team of scholars would take on the “shit knife” as a scientific challenge. I meant no disrespect, but at a time when the entire world of the Inuit is literally melting beneath them, I found it hard to accept that any serious scholar, even the most reductionist, would exhaust time and money in such a pursuit. My interest was piqued when I learned from a reporter at Discover Magazine that the experiments were real, with results that had been published in the Journal of Archaeological Science.For the record, I’ve never published in any academic or scientific journal a claim that Inuit people regularly employ tools made from human excrement. I have in several popular essays and talks repeated a story that I recorded at the tip of Baffin Island from a hunter, Olayuk Narqitarvik. In doing so, I’ve always recalled my sense at the time that it was a classic case of local people having some fun with a visiting tourist/anthropologist.I’ve shared the “shit knife” story in public because it helps audiences grasp and understand the extent to which the Inuit are truly a people of the ice.This said, I’ve also cited a curious comment from Peter Freuchen as he travelled with Knud Rasmussen on the Fifth Thule expedition. He recounts a time when, seeking shelter from a storm, he became trapped in a coffin of his own making, beneath his sled encased in ice. In his journal, he recalls very casually that as he struggled to escape, he thought of making a shit knife. Eren et al quote Freuchan in their paper. “I moved my bowels and from the excrement I managed to fashion a chisel-like instrument which I left to freeze… At last I decided to try my chisel and it worked” (Freuchen, 1953: 179).Coming from one of the most accomplished of polar explorers, the life long partner of Knud Rassmussen, arguably the most perceptive and knowledgable scholar in the history of Arctic ethnography, this account certainly deserves attention. Still, given the distinct possibility that the anecdote might be used to denigrate the Inuit way of life, I’ve always presented the story as a metaphor, always prefacing my comments with a humorous denial that I was suggesting the existence of some kind of assembly line making such implements.Here, for example, is a passage from my 2009 CBC Massey lectures, which appeared in bookform as The Wayfinders: Why Ancient Wisdom Matters in the Modern World. This excerpt is from pages 206-7.Fearful for his life, his family took away all of his tools and weapons, thinking this would oblige him to leave the land. Instead, in the midst of a winter storm, he stepped out of their igloo, defecated, and honed the feces into a frozen blade, which he sharpened with a spray of saliva. With this knife, forged by the cold from human waste, he killed a dog. Using its rib cage as a sled and its hide to harness another dog, he disappeared into the darkness. This story may well be apocryphal, though I did find a reference to just such an implement in the Arctic journals of the Danish explorer Peter Freuchen. But true or not, it is a wonderful symbol of the ingenuity and resilience of the Inuit people, traits of culture that have allowed them to survive.The entire point of the anecdote, a procedure that I’ve never claimed to have witnessed or been able to authenticate, is to remind audiences and readers that the Inuit do not fear the cold, they take advantage of it. This is indisputably true. A moist towel left out over night becomes a shovel by dawn. The runners of their sleds were made of fish, the cross bars of walrus meat. Freuchen quipped that if you ever run out of food in the Arctic you could always eat your sled. I’ve shared the “shit knife” story in public because it helps audiences grasp and understand the extent to which the Inuit are truly a people of the ice. As hunters they depend on it for their survival, even as it inspires the very essence of their character and culture. The writer Gretel Ehrlich, who lived eight years among the Polar Eskimo in Greenland, suggests that it is the nature of ice, the way it moves, recedes, dissolves, and reforms with the seasons, that gives such flexibility to the Inuit heart and spirit. It follows that the melting and recession of the ice in a warming world represents for the Inuit not only a profound adaptive challenge, but also an overwhelming existential and psychological crisis. This is the essential point. The Inuit, who played no role in the creation of the climate crisis, are among those most directly and catastrophically suffering the consequences.Setting aside my motivations, let’s consider the report by Eren et al strictly on its scientific merits. The authors to their credit are forthright about their methods and make no claim to have been able to replicate the conditions of the Arctic. They nevertheless appear to have overlooked a fundamental flaw in their experimental design. The story I first heard from Olayuk Narqitarvik told of his grandfather having used an implement made from frozen human excrement to kill and skin a dog. In their experiments, Eren et al inexplicably tested their implements on the skin of a pig. Anyone who has sliced slabs from a side of bacon or gnawed on rind knows that pig skin and the skin of a dog are hardly equivalent. Olayuk did not say his grandfather used such an improvised tool to kill a walrus. Freuchan only claimed to have used a “shit knife” to punch through hardened snow, and he did so in the severe cold of the Arctic, not in the relative warmth (50 degrees F as noted by the authors) of a university laboratory.What accounts for this blindspot? Was it fear of PETA and animal rights activists who don’t mind the experimental use of pigs, but draw the line at dogs? I suspect that it was, and I fully understand and sympathize with the challenge this presented to Eren and his colleagues.Still, if true, it is surely ironic that a team of scientists, invoking the rigour of the scientific method, took on this challenge, as if operating in the realm of pure reason, while all the while they remained confined by their own cultural constraints, condemned to conduct an experiment that, by definition, betrayed the principles of objectivity that their science so earnestly proclaims.In sharing these thoughts, it’s not my intention to challenge the results or belittle the efforts of the team that conducted the experiments reported in Eren et al. But surely if one wants to invoke the scientific method, and publish results as being conclusive, one must begin with a research protocol free of flaws that by definition limit the utility of your results and thus compromise your conclusions. In an experiment designed to test whether a tool forged by the cold from human waste could be used to kill a dog, surely the frozen implements created in the lab ought to have been tested on the skin of a dog.Wade Davis is the Leadership Chair in Cultures and Ecosystems at Risk and Professor of Anthropology at the University of British Columbia.\n\tThe newest and most popular articles delivered right to your inbox!\nNotesDavis, W., 2009 The Wayfinders, House of Anansi, TorontoFreuchen, P., 1953. Vagrant Viking: My Life and Adventures. J. Messner, Inc, New York. Melchior, Jillian Kay “Fake news comes to academia” The Wall Street Journal, October 5, 2018Eren, M. I., Bebber, M. R., Norris, J.D., Perrone, A., Rutkoski, A., Mary, M.W., Raghanti, A., “Experimental replication shows knives manufactured from frozen human feces do not work,” Journal of Archaeological Science: Reports 27 (2019) 102002 "
  },
  {
    "imageUrl": "http://static.nautil.us/16511_e58a9052057eab54f8b49e8c553d1837.png",
    "title": "Omniviolence Is Coming and the World Isn’t Ready",
    "description": "Posted by Phil Torres on October 21, 2019  In The Future of Violence, Benjamin Wittes and Gabriella Blum discuss a disturbing hypothetical scenario. A lone actor in Nigeria, “home to a great deal of spamming and online fraud activity,”…",
    "category": "Culture",
    "content": "In The Future of Violence, Benjamin Wittes and Gabriella Blum discuss a disturbing hypothetical scenario. A lone actor in Nigeria, “home to a great deal of spamming and online fraud activity,” tricks women and teenage girls into downloading malware that enables him to monitor and record their activity, for the purposes of blackmail. The real story involved a California man who the FBI eventually caught and sent to prison for six years, but if he had been elsewhere in the world he might have gotten away with it. Many countries, as Wittes and Blum note, “have neither the will nor the means to monitor cybercrime, prosecute offenders, or extradite suspects to the United States.” Technology is, in other words, enabling criminals to target anyone anywhere and, due to democratization, increasingly at scale. Emerging bio-, nano-, and cyber-technologies are becoming more and more accessible. The political scientist Daniel Deudney has a word for what can result: “omniviolence.” The ratio of killers to killed, or “K/K ratio,” is falling. For example, computer scientist Stuart Russell has vividly described how a small group of malicious agents might engage in omniviolence: “A very, very small quadcopter, one inch in diameter can carry a one-or two-gram shaped charge,” he says. “You can order them from a drone manufacturer in China. You can program the code to say: ‘Here are thousands of photographs of the kinds of things I want to target.’ A one-gram shaped charge can punch a hole in nine millimeters of steel, so presumably you can also punch a hole in someone’s head. You can fit about three million of those in a semi-tractor-trailer. You can drive up I-95 with three trucks and have 10 million weapons attacking New York City. They don’t have to be very effective, only 5 or 10% of them have to find the target.” Manufacturers will be producing millions of these drones, available for purchase just as with guns now, Russell points out, “except millions of guns don’t matter unless you have a million soldiers. You need only three guys to write the program and launch.” In this scenario, the K/K ratio could be perhaps 3/1,000,000, assuming a 10-percent accuracy and only a single one-gram shaped charge per drone. Will emerging technologies make the state system obsolete? It’s hard to see why not.That’s completely—and horrifyingly—unprecedented. The terrorist or psychopath of the future, however, will have not just the Internet or drones—called “slaughterbots” in this video from the Future of Life Institute—but also synthetic biology, nanotechnology, and advanced AI systems at their disposal. These tools make wreaking havoc across international borders trivial, which raises the question: Will emerging technologies make the state system obsolete? It’s hard to see why not. What justifies the existence of the state, English philosopher Thomas Hobbes argued, is a “social contract.” People give up certain freedoms in exchange for state-provided security, whereby the state acts as a neutral “referee” that can intervene when people get into disputes, punish people who steal and murder, and enforce contracts signed by parties with competing interests. The trouble is that if anyone anywhere can attack anyone anywhere else, then states will become—and are becoming—unable to satisfy their primary duty as referee. It’s a trend toward anarchy, “the war of all against all,” as Hobbes put it—in other words a condition of everyone living in constant fear of being harmed by their neighbors. Indeed, in a recent paper, “The Vulnerable World Hypothesis,” published in Global Policy, the Oxford philosopher Nick Bostrom argues that the only way to defend against a global catastrophe is to employ a universal and invasive surveillance system, what he calls a “High-tech Panopticon.” Sound dystopian? It sure does to me. “Creating and operating the High-tech Panopticon would require substantial investment,” Bostrom writes, “but thanks to the falling price of cameras, data transmission, storage, and computing, and the rapid advances in AI-enabled content analysis, it may soon become both technologically feasible and affordable.” Bostrom is well-aware of the downsides—corrupt actors in a state could exploit this surveillance for totalitarian ends, or hackers could blackmail unsuspecting victims. Yet the fact is that it may still be a better option than suffering one global catastrophe after another. How can societies counterattack omniviolence? One strategy could be a superintelligent machine—essentially, an extremely powerful algorithm—that’s specifically designed to govern fairly. We could then put the algorithm in political charge and, insofar as it governs as something like a “Philosopher King,” not worry constantly about the data collected being misused or abused. Of course, this is a fantastical proposal. Even the real-world use of AI in the justice system is fraught with problems. But at this point, do we have a better idea for preventing the collapse of the state system under the weight of widespread technological empowerment?Perhaps a completely new idea will emerge that can preserve the current system—if we even want it preserved. Or perhaps emerging technologies won’t empower people as much as I and others anticipate. It could be that offensive technologies will actually lag behind defensive technologies, making it very difficult to execute a successful attack. It could also be that before omniviolence and democratization undercut the state, civilization collapses because of climate change-linked stressors like lethal heatwaves, megadroughts, coastal flooding, rising sea-levels, melting glaciers and polar ice caps, desertification, food supply disruptions, disease outbreaks, biodiversity loss, species extinctions, and mass migrations. If we ended up living as hunter-gatherers again, the main worry would be sticks and stones, not designer pathogens and artificial intelligence.Civilization is an experiment. We may not get the results we’re expecting. So humanity would do well to hope for the best but prepare for the worst.Phil Torres is a scholar of global catastrophic risks, and author of several books. His essay, “Superintelligence and the Future of Governance: On Prioritizing the Control Problem at the End of History,” appears in the 2018 anthology, Artificial Intelligence Safety and Security. His articles have been published in TIME, Slate, Nautilus, Motherboard, and the Bulletin of the Atomic Scientists. Follow him on Twitter @xriskology.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16579_1210c03fd21f9347c7e39edc2740fd4e.jpg",
    "title": " How to Give Mars an Atmosphere, Maybe",
    "description": "Posted by Marc Kaufman on November 15, 2019  Earth is most fortunate to have vast webs of magnetic fields surrounding it. Without them, much of our atmosphere would have been gradually torn away by powerful solar winds long ago, making…",
    "category": "Matter",
    "content": "Earth is most fortunate to have vast webs of magnetic fields surrounding it. Without them, much of our atmosphere would have been gradually torn away by powerful solar winds long ago, making it unlikely that anything like us would be here.Scientists know that Mars once supported prominent magnetic fields as well, most likely in the early period of its history when the planet was consequently warmer and much wetter. Very little of them is left, and the planet is frigid and desiccated. These understandings lead to an interesting question: If Mars had a functioning magnetosphere to protect it from those solar winds, could it once again develop a thicker atmosphere, warmer climate, and liquid surface water?James Green, director of NASA’s Planetary Science Division, thinks it could. And perhaps with our help, such changes could occur within a human, rather than an astronomical, time frame.In a talk at the NASA Planetary Science Vision 2050 Workshop at the agency’s headquarters, Green presented simulations, models, and early thinking about how a Martian magnetic field might be re-constituted and how the climate on Mars could then become more friendly for human exploration and, perhaps, communities.It consisted of creating a “magnetic shield” to protect the planet from those high-energy solar particles. The shield structure would consist of a large dipole—a closed electric circuit powerful enough to generate an artificial magnetic field. Simulations showed that a shield of this sort would leave Mars in the relatively protected magnetotail of the magnetic field created by the object. A potential result: an end to large-scale stripping of the Martian atmosphere by the solar wind, and a significant change in climate.“The solar system is ours, let’s take it,” Green told the workshop. “And that, of course, includes Mars. But for humans to be able to explore Mars, together with us doing science, we need a better environment.”Is this “terraforming,” the process by which humans make Mars more suitable for human habitation? That’s an intriguing but controversial idea that has been around for decades, and Green was wary of embracing it fully.“My understanding of terraforming is the deliberate addition, by humans, of directly adding gases to the atmosphere on a planetary scale,” he wrote in an email. “I may be splitting hairs here, but nothing is introduced to the atmosphere in my simulations that Mars doesn’t create itself. In effect, this concept simply accelerates a natural process that would most likely occur over a much longer period of time.”What he is referring to here is that many experts believe Mars will be a lot warmer in the future, and will have a much thicker atmosphere, whatever humans do. On its own, however, the process will take a very long time.A relatively small change in atmospheric pressure can stop an astronaut’s blood from boiling.To explain further, first a little Mars history.More than 3.5 billion years ago, Mars had a much thicker atmosphere that kept the surface temperatures moderate enough to allow for substantial amounts of surface water to flow, pool, and perhaps even form an ocean. (And who knows, maybe even for life to begin.) But since the magnetic field of Mars fell apart after its iron inner core was somehow undone, about 90 percent of the Martian atmosphere was stripped away by charged particles in that solar wind, which can reach speeds of 250 to 750 kilometers per second.Mars, of course, is frigid and dry now, but Green said the dynamics of the solar system point to a time when the planet will warm up again. He said that scientists expect the gradually increasing heat of the sun will warm the planet sufficiently to release the covering of frozen carbon dioxide at the north pole, will start water ice to flow, and will in time create something of a greenhouse atmosphere. But the process is expected to take some 700 million years.“The key to my idea is that we now know that Mars lost its magnetic field long ago, the solar wind has been stripping off the atmosphere (in particular the oxygen) ever since, and the solar wind is in some kind of equilibrium with the outgassing at Mars,” Green said. (Outgassing is the release of gaseous compounds from beneath the planet’s surface.) “If we significantly reduce the stripping, a new, higher pressure atmosphere will evolve over time. The increase in pressure causes an increase in temperature. We have not calculated exactly what the new equilibrium will be and how long it will take.”The reason why is that Green and his colleagues found that they needed to add some additional physics to the atmospheric model, dynamics that will become more important and clear over time. But he is confident those physics will be developed. He also said that the European Space Agency’s Trace Gas Orbiter now circling Mars should be able to identify molecules and compounds that could play a significant role in a changing Mars atmosphere.So based on those new magnetic field models and projections about the future climate of Mars, when might it be sufficiently changed to become significantly more human friendly?In the simulation, the magnetic field is about 1.6 times strong than that of Earth.Well, a relatively small change in atmospheric pressure can stop an astronaut’s blood from boiling, and so protective suits and clothes would be simpler to design. But the average daily range in temperature on Mars now is 170 degrees Fahrenheit, and it will take some substantial atmospheric modification to make that more congenial.Green’s workshop focused on what might be possible in the mid-21st century, so he hopes for some progress in this arena by then.One of many intriguing aspects of the paper is its part in an NASA effort to link fundamental models together for everything from predicting global climate to space weather on Mars. The modeling of a potential artificial magnetosphere for Mars relied, for instance, on work done by NASA heliophysics—the quite advanced study of our own sun.Chuanfei Dong, an expert on space weather at Mars, is a co-author on the paper and did much of the modeling work. He is now an associate research scholar at Princeton University. He used the Block-Adaptive-Tree Solar-Wind Roe-Type Upwind Scheme (BATS-R-US) model to test the potential shielding effect of an artificial magnetosphere, and found that it was substantial when the magnetic field created was sufficiently strong. Substantial enough, in fact, to greatly limit the loss of Martian atmosphere due to the solar wind. As he explained, the artificial dipole magnetic field has to rotate to prevent the dayside reconnection, which in turn prevents the nightside reconnection as well.If the artificial magnetic field does not block the solar winds properly, Mars could lose more of its atmosphere. That’s why the planet needs to be safely within the magnetotail of the artificial magnetosphere.In their paper, the authors acknowledge that the plan for an artificial Martian magnetosphere may sound “fanciful,” but they say that emerging research is starting to show that a miniature magnetsphere can be used to protect humans and spacecraft. In the future, they say, it is quite possible that an inflatable structure can generate a magnetic dipole field at a level of perhaps 1 or 2 Tesla (a unit that measures the strength of a magnetic field) as an active shield against the solar wind. In the simulation, the magnetic field is about 1.6 times strong than that of Earth.As a summary of what Green and others are thinking, here is the “results” section of the short paper:“It has been determined that an average change in the temperature of Mars of about 4 degrees C will provide enough temperature to melt the CO2 veneer over the northern polar cap.“The resulting enhancement in the atmosphere of this CO2, a greenhouse gas, will begin the process of melting the water that is trapped in the northern polar cap of Mars. It has been estimated that nearly 1/7th of the ancient ocean of Mars is trapped in the frozen polar cap. Mars may once again become a more Earth-like habitable environment.The results of these simulations will be reviewed (with) a projection of how long it may take for Mars to become an exciting new planet to study and to live on.”Marc Kaufman is the author of two books about space: Mars Up Close: Inside the Curiosity Mission and First Contact: Scientific Breakthroughs in the Search for Life Beyond Earth. He is also an experienced journalist, having spent three decades at The Washington Post and The Philadelphia Inquirer. While the “Many Worlds” column, from which this post has been reprinted with permission, is supported and informed by NASA’s Astrobiology Program, any opinions expressed are the author’s alone.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Here’s what drives solar wind.This piece was originally republished on Facts So Romantic in November 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/16376_6cd3f3cfef62c9dbf52ea6645d16fb6b.jpg",
    "title": "Most of the Mind Can’t Tell Fact from Fiction",
    "description": "Posted by Jim Davies on September 11, 2019  Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned…",
    "category": "Culture",
    "content": "Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned it from Mary Renault’s novel The Last of the Wine. You’ll also retain false information even if you didn’t mean to. That seems like a liability: Philosophers have long concerned themselves with what they call “the paradox of fiction”—why would we find imagined stories emotionally arousing at all? The answer is that most of our mind does not even realize that fiction is fiction, so we react to it almost as though it were real.At the same time, very young children “can rationally deal with the make-believe aspects of stories,” distinguishing the actual, the possible, and the fantastical with sophistication, as Denis Dutton has written in The Art Instinct. “Not only does the artistic structure of stories speak to Darwinian sources: so does the intense pleasure taken in their universal themes of love, death, adventure, family conflict, justice, and overcoming adversity.” That may help explain why, when stories are done well, we love them so much. Just as artificial sweeteners fool our minds into thinking we’re eating sugar, stories—even weird ones like Alice’s Adventures in Wonderland—take advantage of our natural tendency to want to learn about real people, and how to treat them.Our brains can’t help but believe.There’s experimental evidence for this. Children, for example, sometimes actually believe that puppets are alive. Even animals sometimes react to pictures the same way they react to real things. The industrialized world is so full of human faces, like in ads, that we forget that it’s just ink, or pixels on a computer screen. Every time our ancestors saw something that looked like a human face, it probably was one. As a result, we didn’t evolve to distinguish reality from representation. The same perceptual machinery interprets both. The rational parts of our minds, particularly in the prefrontal cortex, do indeed know that what we’re looking at, or reading, isn’t real. One way to understand this is by thinking about optical illusions. In the Muller-Lyer illusion, we can trace and know the two horizontal lines are the same length, but at the same time appear to be different lengths. Even after you understand how an illusion operates, it continues to fool part of your mind. This is the kind of double knowledge we have when we consume fiction.These perceptual areas of our brains are very closely connected to our emotions. That’s why emotions don’t just motivate us to act in certain ways but force us to interpret the world differently. A 2011 paper, for example, explained how fear can affect vision, moods can make us more or less susceptible to visual illusions, and desire can change the apparent size of goal-relevant objects. The authors proposed that emotions offer information “about the costs and benefits of anticipated action,” knowledge that can be used swiftly, without thought, “circumventing the need for cogitating on the possible consequences of potential actions.” That’s the solution to the paradox of fiction, and why telling ourselves, “It’s only a movie,” can only partially attenuate the feelings we have about it. Our brains can’t help but believe.Jim Davies is a professor of cognitive science at Carleton University and author of Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One With the Universe. His new book, Imagination: The Science of Your Mind’s Greatest Power, comes out in November of 2019. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16451_a27e3f0172e92acc4ae5edb208992313.jpg",
    "title": "Making a Future Among the Stars",
    "description": "Posted by Brian  Gallagher on October 01, 2019  In Boca Chica, Texas, presenting SpaceX’s latest prototype vehicle, Starship, Elon Musk remembered how, 11 years ago, he got mad at his parachute supplier. His young rocket company seemed…",
    "category": "Culture",
    "content": "In Boca Chica, Texas, presenting SpaceX’s latest prototype vehicle, Starship, Elon Musk remembered how, 11 years ago, he got mad at his parachute supplier. His young rocket company seemed doomed: The Falcon 1 rocket had to reach orbit or else SpaceX, out of money and investors, would go under. It reached orbit—“Fate smiled on us that day,” Musk said—but the goal with that fateful launch was also to recover the Falcon 1 from the ocean intact. That didn’t happen. He told his parachute supplier that its parachute didn’t work. He later realized it wasn’t the parachute. That first stage rocket, Musk said, is coming in from space at about Mach 10 to 12. “It hits the atmosphere like it’s a concrete wall—and boom.” He chuckled and went on, “You actually have to orient the rocket carefully, have aerodynamic surfaces, have an entry burn to slow it down, then you’ve got to guide it through the atmosphere and do a propulsive landing. This took us many, many attempts.” Evidently it was worth it. On an outdoor stage, Musk was flanked by both the Falcon 1 and the stainless steel Starship. This was a showcase, as it were, of SpaceX’s evolution from a fledgling startup to a mature enterprise. Musk has helped to advance self-driving electric cars and an implantable brain-machine interface, but when he greeted the crowd in Boca Chica, he said, gesturing behind him, “This is the most inspiring thing I’ve ever seen.”The allure of the stars has arguably always been with us. Humans long ago didn’t just appreciate their beauty but studied their movements with stunning mathematical sophistication, relying on them to plan harvests and navigate across land and sea. Now humans seem, after decades of NASA pushing the space frontier, poised to navigate among the stars themselves—to become, as Musk often says, “multi-planetary”—with research settlements on the moon and, perhaps eventually, Mars. Humanity has a host of problems facing it. Still, I get goosebumps when I look at Starship and think about what my 9-month-old daughter will grow up believing to be normal. I won’t be molding her to be anything other than she wants to be, but I will be saying there’s never been a better time to aspire to be an astronaut or astronomer.Which reminds me of something cool. An acquaintance of mine, Michael Markesbery, the CEO and co-founder of OROS, an outdoor apparel company that uses NASA-inspired materials, shared with me the recent story of young Lily Fogels. Lily, who aspires to be an astronaut—or an astrophysicist as a “back up job”—wrote Target a letter after seeing time and again that the retail store didn’t sell NASA shirts for girls. “I want NASA clothes in the girls area because girls like space too,” she wrote. Her mother, Suzi, later told the media, “If girls aren’t exposed to NASA or other STEM (science, technology, engineering, and mathematics) related jobs, they won’t know they can aim for them.” Lily added that she hopes the future will see more female astronauts.When Markesbery got wind of this, he reached out to Lily and, with NASA’s approval, he made her some custom NASA shirts. With her red hair, Lily reminds me of the young girl who played Murph, who went on to become an astrophysicist in Christopher Nolan’s film Interstellar.Markesbery also spoke to the Chairman of the US Astronaut Hall of Fame. “We’re sending Lily and a parent to the next Hall of Fame induction ceremony at Kennedy Space Center so she can show her heroes what a future astronaut looks like,” Markesbery told me. “Her family told us Lily ‘can’t get the smile off her face.’”Lily says she can’t remember why she got interested in space—she just knows that she loves that the “universe grows and changes and there is always more to learn.” Lily’s childhood curiosity and tenacity echoes that of Chiara Mingarelli. The black-hole and gravity-wave researcher remembers how space captured her imagination. She told Nautilus that her spark for science came from playing outside in a small town in the outskirts of Ottawa, “especially around dusk,” when the light-pollution free sky darkened. “I always wondered what was out there, and if I could make a contribution to our knowledge of what was out there.” Initially, she thought the idea was fanciful. “Who was I to pretend like I could do such a thing?” she said. “But I was always really interested and I think that that was really my strength, that I never gave up. Once I found out what black holes were and that you could actually make money studying black holes—and that could be a career path—I really thought, “Well, why would you do anything else?”Assistant professor at the University of Connecticut and associate research scientist at the Center for Computational Astrophysics at the Flatiron Institute” data-credits=”” style=“width:733px”>Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16369_9bb2b56e6b7174609d788805794a4f46.png",
    "title": "Can New Species Evolve From Cancers? Maybe.",
    "description": "Posted by Christie Wilcox on September 06, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Aggressive cancers can spread so fiercely that they seem less like tissues gone wrong and more like invasive parasites…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Aggressive cancers can spread so fiercely that they seem less like tissues gone wrong and more like invasive parasites looking to consume and then break free of their host. If a wild theory recently floated in Biology Direct is correct, something like that might indeed happen on rare occasions: Cancers that learn how to roam between hosts may gradually evolve into their own multicellular species. Researchers are now scrutinizing a peculiar group of marine parasites called myxosporeans to see whether they might be the first known example.Even among microscopic parasites, myxosporeans are enigmatic. They were first discovered nearly two centuries ago, and more than 2,000 species are recognized today. Their complex life cycles make study particularly difficult: It wasn’t until the 1980s that scientists realized the ones found in fish were the same species as those found in worms, and not completely different classes of parasite. And while most parasites are content merely to snuggle into their animal host’s tissues, myxosporeans often take up residence inside a host’s own cells.It’s not clear how or why a complex multicellular creature discarded these seemingly necessary genes along with huge chunks of its DNA.Until fairly recently, myxosporeans were considered to be protists, offshoots of the eukaryotic line that are neither plants, animals, nor fungi. In 1995, however, Mark Siddall, then at the Virginia Institute of Marine Science, and his colleagues argued that myxosporeans are weird members of the cnidarians, the group that includes jellyfish and corals. Since then, genetic studies have bolstered that position.But their location on the tree of life doesn’t explain how myxosporeans ended up with such strange traits. Myxosporeans boast some of the smallest known animal genomes. The genome of Kudoa iwatai, for example, is estimated to be a mere 22.5 megabases, considerably smaller than that of any other cnidarian genome. It’s less than one-twentieth the size of the genome of Polypodium hydriforme, a closely related cnidarian parasite.Moreover, their genomes have not just been catastrophically reduced. They specifically lack certain genes thought to be essential for multicellular life. It’s not clear how or why a complex multicellular creature discarded these seemingly necessary genes along with huge chunks of its DNA.Yet Alexander Panchin, a senior researcher at the Russian Academy of Sciences, and his colleagues have an intriguing if controversial hypothesis to explain it. Early this year, they proposed that myxosporeans initially branched off from their cnidarian kin not as independent animals, but as tumors.Evolutionary ScandalsPanchin knows the idea of cancer-derived animals sounds far-fetched—so much so that, in the paper, he and his co-authors refer to them as Scandals (an acronym for “speciated by cancer development animals”).At first, Scandals were just a thought experiment. While Panchin was writing about transmissible cancers, he heard his colleagues express surprise at the genes for complex tissues that were turning up in certain unusual but simple parasitic animals. Further conversations led to what Panchin calls the “fantastic” idea that such simple parasites could have cancerous origins. “So we took all the data and we proposed this hypothesis,” he said.According to Panchin’s three-step scenario, a Scandal would start off as a cancer, but not just any cancer. It would have to be transmissible, so that it wouldn’t die when its host did. Then the cancer would have to spread to other species, and then independently evolve multicellularity. Those steps might seem to present insurmountable barriers, and yet there’s reason to believe each one could have happened.The first step, the emergence of the transmissible cancer, is the most straightforward because we know it happens, although it is rare. Devil facial tumor disease (DFTD) has become notorious as a transmissible cancer devastating Tasmanian devils, who transmit it to one another in their bites. More common but perhaps less famous is canine transmissible venereal tumor (CTVT), a sexually transmitted disease among dogs that, according to a recent analysis by Elizabeth Murchison of the University of Cambridge and her colleagues, has been evolving as a transmissible cancer for as long as 8,500 years. (In a 2014 report, Murchison and her co-authors described CTVT as perhaps “the oldest and most widely disseminated cancer in the natural world.”)Transmissible cancers are not confined to mammals; they have also been found in mollusks. There’s no reason to think it would be impossible for transmissible tumors to arise in a cnidarian too. Cnidarians certainly aren’t immune to cancers in general. If myxosporeans are Scandals, they most likely began as tumors of other cnidarian parasites—such as their Polypodium cousins, for instance.Although the spread of a cancer to other species might seem unlikely, “it’s not unheard of,” said Athena Aktipis, an assistant professor at Arizona State University. Aktipis, who specializes in the evolution of cancer, points to cases such as that of a man with HIV who was discovered to be infected with tumor cells from a tapeworm. Such worm cancers have turned up repeatedly among people with compromised immune systems, and the known cases likely represent only the small minority of occurrences in which the source of a strange growth was tracked down. If this kind of species hopping happens right before our eyes, “maybe we should also consider the possibility that things that were cancer or cancerlike sometimes, in the right conditions, could become parasites on other species,” she said.“I think that the field has been way too cautious about talking about when cancer becomes its own species, or its own kind of organism,” Aktipis said. In her view, researchers have seen too many examples of transmissible tumors like CTVT and DFTD. “It’s a parasite. It’s a parasitic organism.”Perhaps the least likely step in the Scandal hypothesis is the one where the cancerous parasite evolves from a unicellular existence to a multicellular one with distinct hosts and stages. Myxosporeans are simple animals but truly multicellular—so if they arose from a transmissible tumor, that tumor would have had to evolve distinct cell types.Multicellularity is thought to have evolved at least 25 times in eukaryotes, the domain of life that includes complex single-celled creatures as well as plants, animals, and fungi. In animals, though, it’s believed to have arisen just once at the very base of our lineage. Some multicellular branches of the eukaryotes have reverted to unicellularity, but no animals have been known to do so (unless, like some scientists, you consider cancer itself to be a kind of reversion). As yet, there don’t seem to be lineages of any kind in which multicellularity was gained, lost, and then gained again, in keeping with the Scandal hypothesis. “We understand that this is a very improbable scenario,” Panchin said.But that doesn’t mean it couldn’t have happened. “I think it’s certainly possible that clusters of cancer cells that are transmissible could evolve to have something like a life cycle,” Aktipis said. “There’s nothing special about the evolutionary process that says you can only evolve a life cycle if you are a branch of the evolutionary tree that didn’t derive from [a part of] another organism.”Following the EvidenceIn the hope of finding more substantive evidence for the Scandal theory, Panchin and his team compared the genomes of a variety of simple species (most of them parasitic) with those of five myxosporeans, three single-celled creatures, and 29 other animals. They looked for hints of a cancerous past by checking for the absence of genes that are often lost when cells turn malignant. These include genes involved in apoptosis, the regulated self-sacrifice that purges abnormal cells from the body. Any organism evolving from a transmissible tumor would presumably lack such genes.Although the scientists had expected other parasites to be the most likely Scandal candidates, only the myxosporeans had lost key tumor-suppressing genes. So they drilled deeper and found that the myxosporeans have lost so many genes related to apoptosis that they probably can’t trigger that death pathway at all. That deficiency stood out: “Even if you look at very simplified parasites which are animals, we don’t see this degree of lack of cancer-related genes,” Panchin said.“I think that the field has been way too cautious about talking about when cancer becomes its own species, or its own kind of organism.”Aktipis thinks that Panchin and his co-authors have presented some intriguing reasons why “we should at least consider the possibility that some of the parasitic organisms that we see today might have evolved from transmissible cancers.” But it’s not case closed, she said. “This paper is a beginning for this work, not a decisive proof of it by any means.”Juliana Naldoni, a parasitologist and myxosporean specialist with the Federal University of São Paulo, isn’t convinced that myxosporeans are Scandals at all. “They are actually much more complex than initially thought and evolve quite intricate [and specific] mechanisms of interaction with their hosts,” she said. Some species also have complex features, such as cells organized into structures resembling muscles for movement, for example. She just doesn’t find it plausible that such complexity arose from a cancer.Adrian Baez-Ortega, a doctoral student and bioinformatician with Murchison’s Transmissible Cancer Group at the University of Cambridge, agrees with Naldoni. “It is a thought-provoking paper, if not a very convincing one,” he wrote in an email to Quanta. He isn’t terribly impressed by the loss of apoptosis genes, for example. “In the context of such a dramatic genome reduction, the claim that the lack of genes specifically related to apoptosis points to a cancerous origin seems rather cherry-picked,” he explained.But mostly he’s skeptical that a transmissible cancer could last long enough to evolve multicellularity. Cancer cells have incredibly unstable genomes. Although this allows them to mutate rapidly and elude their host’s defenses, Baez-Ortega pointed out that on an evolutionary timescale, “this is a very detrimental strategy. As time goes on, a good portion of a cancer’s genome becomes nonfunctional or abnormal, and this might impede not just survival, but also the development of sophisticated traits like multicellularity.” The way he sees it, “even if a transmissible cancer could have survived for millions of years, it would be much more likely to remain a unicellular parasite.”That said, he thinks the Scandal hypothesis is worth further investigation. “There is almost nothing evolution cannot do,” he said. Rather than focusing on specific missing genes, he would like researchers to scan candidate species for the diverse genomic changes that occur in cancers, from point mutations to large-scale chromosome rearrangements. “If a cancer were to become a long-lived species, all these modifications would be preserved in its genome,” he said.Even Panchin and his colleagues aren’t going all-in on the hypothesis that myxosporeans are Scandals. “I think that’s fair to say it’s probably not true,” he said. It’s just that, with the work they’ve done so far, they can’t rule it out. “We’ve been trying to refute it with the means that we have.”He added, “We are going to try to falsify the hypothesis through looking at the Malacosporea genome.” Malacosporeans are cnidarian parasites and the closest known relatives of myxosporeans, but they are so much more complex that they are clearly not cancer derived. If they, too, turn out to lack apoptosis genes, it would suggest that the myxosporean loss doesn’t stem from a cancerous past.Even if, in the end, the data suggest myxosporeans aren’t evolved cancers, Panchin noted that Scandals could still be out there waiting to be discovered. “We are hoping that maybe some zoologists who have been investigating some other peculiar kind of animal at some point will say, ‘Probably those guys are wrong about Myxosporea, but this [animal], he’s obviously a cancer.’”Christie Wilcox is an award-winning science writer who currently edits and writes scripts for SciShow. Her award-winning writing has been featured in multiple anthologies, and her bylines include National Geographic, Popular Science, and The Washington Post. "
  },
  {
    "imageUrl": "http://static.nautil.us/16401_19c768e48aca9308d1a11fe86157731f.jpg",
    "title": " What Trump’s Simplified Language Means",
    "description": "Posted by Brian  Gallagher on September 18, 2019  Acouple years ago, I was surprised that a panel called “The Press and President Trump,” held at the Columbia Journalism School, didn’t broach the subject of mental illness. Just over…",
    "category": "Culture",
    "content": "Acouple years ago, I was surprised that a panel called “The Press and President Trump,” held at the Columbia Journalism School, didn’t broach the subject of mental illness. Just over a week earlier, at a psychiatry conference at Yale, a group of the attendees announced that Trump has a “dangerous mental illness.” “I can recognize dangerousness from a mile away,” James Gilligan, a professor at New York University, who’s worked with murderers and rapists, told the conference. “You don’t have to be an expert on dangerousness or spend 50 years studying it like I have in order to know how dangerous this man is.” In the Q&A segment, I stepped up to the mic and mentioned this, and asked whether the health status of President Trump’s mind would become a more prominent story for journalists in the months ahead. One of the panelists, Carolyn Ryan, a senior editor at The New York Times, demurred. She noted that the paper had published some op-eds voicing this concern, but was wary of having reporters cover it. It could be “dangerous,” she said, to report on something so speculative. Another of the panelists, Matt Bai, a national correspondent for Yahoo! News, denounced the psychiatrists’ statements as “ideology masquerading as science.”Trump isn’t the first presidential figure to receive psychiatric scrutiny. In 1964, the magazine Fact polled psychiatrists on whether Barry Goldwater was psychologically fit for the presidency. “Warped,” “impulsive,” and “paranoid schizophrenic” was how over 1,000 psychiatrists described him. Goldwater successfully sued for libel, and in 1973, in response, the American Psychiatric Association added the “Goldwater rule” to its ethics code. It forbade making diagnoses without an in-person examination and without obtaining permission to publicly discuss the results.“Trump’s language borders on incapacity.”“With regard to Trump, however, the rule has been broken repeatedly” by psychotherapists and psychiatrists, Evan Osnos wrote in The New Yorker, in a story published following the panel. Titled “How Trump Could Get Fired,” the story demonstrates that journalistic coverage of Trump’s mental health doesn’t have to be a flimsy or biased exercise in discerning his thoughts. Over 50,000 mental-health physicians, Osnos noted, have signed a petition declaring Trump, based on copious observational data, is “too seriously mentally ill to perform the duties of president and should be removed” under the 25th Amendment to the Constitution—Section 4 states that a President can be removed if a congressionally appointed body judges him or her to be “unable to discharge the powers and duties of office.” John Gartner, the psychiatrist who started the petition, has said, “The psychiatric interview is hardly the gold standard, by the way. If you have massive amounts of information about a person’s behavior, that can be more accurate. And we have that. If the question is whether we can form a diagnosis from that information, I think it’s clear that we can. You don’t need to have an interview to know if someone has frequently lied or has violated the rights of others.”Osnos wrote that it’s not just psychiatrists who have serious grounds for worry:Bruce Blair, a research scholar at the Program on Science and Global Security, at Princeton, told me that if Trump were an officer in the Air Force, with any connection to nuclear weapons, he would need to pass the Personnel Reliability Program, which includes thirty-seven questions about financial history, emotional volatility, and physical health. (Question No. 28: Do you often lose your temper?) “There’s no doubt in my mind that Trump would never pass muster,” Blair, who was a ballistic-missile launch-control officer in the Army, told me. “Any of us that had our hands anywhere near nuclear weapons had to pass the system. If you were having any arguments, or were in financial trouble, that was a problem. For all we know, Trump is on the brink of that, but the President is exempt from everything.”Trump’s use, or misuse, of language has also been disturbing to experts of constitutional law. Take Laurence Tribe, a Harvard constitutional law professor. He said, according to Osnos, “Trump’s language borders on incapacity.” When the president was asked to explain his reversal on branding China a currency manipulator, Trump said, of President Xi Jinping, “No. 1, he’s not, since my time. You know, very specific formula. You would think it’s like generalities, it’s not. They have—they’ve actually—their currency’s gone up. So it’s a very, very specific formula.” This response could count as an example of “gross and pathological inattention or indifference to, or failure to understand” the mandatory duties of the president mentioned in the 25th Amendment, Tribe said.To psycholinguist Julie Sedivy, it’s not Trump’s rambling language that’s worrisome, it’s his regular usage. “I think we have rarely had a president who uses such simple and simplifying language,” she said in an interview with Nautilus.And why is that concerning? “There’s some interesting research that has looked at the correlation between simple language and the tendency of U.S. presidents to behave in authoritarian ways,” Sedivy said. “There is a predictive relationship that speeches that are expressed using very simple basic language tend to precede very authoritarian acts like the use of executive orders … That certainly plays out in the use of the heavy reliance on simple notions like amazing, sad, bad, unfair. These really strip away a lot of the complexities that are behind them. They reduce information into very gross impressions. The simplification of points of view, the simplification of the good and the bad, and even just the conveyance that, ‘We’re going to make good deals,’ for example. ‘It’s going to be great.’ That this is a simple problem just waiting for someone who has the right instincts to come along and solve this, is absolutely pervasive in Donald Trump’s language.”And what is the downside of that kind of usage? “Well, I think the big downside is that it’s false. The world is a complex place. It’s not a simple environment. There are many interacting forces simultaneously that really elude simple explanations or simple solutions. One thing that I certainly have become very aware of through a couple of decades now of being a scientist is that for every simple, elegant explanation or theory we have come up with, we have discovered that the truth is actually not simple or elegant. It’s messy, noisy, complex.”By reducing complexity to simplified language, Sedivy said, “we’re essentially lying about the nature of the world.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in May 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/16433_62dbdc10715d73b9fbbfdcda0a88848d.jpg",
    "title": " How to Get Evangelicals to Care About Climate Change",
    "description": "Posted by Elaine Howard Ecklund & Christopher P. Scheitle on September 24, 2019  Last year was among the four warmest years ever recorded, 1.5 degrees Fahrenheit above the 20th century average. The three years prior were warmer (2016 the warmest). “The six warmest…",
    "category": "Culture",
    "content": "Last year was among the four warmest years ever recorded, 1.5 degrees Fahrenheit above the 20th century average. The three years prior were warmer (2016 the warmest). “The six warmest years on record for the planet have all occurred since 2010,” the National Oceanic and Atmospheric Administration states on its website.Among those who accept that the cause of this is climate change, and that human actions play a major role, such reports tend to lead to finger pointing at climate change deniers and skeptics, who are seen as obstacles to progress on important climate policy decisions. Such finger pointing is sometimes directed at religious people, especially evangelical Christians who, either because of their theology or political conservatism, are taken to make up much of these deniers and skeptics.But after five years of surveying, interviewing, and analyzing data on what religious people think about science, we have come to understand that evangelicals’ views about climate change, and the environment more broadly, are more complex than some might assume.Scientists would do well to listen to this research.While our survey of over 10,000 Americans found that only 26 percent of evangelical Christians (compared to 33 percent of the total US population) are very interested in environmental care and only 29 percent (compared to nearly 45 percent of Catholics and 48 percent of Jews) accept human-caused climate change, our over-300 in-depth interviews reveal a more nuanced story: Religious people, particularly evangelical Christians, show more environmental concern when its connection to human health and flourishing is made clear.Here, for example, is what a church youth minister had to say about environmental care: “If we have the opportunity we should help take care of this planet that we’ve been given. Having said that, I also believe that the value of human life is higher than the value of a whale, or a species of monkey.”It’s not that evangelicals don’t care about the environment. It’s that they care about people more. Some environmental advocates are spending their energy trying to convince others of the importance of preserving our green spaces and waterways. This is important. But we think a different and more effective angle may be to redirect the conversation about environmental care to caring for people, social justice, and human flourishing, rather than for its own sake.Scientists would do well to listen to this research. And they need to make sure that religious people—in particular evangelical Christians—understand that caring for the environment is caring for people. This is especially important for members of congregations with low economic resources, who are particularly likely to think that other needs are more pressing than environmental issues, but are also likely to be located in places that are hardest hit by the forces of environmental degradation, like poor air quality.Climate change is a global problem. Perhaps many more Americans will agree on how to solve it if scientists frame the challenge as religious as well as political. A physician we spoke to, and who identifies as an evangelical Christian, linked caring for the environment and caring for humanity in similar terms: “Throughout the world the poor are often victims of environmental degradation. And so one of the most important things that the Bible teaches us is that what God cares most about, is the poor. And so to the extent that degradation of the environment is harming the poor and making them even worse off, then it is an essential issue for Christians to get engaged in.”Elaine Howard Ecklund is a sociologist at Rice University, where she directs the Religion and Public Life Program. Christopher P. Scheitle is a sociologist at West Virginia University. Their recent book is Religion Vs. Science: What Religious People Really Think. New York: Oxford University Press.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How the cosmologist Lawrence Krauss, a militant atheist, views religious scientists.This classic Facts So Romantic post was originally published in January 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16398_5939cbe46f1512291125700ee2e7236a.jpg",
    "title": "Mind the Gap Between Science and Religion",
    "description": "Posted by Sabine Hossenfelder on September 16, 2019  Have you heard that we may be living in a computer simulation? Or that our universe is only one of infinitely many parallel worlds in which you live every possible variation of your life?…",
    "category": "Matter",
    "content": "Have you heard that we may be living in a computer simulation? Or that our universe is only one of infinitely many parallel worlds in which you live every possible variation of your life? Or that the laws of nature derive from a beautiful, higher-dimensional theory that is super-symmetric and explains, supposedly, everything?I’ve heard that too. It’s how my research area, fundamental physics, often ends up making headlines: With insights about the nature of reality so mind-boggling you can’t believe it’s still science. Unfortunately, in many cases it’s indeed not science.Believing in an omnipotent Programmer is not science—it’s tech-bro monotheism.Take the idea that we live in a computer simulation. According to our best current knowledge, the universe follows rules that are encoded by a set of equations. We don’t know these equations completely (yet!), but you could rightfully say the universe computes in real time whatever are the correct equations. In that sense, we trivially “live in a computer,” but that’s just a funny way to talk about the laws of nature. You may more specifically ask whether our universe’s computation is similar to the computation performed by computers we build ourselves, that is, pushing around units of information in discrete time-steps. This is a testable hypothesis, and to the extent that it has been tested, it has been falsified. It is not easy to obtain the already known laws of nature using discrete, local operations even approximately, and this mathematical difficulty has, so far, rendered scientifically well-posed versions of the simulation hypotheses incompatible with evidence. And finally, if you are really asking whether our universe has been programmed by a superior intelligence, that’s just a badly concealed form of religion. Since this hypothesis is untestable inside the supposed simulation, it’s not scientific. This is not to say it is in conflict with science. You can believe it, if you want to. But believing in an omnipotent Programmer is not science—it’s tech-bro monotheism. And without that Programmer, the simulation hypothesis is just a modern-day version of the 18th century clockwork universe, a sign of our limited imagination more than anything else.It’s a similar story with all those copies of yourself in parallel worlds. You can believe that they exist, all right. This belief is not in conflict with science and it is surely an entertaining speculation. But there is no way you can ever test whether your copies exist, therefore their existence is not a scientific hypothesis. Most worryingly, this confusion of religion and science does not come from science journalists; it comes directly from the practitioners in my field. Many of my colleagues have become careless in separating belief from fact. They speak of existence without stopping to ask what it means for something to exist in the first place. They confuse postulates with conclusions and mathematics with reality. They don’t know what it means to explain something in scientific terms, and they no longer shy away from hypotheses that are untestable even in principle.Particularly damaging to fundamental physics has been the belief that the equations which describe nature must be beautiful by human standards. There is no rational reason why this should be so, but faith in beautiful math has become pervasive in the community. And that’s despite the fact that relying on beauty as a guide to new natural laws has historically worked badly: The mechanical clockwork universe was once considered beautiful. So were circular planetary orbits, and an eternally unchanging universe. All of which, it turns out, is wrong.And relying on beauty is still working badly for physicists. We see the tragedy playing out in the ongoing failure of ideas like a unification of the fundamental forces, a theory of everything, or new types of symmetries that experiments continue to not find. Such pretty hypotheses remain popular among physicists even though they haven’t led anywhere for decades. The recent Special Breakthrough Prize for Fundamental Physics drove home the point. The prize was awarded for Supergravity, a theory, invented in the 1970s, that is widely praised for its elegance and beauty. Supergravity has to date no observational support. It wins prizes nevertheless.This blurring of the line between science and religion is not innocuous. Resources—both financial and human—that go into elucidating details of untestable ideas are not available for research that could lead to much-needed progress. I like the idea that the laws of nature are beautiful and the universe was made for a purpose as much as everybody else, and I appreciate public interest in our research anytime. But let’s not call it science when it is really not. We have fought hard for secularism, and we don’t want religious leaders to meddle in scientific debate. Scientists, likewise, should respect the limits of their discipline. Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16473_bb6961095e9a6dc1aac2af24ea88fc52.jpg",
    "title": "The Strange Physics of How Babies Talk",
    "description": "Posted by Brian  Gallagher on October 09, 2019  Like all new parents, I must sound like a kook when I babble along with my 9-month-old daughter. That’s okay: It delights her. I sometimes ask her what she might mean as she offers some…",
    "category": "Matter",
    "content": "Like all new parents, I must sound like a kook when I babble along with my 9-month-old daughter. That’s okay: It delights her. I sometimes ask her what she might mean as she offers some apparently affirming utterance and looks at me with her big blue eyes: Oh, you like it when daddy lifts you? Ah, you’re thankful for a new diaper? Her vocalizations—the squeals and whoas and yah-wahs—can have surprising verve and a kind of ecological significance. My daughter’s noises, scientists say, “catalyze” me to produce “simplified, more easily learnable language.” My daughter is now regularly saying “mama.” My wife has taken this to mean the baby wants her attention. So I felt a little disheartened after reading a 2017 paper in Neuroscience & Biobehavioral Reviews, titled “The growth of language: Universal Grammar, experience, and principles of computation.” Charles Yang, a computer scientist at the University of Pennsylvania who studies language acquisition, wrote, along with colleagues, that, “Despite the prevalence of sounds such as ‘mama’ and ‘dada,’ the combination of consonants and vowels in babbling have no referential meaning”—they are nothing more than “rhythmic repetitions of nonsense syllables.” (Sorry, Danielle!) This lack of “semantic content” is nothing to be bummed about, though, for “babbling merges linguistic units (phonemes and syllables) to create combinatorial structures,” the bread and butter of language learning.“Each grammar defines probabilities for sentences,” says theoretical physicist Eric DeGiuli.The puzzle of how language can be so learnable has had me, of late, utterly perplexed. Babies are helpless. I can see how, with some instruction, a child can pick up on a new idea or skill, like shoe-tying—they can understand what you’re saying and mimic movements. Yet my daughter will be obeying grammar reliably and effortlessly, if a bit crudely, before she’s tying her own shoes. It’s a wonder babies learn anything, let alone a language. Parrots, great speech mimickers that they are, can’t babble, or break speech down into its discrete units. Yang and his colleagues note that the “best that Alex, the famous parrot, could offer was a rendition of the word ‘spool’ as ‘swool,’ which he evidently picked up when another parrot was taught to label the object.” Parrots don’t, in other words, partake in the phenomenon of Merge, the engine of hierarchical linguistic structures, a mental technology which starts to rev up when infants start to vocalize. All human languages have an “inaudible and invisible hierarchical structure” that, when we’re children, we impose on sound sequences we hear, writes linguist David Adger in his Nautilus feature, “This Simple Structure Unites All Human Languages.” “We hear sounds or see signs, but our minds think syntax.” Which is why infants can learn any language. There’s something deeply hidden in the mind that recognizes syntax—or the peculiar grammar of each language—as if it were as apparent as a face. “The full scale of linguistic complexity in a toddler’s grammar still eludes linguistic scientists and engineers alike, despite decades of intensive research.” Perhaps it’s no surprise, then, that physicists are now chiming in. Take a recent paper from Eric DeGiuli, a theoretical physicist, formerly at École Normale Supérieure, in Paris, now at Ryerson University in Toronto. In his paper, he argues that a language—or specifically a “context-free grammar,” which almost all languages share—can be treated as a physical object that children interact with as they hear words and sentences. In a context-free grammar, sentences have tree-like graphical structures: “The bear walked into the cave” can be broken down into a noun and verb phrase—“the bear” and “walked into the cave,” respectively—and each of those elements breaks down into more elementary units, like nouns and verbs. You might call these the tree’s leaves.This gave me the absurd image of a baby flying through a forest. The “surface” of a language that babies “touch” are all the possible configurations of words into sentences, both meaningful and meaningless. “Each grammar defines probabilities for sentences,” DeGiuli writes. In English, certain words, coming one after another, are more likely than others to make grammatical sense. DeGiuli says that the ones that do make sense get assigned, in the baby’s mind, greater weight, while word combinations that don’t make sense get assigned less weight. These different arrangements of words “are like the microstates in statistical mechanics—the set of all possible arrangements of a system’s constituent particles,” science writer Philip Ball wrote, in an article on DeGiuli’s paper. “The question is, among all possible [context-free grammars], what kind of weight distributions distinguish [context-free grammars] that produce random-word sentences from those that produce information-rich ones?” Surprisingly, it has to do with “temperature.” Not literal temperature, as in hot and cold, but a measure akin to temperature. For DeGiuli, hot is associated with randomness while cold is more deterministic, not unlike how high temperature in a room or object is due to the quick and chaotic movement of atoms. He writes that context-free grammars have “two natural ‘temperature’ scales that control grammar complexity, one at the surface interface, and another in the tree interior.” As Ball explains:DeGiuli’s theoretical analysis—which uses techniques from statistical mechanics—shows that there are two key factors involved: how much the weightings “prune” branches deep within the hierarchical tree, and how much they do so at the surface (where specific sentences appear). In both cases, this sparseness of branches plays a role analogous to a temperature in statistical mechanics. Lowering the temperature both at the surface and in the interior means reducing more of the weights. As the deep temperature is lowered—meaning the tree interior becomes sparser—DeGiuli sees an abrupt switch from [context-free grammars] that are random and disorderly to ones that have high information content. This switch is a phase transition analogous to the freezing of water. He thinks that something like this switch may explain why, at a certain stage of development, a child learns very quickly how to construct grammatical sentences.The idea that language locks in place, as water does when it’s chilled, is a weird, attractive idea. It’s fair to say that something has “crystallized” in the mind of child able to talk. How that exactly happens, though, as in so much in the natural world, on which theoretical physics turns its gaze, is difficult to say. As I play with my daughter and listen to her form proto-words, I sympathize with Yang and his colleagues: “The growth of language is nothing short of a miracle.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16355_9f482bc662193b9b94846e168dd1df08.jpg",
    "title": "What Color Really Evolved For",
    "description": "Posted by Gareth Willmer on September 01, 2019  What color were the dinosaurs? If you have a picture in your head, fresh studies suggest you may need to revise it. New fossil research also suggests that pigment-producing structures go…",
    "category": "Biology",
    "content": "What color were the dinosaurs? If you have a picture in your head, fresh studies suggest you may need to revise it. New fossil research also suggests that pigment-producing structures go beyond how the dinosaurs looked and may have played a fundamental role inside their bodies too. The latest findings have also paved the way for a more accurate reconstruction of the internal anatomy of extinct animals, and insight into the origins of features such as feathers and flight.Much of this stems from investigations into melanin, a pigment found in structures called melanosomes inside cells that gives external features including hair, feather, skin and eyes their color—and which, it now turns out, is abundant inside animals’ bodies too. “We’ve found it in places where we didn’t think it existed,” said Maria McNamara, a paleobiologist at University College Cork, in Ireland. “We’ve found melanosomes in lungs, the heart, liver, spleen, connective tissues, kidneys…They’re pretty much everywhere.”We’re just at the tip of the iceberg when it comes to fossil color research.The discoveries in her team’s newest research, published in mid-August, were made using advanced microscopy and synchrotron X-ray techniques, which harness the energy of fast-moving electrons to help examine fossils in minute detail. Using these, the researchers found that melanin was widespread in the internal organs of both modern and fossil amphibians, reptiles, birds, and mammals—following up a finding they made last year that melanosomes in the body of existing and fossil frogs in fact vastly outnumbered those found externally. What’s more, they were surprised to discover that the chemical make-up and shape of the melanosomes varied between organ types, thus opening up exciting opportunities to use them to map the soft tissues of ancient animals. These studies also have further implications. For one, the finding that melanosomes are so common inside animals’ bodies may overhaul our very understanding of melanin’s function, says McNamara. “There’s the potential that melanin didn’t evolve for color at all,” she said. “That role may actually be secondary to much more important physiological functions.” Her research indicates that it may have an important role in homeostasis, or regulation of the internal chemical and physical state of the body, and the balance of its metallic elements. “A big question now is does this apply to the first, most primitive vertebrates?” said McNamara. “Can we find fossil evidence of this? Which function of melanin is evolutionarily primitive—production of color or homeostasis?”At the same time, the findings imply that we may need to review our understanding of the colors of ancient animals. That’s because fossil melanosomes previously assumed to represent external hues may in fact be from internal tissues, especially if the fossil has been disturbed over time. McNamara says her research has also shown that melanosomes can change shape and shrink over the course of millions of years, potentially affecting color reconstructions.Further complicating the picture is that animals contain additional non-melanin pigments such as carotenoids and what is known as structural color, which was only recently identified in fossils. In 2016, a study by McNamara’s team on the skin of a 10-million-year-old snake found that these could be preserved in certain mineralized remains. “These have the potential to preserve all aspects of the color-producing gamut that vertebrates have,” she said. She hopes over time that these findings and techniques will together help us to much more accurately interpret the colors of ancient organisms—though in these early days, she doesn’t have examples of animals for which this has already changed.Many of the significant strides in this area have come out of a project that McNamara leads called ANICOLEVO, which set out to look into the evolution of color in animals over deep time—or hundreds of millions of years. The project’s starting point was that previous animal color studies largely omitted in-depth fossil analysis, leaving a significant gap by basing what we know about color mainly on modern organisms. But it has since led to even wider investigation. McNamara says it is providing fresh hints on the kinds of biological structures and processes that are essential for survival in terrestrial and aquatic environments: “It looks like we’ll be able to look into much broader, exciting questions about what it means to be an animal.”Part of her research on two fossils found in China even showed, for the first time, that flying reptiles known as pterosaurs had feathers, potentially taking the evolution of these structures back a further 80 million years to 250 million years ago. The fossils contained preserved melanosomes with diverse shapes and sizes, one of the tell-tale signs of feathers. Another project she worked on, called FOSSIL COLOR, compared the chemistry of color patterns between fossil and modern insects. Again, McNamara says, these don’t entirely map onto each other. “It’s already clear that the fossilization process has altered the chemistry somewhat,” she said, “so we’re doing experiments to try to understand these changes. We’re just at the tip of the iceberg when it comes to fossil color research.”Other researchers agree that there’s more to animal color than meets the eye. Matthew Shawkey, an evolutionary biologist at Ghent University, in Belgium, said that looking into properties and functions beyond color’s use for visual means like signaling and camouflage will be critical to understanding its true significance. “For example, how do colors affect thermoregulation? Flight?” he said. “Such functions may be complementary to, or even more significant, than purely visual functions.”Shawkey is looking into such questions, with one of his recent studies indicating that the wing color of birds may play an important role in flight efficiency by leading to different rates of heating. “What started as a novelty of deciphering dinosaur colors has turned into a very serious field which is studying the origins of key pigment systems, how the evolution of colorful structures may have helped drive major evolutionary transitions like the origin of flight, and how color is related to ecology and sexual selection,” said Steve Brusatte, a vertebrate paleontologist and evolutionary biologist at the University of Edinburgh. “When I was growing up, so many of the dinosaur books I read in school said that we would never know what color they were. But as is so often the case in science, it was silly to treat this as impossible.”Gareth Willmer is a freelance writer and subeditor based in London.\n\tThe newest and most popular articles delivered right to your inbox!\nThe research in this article was funded by the EU. If you liked this article, please consider sharing it on social media.Originally published on Horizon: the EU Research & Innovation magazine | European Commission. "
  },
  {
    "imageUrl": "http://static.nautil.us/16352_8bcf12d1a11564975f1e62b898ef0a0d.jpg",
    "title": " Butterfly Wonk Robert Pyle Pens His First Novel 44 Years in the Making",
    "description": "Posted by Mary Ellen Hannibal on August 30, 2019  Last year marked a first for 71-year old Robert Michael Pyle, the acclaimed author, naturalist, and ecologist: the publication of his long-awaited first novel, Magdalena Mountain, nearly…",
    "category": "Ideas",
    "content": "Last year marked a first for 71-year old Robert Michael Pyle, the acclaimed author, naturalist, and ecologist: the publication of his long-awaited first novel, Magdalena Mountain, nearly half a century in the making.Pyle has been investigating the butterfly for about 60 years. In that time his prolific output has included several butterfly field guides, chronicles of his adventures in the great outdoors (Where Bigfoot Walks, Thunder Tree, and Mariposa Road among them), and the magnum opus Nabokov’s Butterflies, a collection of the novelist’s butterfly writings. Pyle has also made seminal contributions to butterfly research, including his establishment of the Western Monarch’s migration pathway across the U.S. and into Mexico. Forty-eight years ago, he foresaw the acceleration of insect extinctions and founded the Xerces Society, devoted to invertebrate conservation.For those of us devoted to his nonfiction, Pyle’s imaginative treatment of the nonhuman world—he includes a butterfly and a mountain among his cast of characters—is both a surprise and perhaps a natural result of his artistic development. Like Nabokov, Pyle has consistently straddled the worlds of science and the imagination.Pyle caught up with Nautilus to chat about his work and the way butterflies shaped his early life.Is Magdalena Mountain your first foray into fiction?Fiction isn’t entirely new to me—I’ve published some short stories and poetry. I worked on this novel for 44 years. It went through ten drafts. I started it at Yale, while working on my dissertation, which was about the distribution of Washington State butterflies with respect to conserved land. It was a “gap analysis” before that term came into being. It was deeply enjoyable but very intense intellectually, and left-brain. I was inclined to write something creative at the same time.What inspired you to make a butterfly and a mountain into fictional characters?The idea came from a book called Wings in the Meadow, published by Jo Brewer in 1967. Brewer was a wonderful teacher, and her young adult novel was about a single Monarch butterfly and his migration—she called him Daneaus. I thought, I’ll write about a butterfly too, but about the Erebia Magdalena, which I had fallen in love with.Tell us about your love affair with butterflies.I started to notice butterflies in the spring I turned 11. That year, my father re-married and I acquired a step-mother and a step-brother and didn’t get along with either. But my step-mother’s family had a cabin in Crested Butte, Colorado. It was not remotely fancy then (it is a ski resort now). It was a crusty old coal town with cheap, crappy cabins. It happened to be a butterfly Valhalla. My father and I would escape the family’s endless smoky bridge games by going fishing. Luckily he never really taught me to tie flies or I might’ve gotten into it. As soon as I discreetly could, I would put my pole down and pick up my butterfly net. I would go up into these meadows full of skippers and fritillaries. I developed a passion for the satyrs, the wood nymphs. I liked their soft, striated browns and their eye spots. The Erebia Magdalena was the ultimate, the greatest among them. Something about it being black really captured me.And what drew you to center a mountain in your story?Thomas Hardy’s The Return of the Native was an inspiration. The first six pages of the book take place before a human steps on the page. Landscape is character. It is the place where things happen. One day in 1959, I looked down over a ridge and I saw about 100 people with butterfly nets. Huh? I uttered a childish version of “what the fuck?” I’d never seen anyone else with nets. I had been reading about butterflies and recognized Charles Remington and Paul Ehrlich. The living gods of lepidoptera! They were in their 30s, just kids—out with grad students near the Rocky Mountain Biological Lab. I was pathologically shy but somehow emboldened. I asked these guys if they knew where I could find the Magdalena—they told me there was a big rockslide in back of Ehrlich’s cabin where I would find them. And they were very generous with me, let me go butterfly hunting with them, sometimes even driving to pick me up. (Editor’s note: Remington convinced Pyle to apply to Yale, where he became Pyle’s mentor. Ehrlich went on to write The Population Bomb, with his wife Anne. Remington and Ehrlich both appear in Magdalena Mountain.)“Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?”The novel contains several storylines. There’s the journey of a woman who loses her memory and believes she is Mary Magdalene incarnate. A budding scientist struggles with a difficult past and graduate school. And a group of men seeking a return to pantheism convene a community called the Grove in the mountains. Did these narratives develop at different times, in different drafts, over the 44 years you were writing the novel?Yes, I did. I wanted to do this experiment. The butterfly was always at the center of the story and would drive the story, but also one of a cast. While the book is not a roman à clef, a butterfly makes things happen in the book, as it has made things happen in my life. The Magdalenian part is pseudo-religious, developed around the name of the actual black butterfly, but the story is grounded in reality. Researching Mary Magdalene—the most-written about woman in the Bible—was fascinating for me. How the mystery around the name “Magdalena” resolves is all true, as is the entomological research story. I had a lot of fun with it all.Vladimir Nabokov famously hunted butterflies in Colorado, references to which are woven throughout the book.For nine years I worked on compiling and annotating Nabokov’s butterfly writings, with his biographer Brian Boyd, and his son, Dmitri Nabokov. During the process I discovered Nabokov’s wonderful question, “Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?” It inspired the title of my book Walking the High Ridge: Life as Field Trip. But there was more. In Look at the Harlequins, Nabokov has Vadim and Bel stay at Lupine Lodge, which was surely modeled on Columbine Lodge where Nabokov stayed in 1947. Bel scribbles a poem: “Longs’ Peacock Lake: / the Hut and its Old Marmot; / Boulderfield and its Black Butterfly; / And the intelligent trail.” The black butterfly! My Erebia! And of course it was near Telluride that Humbert realized the moral consequences of his crimes in Lolita.How do you use Nabokov’s idea of a “high ridge” between art and science as an educator?When I teach environmental writing I use Nabokov’s quote to build a model. I start by trying to establish the difference between the mirror-like reflection of reality sought by science and journalism and the refractive aim of those writing essays, fiction and poetry. C.P. Snow’s Two Cultures metaphorically fall on either side of a mountain, with subjective, lyrical imagination on one side, and objective analysis on the other. This becomes a model of the bicameral mind. I’m not a brain scientist, but the region of the brain lying between the hemispheres is the corpus callosum and it integrates the two. Now we’re stepping into metaphor. Essayists are the true ice skaters who need to get across. But why should any of us operate with only one set of tools? That’s like having one hand tied behind your back.It would seem that fiction allows a writer to use time differently. Nonfiction is mostly constrained by the linear necessity of then and now, perhaps looking into the future. In the novel, an ancient figure like Mary Magdalene can seamlessly play a present role.Yes, that’s right. And the long time-frame of a mountain’s life, as well as the cyclical life over time of butterfly ecology, can be integrated into the present time experienced by human characters. Fiction allowed me to intermingle the life of Erebia and its kind with the human cast, thus further smearing the invidious line between humans and the rest of nature. People are as much a vehicle for moving the butterfly plot as the reverse. I like their inextricability in this story. Fiction allows for a certain kind of wholeness.Mary Ellen Hannibal is most recently the author of Citizen Scientist: Searching for Heroes and Hope in an Age of Extinction, and a Stanford media fellow.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in November 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16470_0c89807ba823331f1ad1f93fd7859f92.jpg",
    "title": "Why Campaigns to Change Language Often Backfire",
    "description": "Posted by Julie Sedivy on October 08, 2019  In the first decades of the 20th century, people around the world began succumbing to an entirely new cause of mortality. These new deaths, due to the dangers of the automobile, soon became…",
    "category": "Culture",
    "content": "In the first decades of the 20th century, people around the world began succumbing to an entirely new cause of mortality. These new deaths, due to the dangers of the automobile, soon became accepted as a lamentable but normal part of modern life. A hundred years later, with 1.25 million people worldwide (about 30,000 in the U.S.) being killed every year in road crashes, there’s now an effort to reject the perception that these deaths are normal or acceptable.As reported in the New York Times, a growing number of safety advocates, government officials, and journalists are moving away from the phrase “car accident” on the grounds that it presumes that the drivers involved are blameless—a presumption that is correct only 6 percent of the time, according to a report by the U.S. Department of Transportation. The vast majority of such incidents are caused by drivers who make mistakes, take risks, or drive while distracted or impaired. This linguistic shift is propelled by passionate advocates like Jeff Larason, who runs a blog and Twitter account called Drop the “A” Word, and Mark Rosekind, head of the National Highway Traffic Safety Administration, who explained at a conference on driver safety why his agency shuns that particular word: “When you use the word ‘accident,’ it’s like God made it happen.”When a linguistic shift is too heavy-handed, too obviously driven by an agenda to change hearts and minds, it can run up against a response known as reactance.These advocates believe that by changing the language we hear and use, they can shift how we think about the causes of car crashes—and, they hope, the choices we make as drivers. Their faith in the persuasive power of word choice puts them in the company of people like George Orwell, whose dystopian society in his novel 1984 used language as a tool of mind control, and Republican strategist Frank Luntz, who bragged about the successes of his political wordsmithing (Don’t say “drilling for oil,” say “energy exploration”) in his 2008 book Words That Work. But just how much evidence is there for the notion that attitudes and behavior can be shaped by careful phrasing?A lot, it turns out. For example, research conducted back in 1974 showed that wording can affect how people report and remember traffic acci—uh, collisions. Elizabeth Loftus and John Palmer, psychologists at the University of Washington, showed participants movies of traffic incidents and then asked them to estimate how fast the cars were going when they smashed into each other. People who heard this version of the question, in which the verb smashed was used rather than hit, gave higher speed estimates: an average of 40 miles per hour versus 34. A week later, they were more likely to say that they remembered seeing broken glass even though there was none in the film.Opponents of the phrase “car accident” argue that language is intertwined with accountability. They can point to supporting evidence in a 2016 study published by Laura Niemi and Liane Young, psychologists at Harvard and Boston College, respectively, showing that subtle changes in syntax can tilt the apportioning of blame to victims of sexual assault. People read descriptions of assaults in which the structure of most of the sentences placed focus on either the victim or the perpetrator (She goes with Jeff when he says, “Come up to my place for a while versus Jeff says, “Come up to my place for a while,” and she goes with him.) Those who read accounts focusing on the victim rated the victim’s responsibility as slightly higher and the perpetrator’s use of force as slightly lower than those whose descriptions focused on the perpetrator.These studies are just two from among a bulging collection demonstrating the effects—albeit often subtle ones—of linguistic tinkering on thoughts and actions. But showing that changes in wording can often change attitudes and behavior doesn’t mean that a particular one will. A well-intentioned campaign to change language may prove to be ineffective or can even backfire—as was the case when Mahatma Gandhi tried to popularize the use of the term Harijan (“Children of God”) to refer to India’s “Untouchable” caste. Today, Gandhi’s term is deemed to be offensively condescending. And many attempts to rid the English language of gender bias—for example, saying person-hole cover instead of manhole cover—have been met with widespread derision or annoyance.When a linguistic shift is too heavy-handed, too obviously driven by an agenda to change hearts and minds, it can run up against a response known as reactance. Reactance is our mind’s instinctive defense against the attempts of others to control our thoughts and behavior. It is more active in some people than in others, but for all of us, sensing someone’s intent to persuade can be like the body’s detection of an invading organism, triggering a counterattack that turns us against the attempted persuasion.Reactance can be sparked even without our conscious awareness. A 2011 study led by Juliano Laran, who studies the psychology of marketing at the University of Miami, found that when people saw luxury-brand logos (for example, Nordstrom’s), they were primed to think about spending more money than when they saw budget-brand logos (such as Walmart’s). But exposure to slogans—which most people recognize as being loaded with persuasive intent—led to exactly the opposite effect, with people planning to spend less when exposed to slogans associated with luxury brands. The authors argued that the reverse-priming effect hinged on detecting the persuasive force of slogans; asking people to judge the creativity of the slogans, thereby shifting their attention away from their persuasive nature, had a disarming effect, with luxury-brand slogans now eliciting the expected response, to spend more money. Even the word that preceded phrases like “Always try to impress” or “Don’t waste your money” affected the responses. If researchers quickly flashed the word slogan, subjects did exactly the opposite of what they were being exhorted to do; if researchers instead used sentence, subjects tended to heed the message.Knowledge of persuasive intent—whether this knowledge is implicit or overt—appears to be an essential ingredient in the mind’s ability to resist outside influence. And this is why campaigns such as Drop the A-word may carry within themselves the seeds of their own destruction. Marieke Fransen, who has conducted extensive research on resistance to persuasion, says that most people are unlikely to pick up on the persuasive import of very subtle shifts in wording, such as the difference between “car accident” and “collision,” unless these are pointed out. And indeed, I only noticed that the traffic updates I tune my car radio to was avoiding the word “accident” after I became aware of the campaign opposing its use. The fact that these linguistic shifts can fly under our radar may be precisely what gives them their power. And paradoxically, the more successful an advocacy campaign is in raising our consciousness about the use of language, the more it may enable us to resist being persuaded.Julie Sedivy has taught linguistics and psychology at Brown University and the University of Calgary. She is the co-author of Sold on Language: How Advertisers Talk to You and What This Says About You and more recently, the author of Language in Mind: An Introduction to Psycholinguistics. Follow her on Twitter @soldonlanguage.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in July 2016. "
  },
  {
    "imageUrl": "http://static.nautil.us/16352_8bcf12d1a11564975f1e62b898ef0a0d.jpg",
    "title": " Butterfly Wonk Robert Pyle Pens His First Novel 44 Years in the Making",
    "description": "Posted by Mary Ellen Hannibal on August 30, 2019  Last year marked a first for 71-year old Robert Michael Pyle, the acclaimed author, naturalist, and ecologist: the publication of his long-awaited first novel, Magdalena Mountain, nearly…",
    "category": "Ideas",
    "content": "Last year marked a first for 71-year old Robert Michael Pyle, the acclaimed author, naturalist, and ecologist: the publication of his long-awaited first novel, Magdalena Mountain, nearly half a century in the making.Pyle has been investigating the butterfly for about 60 years. In that time his prolific output has included several butterfly field guides, chronicles of his adventures in the great outdoors (Where Bigfoot Walks, Thunder Tree, and Mariposa Road among them), and the magnum opus Nabokov’s Butterflies, a collection of the novelist’s butterfly writings. Pyle has also made seminal contributions to butterfly research, including his establishment of the Western Monarch’s migration pathway across the U.S. and into Mexico. Forty-eight years ago, he foresaw the acceleration of insect extinctions and founded the Xerces Society, devoted to invertebrate conservation.For those of us devoted to his nonfiction, Pyle’s imaginative treatment of the nonhuman world—he includes a butterfly and a mountain among his cast of characters—is both a surprise and perhaps a natural result of his artistic development. Like Nabokov, Pyle has consistently straddled the worlds of science and the imagination.Pyle caught up with Nautilus to chat about his work and the way butterflies shaped his early life.Is Magdalena Mountain your first foray into fiction?Fiction isn’t entirely new to me—I’ve published some short stories and poetry. I worked on this novel for 44 years. It went through ten drafts. I started it at Yale, while working on my dissertation, which was about the distribution of Washington State butterflies with respect to conserved land. It was a “gap analysis” before that term came into being. It was deeply enjoyable but very intense intellectually, and left-brain. I was inclined to write something creative at the same time.What inspired you to make a butterfly and a mountain into fictional characters?The idea came from a book called Wings in the Meadow, published by Jo Brewer in 1967. Brewer was a wonderful teacher, and her young adult novel was about a single Monarch butterfly and his migration—she called him Daneaus. I thought, I’ll write about a butterfly too, but about the Erebia Magdalena, which I had fallen in love with.Tell us about your love affair with butterflies.I started to notice butterflies in the spring I turned 11. That year, my father re-married and I acquired a step-mother and a step-brother and didn’t get along with either. But my step-mother’s family had a cabin in Crested Butte, Colorado. It was not remotely fancy then (it is a ski resort now). It was a crusty old coal town with cheap, crappy cabins. It happened to be a butterfly Valhalla. My father and I would escape the family’s endless smoky bridge games by going fishing. Luckily he never really taught me to tie flies or I might’ve gotten into it. As soon as I discreetly could, I would put my pole down and pick up my butterfly net. I would go up into these meadows full of skippers and fritillaries. I developed a passion for the satyrs, the wood nymphs. I liked their soft, striated browns and their eye spots. The Erebia Magdalena was the ultimate, the greatest among them. Something about it being black really captured me.And what drew you to center a mountain in your story?Thomas Hardy’s The Return of the Native was an inspiration. The first six pages of the book take place before a human steps on the page. Landscape is character. It is the place where things happen. One day in 1959, I looked down over a ridge and I saw about 100 people with butterfly nets. Huh? I uttered a childish version of “what the fuck?” I’d never seen anyone else with nets. I had been reading about butterflies and recognized Charles Remington and Paul Ehrlich. The living gods of lepidoptera! They were in their 30s, just kids—out with grad students near the Rocky Mountain Biological Lab. I was pathologically shy but somehow emboldened. I asked these guys if they knew where I could find the Magdalena—they told me there was a big rockslide in back of Ehrlich’s cabin where I would find them. And they were very generous with me, let me go butterfly hunting with them, sometimes even driving to pick me up. (Editor’s note: Remington convinced Pyle to apply to Yale, where he became Pyle’s mentor. Ehrlich went on to write The Population Bomb, with his wife Anne. Remington and Ehrlich both appear in Magdalena Mountain.)“Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?”The novel contains several storylines. There’s the journey of a woman who loses her memory and believes she is Mary Magdalene incarnate. A budding scientist struggles with a difficult past and graduate school. And a group of men seeking a return to pantheism convene a community called the Grove in the mountains. Did these narratives develop at different times, in different drafts, over the 44 years you were writing the novel?Yes, I did. I wanted to do this experiment. The butterfly was always at the center of the story and would drive the story, but also one of a cast. While the book is not a roman à clef, a butterfly makes things happen in the book, as it has made things happen in my life. The Magdalenian part is pseudo-religious, developed around the name of the actual black butterfly, but the story is grounded in reality. Researching Mary Magdalene—the most-written about woman in the Bible—was fascinating for me. How the mystery around the name “Magdalena” resolves is all true, as is the entomological research story. I had a lot of fun with it all.Vladimir Nabokov famously hunted butterflies in Colorado, references to which are woven throughout the book.For nine years I worked on compiling and annotating Nabokov’s butterfly writings, with his biographer Brian Boyd, and his son, Dmitri Nabokov. During the process I discovered Nabokov’s wonderful question, “Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?” It inspired the title of my book Walking the High Ridge: Life as Field Trip. But there was more. In Look at the Harlequins, Nabokov has Vadim and Bel stay at Lupine Lodge, which was surely modeled on Columbine Lodge where Nabokov stayed in 1947. Bel scribbles a poem: “Longs’ Peacock Lake: / the Hut and its Old Marmot; / Boulderfield and its Black Butterfly; / And the intelligent trail.” The black butterfly! My Erebia! And of course it was near Telluride that Humbert realized the moral consequences of his crimes in Lolita.How do you use Nabokov’s idea of a “high ridge” between art and science as an educator?When I teach environmental writing I use Nabokov’s quote to build a model. I start by trying to establish the difference between the mirror-like reflection of reality sought by science and journalism and the refractive aim of those writing essays, fiction and poetry. C.P. Snow’s Two Cultures metaphorically fall on either side of a mountain, with subjective, lyrical imagination on one side, and objective analysis on the other. This becomes a model of the bicameral mind. I’m not a brain scientist, but the region of the brain lying between the hemispheres is the corpus callosum and it integrates the two. Now we’re stepping into metaphor. Essayists are the true ice skaters who need to get across. But why should any of us operate with only one set of tools? That’s like having one hand tied behind your back.It would seem that fiction allows a writer to use time differently. Nonfiction is mostly constrained by the linear necessity of then and now, perhaps looking into the future. In the novel, an ancient figure like Mary Magdalene can seamlessly play a present role.Yes, that’s right. And the long time-frame of a mountain’s life, as well as the cyclical life over time of butterfly ecology, can be integrated into the present time experienced by human characters. Fiction allowed me to intermingle the life of Erebia and its kind with the human cast, thus further smearing the invidious line between humans and the rest of nature. People are as much a vehicle for moving the butterfly plot as the reverse. I like their inextricability in this story. Fiction allows for a certain kind of wholeness.Mary Ellen Hannibal is most recently the author of Citizen Scientist: Searching for Heroes and Hope in an Age of Extinction, and a Stanford media fellow.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in November 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16433_62dbdc10715d73b9fbbfdcda0a88848d.jpg",
    "title": " How to Get Evangelicals to Care About Climate Change",
    "description": "Posted by Elaine Howard Ecklund & Christopher P. Scheitle on September 24, 2019  Last year was among the four warmest years ever recorded, 1.5 degrees Fahrenheit above the 20th century average. The three years prior were warmer (2016 the warmest). “The six warmest…",
    "category": "Culture",
    "content": "Last year was among the four warmest years ever recorded, 1.5 degrees Fahrenheit above the 20th century average. The three years prior were warmer (2016 the warmest). “The six warmest years on record for the planet have all occurred since 2010,” the National Oceanic and Atmospheric Administration states on its website.Among those who accept that the cause of this is climate change, and that human actions play a major role, such reports tend to lead to finger pointing at climate change deniers and skeptics, who are seen as obstacles to progress on important climate policy decisions. Such finger pointing is sometimes directed at religious people, especially evangelical Christians who, either because of their theology or political conservatism, are taken to make up much of these deniers and skeptics.But after five years of surveying, interviewing, and analyzing data on what religious people think about science, we have come to understand that evangelicals’ views about climate change, and the environment more broadly, are more complex than some might assume.Scientists would do well to listen to this research.While our survey of over 10,000 Americans found that only 26 percent of evangelical Christians (compared to 33 percent of the total US population) are very interested in environmental care and only 29 percent (compared to nearly 45 percent of Catholics and 48 percent of Jews) accept human-caused climate change, our over-300 in-depth interviews reveal a more nuanced story: Religious people, particularly evangelical Christians, show more environmental concern when its connection to human health and flourishing is made clear.Here, for example, is what a church youth minister had to say about environmental care: “If we have the opportunity we should help take care of this planet that we’ve been given. Having said that, I also believe that the value of human life is higher than the value of a whale, or a species of monkey.”It’s not that evangelicals don’t care about the environment. It’s that they care about people more. Some environmental advocates are spending their energy trying to convince others of the importance of preserving our green spaces and waterways. This is important. But we think a different and more effective angle may be to redirect the conversation about environmental care to caring for people, social justice, and human flourishing, rather than for its own sake.Scientists would do well to listen to this research. And they need to make sure that religious people—in particular evangelical Christians—understand that caring for the environment is caring for people. This is especially important for members of congregations with low economic resources, who are particularly likely to think that other needs are more pressing than environmental issues, but are also likely to be located in places that are hardest hit by the forces of environmental degradation, like poor air quality.Climate change is a global problem. Perhaps many more Americans will agree on how to solve it if scientists frame the challenge as religious as well as political. A physician we spoke to, and who identifies as an evangelical Christian, linked caring for the environment and caring for humanity in similar terms: “Throughout the world the poor are often victims of environmental degradation. And so one of the most important things that the Bible teaches us is that what God cares most about, is the poor. And so to the extent that degradation of the environment is harming the poor and making them even worse off, then it is an essential issue for Christians to get engaged in.”Elaine Howard Ecklund is a sociologist at Rice University, where she directs the Religion and Public Life Program. Christopher P. Scheitle is a sociologist at West Virginia University. Their recent book is Religion Vs. Science: What Religious People Really Think. New York: Oxford University Press.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How the cosmologist Lawrence Krauss, a militant atheist, views religious scientists.This classic Facts So Romantic post was originally published in January 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16398_5939cbe46f1512291125700ee2e7236a.jpg",
    "title": "Mind the Gap Between Science and Religion",
    "description": "Posted by Sabine Hossenfelder on September 16, 2019  Have you heard that we may be living in a computer simulation? Or that our universe is only one of infinitely many parallel worlds in which you live every possible variation of your life?…",
    "category": "Matter",
    "content": "Have you heard that we may be living in a computer simulation? Or that our universe is only one of infinitely many parallel worlds in which you live every possible variation of your life? Or that the laws of nature derive from a beautiful, higher-dimensional theory that is super-symmetric and explains, supposedly, everything?I’ve heard that too. It’s how my research area, fundamental physics, often ends up making headlines: With insights about the nature of reality so mind-boggling you can’t believe it’s still science. Unfortunately, in many cases it’s indeed not science.Believing in an omnipotent Programmer is not science—it’s tech-bro monotheism.Take the idea that we live in a computer simulation. According to our best current knowledge, the universe follows rules that are encoded by a set of equations. We don’t know these equations completely (yet!), but you could rightfully say the universe computes in real time whatever are the correct equations. In that sense, we trivially “live in a computer,” but that’s just a funny way to talk about the laws of nature. You may more specifically ask whether our universe’s computation is similar to the computation performed by computers we build ourselves, that is, pushing around units of information in discrete time-steps. This is a testable hypothesis, and to the extent that it has been tested, it has been falsified. It is not easy to obtain the already known laws of nature using discrete, local operations even approximately, and this mathematical difficulty has, so far, rendered scientifically well-posed versions of the simulation hypotheses incompatible with evidence. And finally, if you are really asking whether our universe has been programmed by a superior intelligence, that’s just a badly concealed form of religion. Since this hypothesis is untestable inside the supposed simulation, it’s not scientific. This is not to say it is in conflict with science. You can believe it, if you want to. But believing in an omnipotent Programmer is not science—it’s tech-bro monotheism. And without that Programmer, the simulation hypothesis is just a modern-day version of the 18th century clockwork universe, a sign of our limited imagination more than anything else.It’s a similar story with all those copies of yourself in parallel worlds. You can believe that they exist, all right. This belief is not in conflict with science and it is surely an entertaining speculation. But there is no way you can ever test whether your copies exist, therefore their existence is not a scientific hypothesis. Most worryingly, this confusion of religion and science does not come from science journalists; it comes directly from the practitioners in my field. Many of my colleagues have become careless in separating belief from fact. They speak of existence without stopping to ask what it means for something to exist in the first place. They confuse postulates with conclusions and mathematics with reality. They don’t know what it means to explain something in scientific terms, and they no longer shy away from hypotheses that are untestable even in principle.Particularly damaging to fundamental physics has been the belief that the equations which describe nature must be beautiful by human standards. There is no rational reason why this should be so, but faith in beautiful math has become pervasive in the community. And that’s despite the fact that relying on beauty as a guide to new natural laws has historically worked badly: The mechanical clockwork universe was once considered beautiful. So were circular planetary orbits, and an eternally unchanging universe. All of which, it turns out, is wrong.And relying on beauty is still working badly for physicists. We see the tragedy playing out in the ongoing failure of ideas like a unification of the fundamental forces, a theory of everything, or new types of symmetries that experiments continue to not find. Such pretty hypotheses remain popular among physicists even though they haven’t led anywhere for decades. The recent Special Breakthrough Prize for Fundamental Physics drove home the point. The prize was awarded for Supergravity, a theory, invented in the 1970s, that is widely praised for its elegance and beauty. Supergravity has to date no observational support. It wins prizes nevertheless.This blurring of the line between science and religion is not innocuous. Resources—both financial and human—that go into elucidating details of untestable ideas are not available for research that could lead to much-needed progress. I like the idea that the laws of nature are beautiful and the universe was made for a purpose as much as everybody else, and I appreciate public interest in our research anytime. But let’s not call it science when it is really not. We have fought hard for secularism, and we don’t want religious leaders to meddle in scientific debate. Scientists, likewise, should respect the limits of their discipline. Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16473_bb6961095e9a6dc1aac2af24ea88fc52.jpg",
    "title": "The Strange Physics of How Babies Talk",
    "description": "Posted by Brian  Gallagher on October 09, 2019  Like all new parents, I must sound like a kook when I babble along with my 9-month-old daughter. That’s okay: It delights her. I sometimes ask her what she might mean as she offers some…",
    "category": "Matter",
    "content": "Like all new parents, I must sound like a kook when I babble along with my 9-month-old daughter. That’s okay: It delights her. I sometimes ask her what she might mean as she offers some apparently affirming utterance and looks at me with her big blue eyes: Oh, you like it when daddy lifts you? Ah, you’re thankful for a new diaper? Her vocalizations—the squeals and whoas and yah-wahs—can have surprising verve and a kind of ecological significance. My daughter’s noises, scientists say, “catalyze” me to produce “simplified, more easily learnable language.” My daughter is now regularly saying “mama.” My wife has taken this to mean the baby wants her attention. So I felt a little disheartened after reading a 2017 paper in Neuroscience & Biobehavioral Reviews, titled “The growth of language: Universal Grammar, experience, and principles of computation.” Charles Yang, a computer scientist at the University of Pennsylvania who studies language acquisition, wrote, along with colleagues, that, “Despite the prevalence of sounds such as ‘mama’ and ‘dada,’ the combination of consonants and vowels in babbling have no referential meaning”—they are nothing more than “rhythmic repetitions of nonsense syllables.” (Sorry, Danielle!) This lack of “semantic content” is nothing to be bummed about, though, for “babbling merges linguistic units (phonemes and syllables) to create combinatorial structures,” the bread and butter of language learning.“Each grammar defines probabilities for sentences,” says theoretical physicist Eric DeGiuli.The puzzle of how language can be so learnable has had me, of late, utterly perplexed. Babies are helpless. I can see how, with some instruction, a child can pick up on a new idea or skill, like shoe-tying—they can understand what you’re saying and mimic movements. Yet my daughter will be obeying grammar reliably and effortlessly, if a bit crudely, before she’s tying her own shoes. It’s a wonder babies learn anything, let alone a language. Parrots, great speech mimickers that they are, can’t babble, or break speech down into its discrete units. Yang and his colleagues note that the “best that Alex, the famous parrot, could offer was a rendition of the word ‘spool’ as ‘swool,’ which he evidently picked up when another parrot was taught to label the object.” Parrots don’t, in other words, partake in the phenomenon of Merge, the engine of hierarchical linguistic structures, a mental technology which starts to rev up when infants start to vocalize. All human languages have an “inaudible and invisible hierarchical structure” that, when we’re children, we impose on sound sequences we hear, writes linguist David Adger in his Nautilus feature, “This Simple Structure Unites All Human Languages.” “We hear sounds or see signs, but our minds think syntax.” Which is why infants can learn any language. There’s something deeply hidden in the mind that recognizes syntax—or the peculiar grammar of each language—as if it were as apparent as a face. “The full scale of linguistic complexity in a toddler’s grammar still eludes linguistic scientists and engineers alike, despite decades of intensive research.” Perhaps it’s no surprise, then, that physicists are now chiming in. Take a recent paper from Eric DeGiuli, a theoretical physicist, formerly at École Normale Supérieure, in Paris, now at Ryerson University in Toronto. In his paper, he argues that a language—or specifically a “context-free grammar,” which almost all languages share—can be treated as a physical object that children interact with as they hear words and sentences. In a context-free grammar, sentences have tree-like graphical structures: “The bear walked into the cave” can be broken down into a noun and verb phrase—“the bear” and “walked into the cave,” respectively—and each of those elements breaks down into more elementary units, like nouns and verbs. You might call these the tree’s leaves.This gave me the absurd image of a baby flying through a forest. The “surface” of a language that babies “touch” are all the possible configurations of words into sentences, both meaningful and meaningless. “Each grammar defines probabilities for sentences,” DeGiuli writes. In English, certain words, coming one after another, are more likely than others to make grammatical sense. DeGiuli says that the ones that do make sense get assigned, in the baby’s mind, greater weight, while word combinations that don’t make sense get assigned less weight. These different arrangements of words “are like the microstates in statistical mechanics—the set of all possible arrangements of a system’s constituent particles,” science writer Philip Ball wrote, in an article on DeGiuli’s paper. “The question is, among all possible [context-free grammars], what kind of weight distributions distinguish [context-free grammars] that produce random-word sentences from those that produce information-rich ones?” Surprisingly, it has to do with “temperature.” Not literal temperature, as in hot and cold, but a measure akin to temperature. For DeGiuli, hot is associated with randomness while cold is more deterministic, not unlike how high temperature in a room or object is due to the quick and chaotic movement of atoms. He writes that context-free grammars have “two natural ‘temperature’ scales that control grammar complexity, one at the surface interface, and another in the tree interior.” As Ball explains:DeGiuli’s theoretical analysis—which uses techniques from statistical mechanics—shows that there are two key factors involved: how much the weightings “prune” branches deep within the hierarchical tree, and how much they do so at the surface (where specific sentences appear). In both cases, this sparseness of branches plays a role analogous to a temperature in statistical mechanics. Lowering the temperature both at the surface and in the interior means reducing more of the weights. As the deep temperature is lowered—meaning the tree interior becomes sparser—DeGiuli sees an abrupt switch from [context-free grammars] that are random and disorderly to ones that have high information content. This switch is a phase transition analogous to the freezing of water. He thinks that something like this switch may explain why, at a certain stage of development, a child learns very quickly how to construct grammatical sentences.The idea that language locks in place, as water does when it’s chilled, is a weird, attractive idea. It’s fair to say that something has “crystallized” in the mind of child able to talk. How that exactly happens, though, as in so much in the natural world, on which theoretical physics turns its gaze, is difficult to say. As I play with my daughter and listen to her form proto-words, I sympathize with Yang and his colleagues: “The growth of language is nothing short of a miracle.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16401_19c768e48aca9308d1a11fe86157731f.jpg",
    "title": " What Trump’s Simplified Language Means",
    "description": "Posted by Brian  Gallagher on September 18, 2019  Acouple years ago, I was surprised that a panel called “The Press and President Trump,” held at the Columbia Journalism School, didn’t broach the subject of mental illness. Just over…",
    "category": "Culture",
    "content": "Acouple years ago, I was surprised that a panel called “The Press and President Trump,” held at the Columbia Journalism School, didn’t broach the subject of mental illness. Just over a week earlier, at a psychiatry conference at Yale, a group of the attendees announced that Trump has a “dangerous mental illness.” “I can recognize dangerousness from a mile away,” James Gilligan, a professor at New York University, who’s worked with murderers and rapists, told the conference. “You don’t have to be an expert on dangerousness or spend 50 years studying it like I have in order to know how dangerous this man is.” In the Q&A segment, I stepped up to the mic and mentioned this, and asked whether the health status of President Trump’s mind would become a more prominent story for journalists in the months ahead. One of the panelists, Carolyn Ryan, a senior editor at The New York Times, demurred. She noted that the paper had published some op-eds voicing this concern, but was wary of having reporters cover it. It could be “dangerous,” she said, to report on something so speculative. Another of the panelists, Matt Bai, a national correspondent for Yahoo! News, denounced the psychiatrists’ statements as “ideology masquerading as science.”Trump isn’t the first presidential figure to receive psychiatric scrutiny. In 1964, the magazine Fact polled psychiatrists on whether Barry Goldwater was psychologically fit for the presidency. “Warped,” “impulsive,” and “paranoid schizophrenic” was how over 1,000 psychiatrists described him. Goldwater successfully sued for libel, and in 1973, in response, the American Psychiatric Association added the “Goldwater rule” to its ethics code. It forbade making diagnoses without an in-person examination and without obtaining permission to publicly discuss the results.“Trump’s language borders on incapacity.”“With regard to Trump, however, the rule has been broken repeatedly” by psychotherapists and psychiatrists, Evan Osnos wrote in The New Yorker, in a story published following the panel. Titled “How Trump Could Get Fired,” the story demonstrates that journalistic coverage of Trump’s mental health doesn’t have to be a flimsy or biased exercise in discerning his thoughts. Over 50,000 mental-health physicians, Osnos noted, have signed a petition declaring Trump, based on copious observational data, is “too seriously mentally ill to perform the duties of president and should be removed” under the 25th Amendment to the Constitution—Section 4 states that a President can be removed if a congressionally appointed body judges him or her to be “unable to discharge the powers and duties of office.” John Gartner, the psychiatrist who started the petition, has said, “The psychiatric interview is hardly the gold standard, by the way. If you have massive amounts of information about a person’s behavior, that can be more accurate. And we have that. If the question is whether we can form a diagnosis from that information, I think it’s clear that we can. You don’t need to have an interview to know if someone has frequently lied or has violated the rights of others.”Osnos wrote that it’s not just psychiatrists who have serious grounds for worry:Bruce Blair, a research scholar at the Program on Science and Global Security, at Princeton, told me that if Trump were an officer in the Air Force, with any connection to nuclear weapons, he would need to pass the Personnel Reliability Program, which includes thirty-seven questions about financial history, emotional volatility, and physical health. (Question No. 28: Do you often lose your temper?) “There’s no doubt in my mind that Trump would never pass muster,” Blair, who was a ballistic-missile launch-control officer in the Army, told me. “Any of us that had our hands anywhere near nuclear weapons had to pass the system. If you were having any arguments, or were in financial trouble, that was a problem. For all we know, Trump is on the brink of that, but the President is exempt from everything.”Trump’s use, or misuse, of language has also been disturbing to experts of constitutional law. Take Laurence Tribe, a Harvard constitutional law professor. He said, according to Osnos, “Trump’s language borders on incapacity.” When the president was asked to explain his reversal on branding China a currency manipulator, Trump said, of President Xi Jinping, “No. 1, he’s not, since my time. You know, very specific formula. You would think it’s like generalities, it’s not. They have—they’ve actually—their currency’s gone up. So it’s a very, very specific formula.” This response could count as an example of “gross and pathological inattention or indifference to, or failure to understand” the mandatory duties of the president mentioned in the 25th Amendment, Tribe said.To psycholinguist Julie Sedivy, it’s not Trump’s rambling language that’s worrisome, it’s his regular usage. “I think we have rarely had a president who uses such simple and simplifying language,” she said in an interview with Nautilus.And why is that concerning? “There’s some interesting research that has looked at the correlation between simple language and the tendency of U.S. presidents to behave in authoritarian ways,” Sedivy said. “There is a predictive relationship that speeches that are expressed using very simple basic language tend to precede very authoritarian acts like the use of executive orders … That certainly plays out in the use of the heavy reliance on simple notions like amazing, sad, bad, unfair. These really strip away a lot of the complexities that are behind them. They reduce information into very gross impressions. The simplification of points of view, the simplification of the good and the bad, and even just the conveyance that, ‘We’re going to make good deals,’ for example. ‘It’s going to be great.’ That this is a simple problem just waiting for someone who has the right instincts to come along and solve this, is absolutely pervasive in Donald Trump’s language.”And what is the downside of that kind of usage? “Well, I think the big downside is that it’s false. The world is a complex place. It’s not a simple environment. There are many interacting forces simultaneously that really elude simple explanations or simple solutions. One thing that I certainly have become very aware of through a couple of decades now of being a scientist is that for every simple, elegant explanation or theory we have come up with, we have discovered that the truth is actually not simple or elegant. It’s messy, noisy, complex.”By reducing complexity to simplified language, Sedivy said, “we’re essentially lying about the nature of the world.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in May 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/16451_a27e3f0172e92acc4ae5edb208992313.jpg",
    "title": "Making a Future Among the Stars",
    "description": "Posted by Brian  Gallagher on October 01, 2019  In Boca Chica, Texas, presenting SpaceX’s latest prototype vehicle, Starship, Elon Musk remembered how, 11 years ago, he got mad at his parachute supplier. His young rocket company seemed…",
    "category": "Culture",
    "content": "In Boca Chica, Texas, presenting SpaceX’s latest prototype vehicle, Starship, Elon Musk remembered how, 11 years ago, he got mad at his parachute supplier. His young rocket company seemed doomed: The Falcon 1 rocket had to reach orbit or else SpaceX, out of money and investors, would go under. It reached orbit—“Fate smiled on us that day,” Musk said—but the goal with that fateful launch was also to recover the Falcon 1 from the ocean intact. That didn’t happen. He told his parachute supplier that its parachute didn’t work. He later realized it wasn’t the parachute. That first stage rocket, Musk said, is coming in from space at about Mach 10 to 12. “It hits the atmosphere like it’s a concrete wall—and boom.” He chuckled and went on, “You actually have to orient the rocket carefully, have aerodynamic surfaces, have an entry burn to slow it down, then you’ve got to guide it through the atmosphere and do a propulsive landing. This took us many, many attempts.” Evidently it was worth it. On an outdoor stage, Musk was flanked by both the Falcon 1 and the stainless steel Starship. This was a showcase, as it were, of SpaceX’s evolution from a fledgling startup to a mature enterprise. Musk has helped to advance self-driving electric cars and an implantable brain-machine interface, but when he greeted the crowd in Boca Chica, he said, gesturing behind him, “This is the most inspiring thing I’ve ever seen.”The allure of the stars has arguably always been with us. Humans long ago didn’t just appreciate their beauty but studied their movements with stunning mathematical sophistication, relying on them to plan harvests and navigate across land and sea. Now humans seem, after decades of NASA pushing the space frontier, poised to navigate among the stars themselves—to become, as Musk often says, “multi-planetary”—with research settlements on the moon and, perhaps eventually, Mars. Humanity has a host of problems facing it. Still, I get goosebumps when I look at Starship and think about what my 9-month-old daughter will grow up believing to be normal. I won’t be molding her to be anything other than she wants to be, but I will be saying there’s never been a better time to aspire to be an astronaut or astronomer.Which reminds me of something cool. An acquaintance of mine, Michael Markesbery, the CEO and co-founder of OROS, an outdoor apparel company that uses NASA-inspired materials, shared with me the recent story of young Lily Fogels. Lily, who aspires to be an astronaut—or an astrophysicist as a “back up job”—wrote Target a letter after seeing time and again that the retail store didn’t sell NASA shirts for girls. “I want NASA clothes in the girls area because girls like space too,” she wrote. Her mother, Suzi, later told the media, “If girls aren’t exposed to NASA or other STEM (science, technology, engineering, and mathematics) related jobs, they won’t know they can aim for them.” Lily added that she hopes the future will see more female astronauts.When Markesbery got wind of this, he reached out to Lily and, with NASA’s approval, he made her some custom NASA shirts. With her red hair, Lily reminds me of the young girl who played Murph, who went on to become an astrophysicist in Christopher Nolan’s film Interstellar.Markesbery also spoke to the Chairman of the US Astronaut Hall of Fame. “We’re sending Lily and a parent to the next Hall of Fame induction ceremony at Kennedy Space Center so she can show her heroes what a future astronaut looks like,” Markesbery told me. “Her family told us Lily ‘can’t get the smile off her face.’”Lily says she can’t remember why she got interested in space—she just knows that she loves that the “universe grows and changes and there is always more to learn.” Lily’s childhood curiosity and tenacity echoes that of Chiara Mingarelli. The black-hole and gravity-wave researcher remembers how space captured her imagination. She told Nautilus that her spark for science came from playing outside in a small town in the outskirts of Ottawa, “especially around dusk,” when the light-pollution free sky darkened. “I always wondered what was out there, and if I could make a contribution to our knowledge of what was out there.” Initially, she thought the idea was fanciful. “Who was I to pretend like I could do such a thing?” she said. “But I was always really interested and I think that that was really my strength, that I never gave up. Once I found out what black holes were and that you could actually make money studying black holes—and that could be a career path—I really thought, “Well, why would you do anything else?”Assistant professor at the University of Connecticut and associate research scientist at the Center for Computational Astrophysics at the Flatiron Institute” data-credits=”” style=“width:733px”>Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16376_6cd3f3cfef62c9dbf52ea6645d16fb6b.jpg",
    "title": "Most of the Mind Can’t Tell Fact from Fiction",
    "description": "Posted by Jim Davies on September 11, 2019  Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned…",
    "category": "Culture",
    "content": "Stories, fiction included, act as a kind of surrogate life. You can learn from them so seamlessly that you might believe you knew something—about ancient Greece, say—before having gleaned it from Mary Renault’s novel The Last of the Wine. You’ll also retain false information even if you didn’t mean to. That seems like a liability: Philosophers have long concerned themselves with what they call “the paradox of fiction”—why would we find imagined stories emotionally arousing at all? The answer is that most of our mind does not even realize that fiction is fiction, so we react to it almost as though it were real.At the same time, very young children “can rationally deal with the make-believe aspects of stories,” distinguishing the actual, the possible, and the fantastical with sophistication, as Denis Dutton has written in The Art Instinct. “Not only does the artistic structure of stories speak to Darwinian sources: so does the intense pleasure taken in their universal themes of love, death, adventure, family conflict, justice, and overcoming adversity.” That may help explain why, when stories are done well, we love them so much. Just as artificial sweeteners fool our minds into thinking we’re eating sugar, stories—even weird ones like Alice’s Adventures in Wonderland—take advantage of our natural tendency to want to learn about real people, and how to treat them.Our brains can’t help but believe.There’s experimental evidence for this. Children, for example, sometimes actually believe that puppets are alive. Even animals sometimes react to pictures the same way they react to real things. The industrialized world is so full of human faces, like in ads, that we forget that it’s just ink, or pixels on a computer screen. Every time our ancestors saw something that looked like a human face, it probably was one. As a result, we didn’t evolve to distinguish reality from representation. The same perceptual machinery interprets both. The rational parts of our minds, particularly in the prefrontal cortex, do indeed know that what we’re looking at, or reading, isn’t real. One way to understand this is by thinking about optical illusions. In the Muller-Lyer illusion, we can trace and know the two horizontal lines are the same length, but at the same time appear to be different lengths. Even after you understand how an illusion operates, it continues to fool part of your mind. This is the kind of double knowledge we have when we consume fiction.These perceptual areas of our brains are very closely connected to our emotions. That’s why emotions don’t just motivate us to act in certain ways but force us to interpret the world differently. A 2011 paper, for example, explained how fear can affect vision, moods can make us more or less susceptible to visual illusions, and desire can change the apparent size of goal-relevant objects. The authors proposed that emotions offer information “about the costs and benefits of anticipated action,” knowledge that can be used swiftly, without thought, “circumventing the need for cogitating on the possible consequences of potential actions.” That’s the solution to the paradox of fiction, and why telling ourselves, “It’s only a movie,” can only partially attenuate the feelings we have about it. Our brains can’t help but believe.Jim Davies is a professor of cognitive science at Carleton University and author of Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One With the Universe. His new book, Imagination: The Science of Your Mind’s Greatest Power, comes out in November of 2019. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16355_9f482bc662193b9b94846e168dd1df08.jpg",
    "title": "What Color Really Evolved For",
    "description": "Posted by Gareth Willmer on September 01, 2019  What color were the dinosaurs? If you have a picture in your head, fresh studies suggest you may need to revise it. New fossil research also suggests that pigment-producing structures go…",
    "category": "Biology",
    "content": "What color were the dinosaurs? If you have a picture in your head, fresh studies suggest you may need to revise it. New fossil research also suggests that pigment-producing structures go beyond how the dinosaurs looked and may have played a fundamental role inside their bodies too. The latest findings have also paved the way for a more accurate reconstruction of the internal anatomy of extinct animals, and insight into the origins of features such as feathers and flight.Much of this stems from investigations into melanin, a pigment found in structures called melanosomes inside cells that gives external features including hair, feather, skin and eyes their color—and which, it now turns out, is abundant inside animals’ bodies too. “We’ve found it in places where we didn’t think it existed,” said Maria McNamara, a paleobiologist at University College Cork, in Ireland. “We’ve found melanosomes in lungs, the heart, liver, spleen, connective tissues, kidneys…They’re pretty much everywhere.”We’re just at the tip of the iceberg when it comes to fossil color research.The discoveries in her team’s newest research, published in mid-August, were made using advanced microscopy and synchrotron X-ray techniques, which harness the energy of fast-moving electrons to help examine fossils in minute detail. Using these, the researchers found that melanin was widespread in the internal organs of both modern and fossil amphibians, reptiles, birds, and mammals—following up a finding they made last year that melanosomes in the body of existing and fossil frogs in fact vastly outnumbered those found externally. What’s more, they were surprised to discover that the chemical make-up and shape of the melanosomes varied between organ types, thus opening up exciting opportunities to use them to map the soft tissues of ancient animals. These studies also have further implications. For one, the finding that melanosomes are so common inside animals’ bodies may overhaul our very understanding of melanin’s function, says McNamara. “There’s the potential that melanin didn’t evolve for color at all,” she said. “That role may actually be secondary to much more important physiological functions.” Her research indicates that it may have an important role in homeostasis, or regulation of the internal chemical and physical state of the body, and the balance of its metallic elements. “A big question now is does this apply to the first, most primitive vertebrates?” said McNamara. “Can we find fossil evidence of this? Which function of melanin is evolutionarily primitive—production of color or homeostasis?”At the same time, the findings imply that we may need to review our understanding of the colors of ancient animals. That’s because fossil melanosomes previously assumed to represent external hues may in fact be from internal tissues, especially if the fossil has been disturbed over time. McNamara says her research has also shown that melanosomes can change shape and shrink over the course of millions of years, potentially affecting color reconstructions.Further complicating the picture is that animals contain additional non-melanin pigments such as carotenoids and what is known as structural color, which was only recently identified in fossils. In 2016, a study by McNamara’s team on the skin of a 10-million-year-old snake found that these could be preserved in certain mineralized remains. “These have the potential to preserve all aspects of the color-producing gamut that vertebrates have,” she said. She hopes over time that these findings and techniques will together help us to much more accurately interpret the colors of ancient organisms—though in these early days, she doesn’t have examples of animals for which this has already changed.Many of the significant strides in this area have come out of a project that McNamara leads called ANICOLEVO, which set out to look into the evolution of color in animals over deep time—or hundreds of millions of years. The project’s starting point was that previous animal color studies largely omitted in-depth fossil analysis, leaving a significant gap by basing what we know about color mainly on modern organisms. But it has since led to even wider investigation. McNamara says it is providing fresh hints on the kinds of biological structures and processes that are essential for survival in terrestrial and aquatic environments: “It looks like we’ll be able to look into much broader, exciting questions about what it means to be an animal.”Part of her research on two fossils found in China even showed, for the first time, that flying reptiles known as pterosaurs had feathers, potentially taking the evolution of these structures back a further 80 million years to 250 million years ago. The fossils contained preserved melanosomes with diverse shapes and sizes, one of the tell-tale signs of feathers. Another project she worked on, called FOSSIL COLOR, compared the chemistry of color patterns between fossil and modern insects. Again, McNamara says, these don’t entirely map onto each other. “It’s already clear that the fossilization process has altered the chemistry somewhat,” she said, “so we’re doing experiments to try to understand these changes. We’re just at the tip of the iceberg when it comes to fossil color research.”Other researchers agree that there’s more to animal color than meets the eye. Matthew Shawkey, an evolutionary biologist at Ghent University, in Belgium, said that looking into properties and functions beyond color’s use for visual means like signaling and camouflage will be critical to understanding its true significance. “For example, how do colors affect thermoregulation? Flight?” he said. “Such functions may be complementary to, or even more significant, than purely visual functions.”Shawkey is looking into such questions, with one of his recent studies indicating that the wing color of birds may play an important role in flight efficiency by leading to different rates of heating. “What started as a novelty of deciphering dinosaur colors has turned into a very serious field which is studying the origins of key pigment systems, how the evolution of colorful structures may have helped drive major evolutionary transitions like the origin of flight, and how color is related to ecology and sexual selection,” said Steve Brusatte, a vertebrate paleontologist and evolutionary biologist at the University of Edinburgh. “When I was growing up, so many of the dinosaur books I read in school said that we would never know what color they were. But as is so often the case in science, it was silly to treat this as impossible.”Gareth Willmer is a freelance writer and subeditor based in London.\n\tThe newest and most popular articles delivered right to your inbox!\nThe research in this article was funded by the EU. If you liked this article, please consider sharing it on social media.Originally published on Horizon: the EU Research & Innovation magazine | European Commission. "
  },
  {
    "imageUrl": "http://static.nautil.us/16470_0c89807ba823331f1ad1f93fd7859f92.jpg",
    "title": "Why Campaigns to Change Language Often Backfire",
    "description": "Posted by Julie Sedivy on October 08, 2019  In the first decades of the 20th century, people around the world began succumbing to an entirely new cause of mortality. These new deaths, due to the dangers of the automobile, soon became…",
    "category": "Culture",
    "content": "In the first decades of the 20th century, people around the world began succumbing to an entirely new cause of mortality. These new deaths, due to the dangers of the automobile, soon became accepted as a lamentable but normal part of modern life. A hundred years later, with 1.25 million people worldwide (about 30,000 in the U.S.) being killed every year in road crashes, there’s now an effort to reject the perception that these deaths are normal or acceptable.As reported in the New York Times, a growing number of safety advocates, government officials, and journalists are moving away from the phrase “car accident” on the grounds that it presumes that the drivers involved are blameless—a presumption that is correct only 6 percent of the time, according to a report by the U.S. Department of Transportation. The vast majority of such incidents are caused by drivers who make mistakes, take risks, or drive while distracted or impaired. This linguistic shift is propelled by passionate advocates like Jeff Larason, who runs a blog and Twitter account called Drop the “A” Word, and Mark Rosekind, head of the National Highway Traffic Safety Administration, who explained at a conference on driver safety why his agency shuns that particular word: “When you use the word ‘accident,’ it’s like God made it happen.”When a linguistic shift is too heavy-handed, too obviously driven by an agenda to change hearts and minds, it can run up against a response known as reactance.These advocates believe that by changing the language we hear and use, they can shift how we think about the causes of car crashes—and, they hope, the choices we make as drivers. Their faith in the persuasive power of word choice puts them in the company of people like George Orwell, whose dystopian society in his novel 1984 used language as a tool of mind control, and Republican strategist Frank Luntz, who bragged about the successes of his political wordsmithing (Don’t say “drilling for oil,” say “energy exploration”) in his 2008 book Words That Work. But just how much evidence is there for the notion that attitudes and behavior can be shaped by careful phrasing?A lot, it turns out. For example, research conducted back in 1974 showed that wording can affect how people report and remember traffic acci—uh, collisions. Elizabeth Loftus and John Palmer, psychologists at the University of Washington, showed participants movies of traffic incidents and then asked them to estimate how fast the cars were going when they smashed into each other. People who heard this version of the question, in which the verb smashed was used rather than hit, gave higher speed estimates: an average of 40 miles per hour versus 34. A week later, they were more likely to say that they remembered seeing broken glass even though there was none in the film.Opponents of the phrase “car accident” argue that language is intertwined with accountability. They can point to supporting evidence in a 2016 study published by Laura Niemi and Liane Young, psychologists at Harvard and Boston College, respectively, showing that subtle changes in syntax can tilt the apportioning of blame to victims of sexual assault. People read descriptions of assaults in which the structure of most of the sentences placed focus on either the victim or the perpetrator (She goes with Jeff when he says, “Come up to my place for a while versus Jeff says, “Come up to my place for a while,” and she goes with him.) Those who read accounts focusing on the victim rated the victim’s responsibility as slightly higher and the perpetrator’s use of force as slightly lower than those whose descriptions focused on the perpetrator.These studies are just two from among a bulging collection demonstrating the effects—albeit often subtle ones—of linguistic tinkering on thoughts and actions. But showing that changes in wording can often change attitudes and behavior doesn’t mean that a particular one will. A well-intentioned campaign to change language may prove to be ineffective or can even backfire—as was the case when Mahatma Gandhi tried to popularize the use of the term Harijan (“Children of God”) to refer to India’s “Untouchable” caste. Today, Gandhi’s term is deemed to be offensively condescending. And many attempts to rid the English language of gender bias—for example, saying person-hole cover instead of manhole cover—have been met with widespread derision or annoyance.When a linguistic shift is too heavy-handed, too obviously driven by an agenda to change hearts and minds, it can run up against a response known as reactance. Reactance is our mind’s instinctive defense against the attempts of others to control our thoughts and behavior. It is more active in some people than in others, but for all of us, sensing someone’s intent to persuade can be like the body’s detection of an invading organism, triggering a counterattack that turns us against the attempted persuasion.Reactance can be sparked even without our conscious awareness. A 2011 study led by Juliano Laran, who studies the psychology of marketing at the University of Miami, found that when people saw luxury-brand logos (for example, Nordstrom’s), they were primed to think about spending more money than when they saw budget-brand logos (such as Walmart’s). But exposure to slogans—which most people recognize as being loaded with persuasive intent—led to exactly the opposite effect, with people planning to spend less when exposed to slogans associated with luxury brands. The authors argued that the reverse-priming effect hinged on detecting the persuasive force of slogans; asking people to judge the creativity of the slogans, thereby shifting their attention away from their persuasive nature, had a disarming effect, with luxury-brand slogans now eliciting the expected response, to spend more money. Even the word that preceded phrases like “Always try to impress” or “Don’t waste your money” affected the responses. If researchers quickly flashed the word slogan, subjects did exactly the opposite of what they were being exhorted to do; if researchers instead used sentence, subjects tended to heed the message.Knowledge of persuasive intent—whether this knowledge is implicit or overt—appears to be an essential ingredient in the mind’s ability to resist outside influence. And this is why campaigns such as Drop the A-word may carry within themselves the seeds of their own destruction. Marieke Fransen, who has conducted extensive research on resistance to persuasion, says that most people are unlikely to pick up on the persuasive import of very subtle shifts in wording, such as the difference between “car accident” and “collision,” unless these are pointed out. And indeed, I only noticed that the traffic updates I tune my car radio to was avoiding the word “accident” after I became aware of the campaign opposing its use. The fact that these linguistic shifts can fly under our radar may be precisely what gives them their power. And paradoxically, the more successful an advocacy campaign is in raising our consciousness about the use of language, the more it may enable us to resist being persuaded.Julie Sedivy has taught linguistics and psychology at Brown University and the University of Calgary. She is the co-author of Sold on Language: How Advertisers Talk to You and What This Says About You and more recently, the author of Language in Mind: An Introduction to Psycholinguistics. Follow her on Twitter @soldonlanguage.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in July 2016. "
  },
  {
    "imageUrl": "http://static.nautil.us/16369_9bb2b56e6b7174609d788805794a4f46.png",
    "title": "Can New Species Evolve From Cancers? Maybe.",
    "description": "Posted by Christie Wilcox on September 06, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Aggressive cancers can spread so fiercely that they seem less like tissues gone wrong and more like invasive parasites…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Aggressive cancers can spread so fiercely that they seem less like tissues gone wrong and more like invasive parasites looking to consume and then break free of their host. If a wild theory recently floated in Biology Direct is correct, something like that might indeed happen on rare occasions: Cancers that learn how to roam between hosts may gradually evolve into their own multicellular species. Researchers are now scrutinizing a peculiar group of marine parasites called myxosporeans to see whether they might be the first known example.Even among microscopic parasites, myxosporeans are enigmatic. They were first discovered nearly two centuries ago, and more than 2,000 species are recognized today. Their complex life cycles make study particularly difficult: It wasn’t until the 1980s that scientists realized the ones found in fish were the same species as those found in worms, and not completely different classes of parasite. And while most parasites are content merely to snuggle into their animal host’s tissues, myxosporeans often take up residence inside a host’s own cells.It’s not clear how or why a complex multicellular creature discarded these seemingly necessary genes along with huge chunks of its DNA.Until fairly recently, myxosporeans were considered to be protists, offshoots of the eukaryotic line that are neither plants, animals, nor fungi. In 1995, however, Mark Siddall, then at the Virginia Institute of Marine Science, and his colleagues argued that myxosporeans are weird members of the cnidarians, the group that includes jellyfish and corals. Since then, genetic studies have bolstered that position.But their location on the tree of life doesn’t explain how myxosporeans ended up with such strange traits. Myxosporeans boast some of the smallest known animal genomes. The genome of Kudoa iwatai, for example, is estimated to be a mere 22.5 megabases, considerably smaller than that of any other cnidarian genome. It’s less than one-twentieth the size of the genome of Polypodium hydriforme, a closely related cnidarian parasite.Moreover, their genomes have not just been catastrophically reduced. They specifically lack certain genes thought to be essential for multicellular life. It’s not clear how or why a complex multicellular creature discarded these seemingly necessary genes along with huge chunks of its DNA.Yet Alexander Panchin, a senior researcher at the Russian Academy of Sciences, and his colleagues have an intriguing if controversial hypothesis to explain it. Early this year, they proposed that myxosporeans initially branched off from their cnidarian kin not as independent animals, but as tumors.Evolutionary ScandalsPanchin knows the idea of cancer-derived animals sounds far-fetched—so much so that, in the paper, he and his co-authors refer to them as Scandals (an acronym for “speciated by cancer development animals”).At first, Scandals were just a thought experiment. While Panchin was writing about transmissible cancers, he heard his colleagues express surprise at the genes for complex tissues that were turning up in certain unusual but simple parasitic animals. Further conversations led to what Panchin calls the “fantastic” idea that such simple parasites could have cancerous origins. “So we took all the data and we proposed this hypothesis,” he said.According to Panchin’s three-step scenario, a Scandal would start off as a cancer, but not just any cancer. It would have to be transmissible, so that it wouldn’t die when its host did. Then the cancer would have to spread to other species, and then independently evolve multicellularity. Those steps might seem to present insurmountable barriers, and yet there’s reason to believe each one could have happened.The first step, the emergence of the transmissible cancer, is the most straightforward because we know it happens, although it is rare. Devil facial tumor disease (DFTD) has become notorious as a transmissible cancer devastating Tasmanian devils, who transmit it to one another in their bites. More common but perhaps less famous is canine transmissible venereal tumor (CTVT), a sexually transmitted disease among dogs that, according to a recent analysis by Elizabeth Murchison of the University of Cambridge and her colleagues, has been evolving as a transmissible cancer for as long as 8,500 years. (In a 2014 report, Murchison and her co-authors described CTVT as perhaps “the oldest and most widely disseminated cancer in the natural world.”)Transmissible cancers are not confined to mammals; they have also been found in mollusks. There’s no reason to think it would be impossible for transmissible tumors to arise in a cnidarian too. Cnidarians certainly aren’t immune to cancers in general. If myxosporeans are Scandals, they most likely began as tumors of other cnidarian parasites—such as their Polypodium cousins, for instance.Although the spread of a cancer to other species might seem unlikely, “it’s not unheard of,” said Athena Aktipis, an assistant professor at Arizona State University. Aktipis, who specializes in the evolution of cancer, points to cases such as that of a man with HIV who was discovered to be infected with tumor cells from a tapeworm. Such worm cancers have turned up repeatedly among people with compromised immune systems, and the known cases likely represent only the small minority of occurrences in which the source of a strange growth was tracked down. If this kind of species hopping happens right before our eyes, “maybe we should also consider the possibility that things that were cancer or cancerlike sometimes, in the right conditions, could become parasites on other species,” she said.“I think that the field has been way too cautious about talking about when cancer becomes its own species, or its own kind of organism,” Aktipis said. In her view, researchers have seen too many examples of transmissible tumors like CTVT and DFTD. “It’s a parasite. It’s a parasitic organism.”Perhaps the least likely step in the Scandal hypothesis is the one where the cancerous parasite evolves from a unicellular existence to a multicellular one with distinct hosts and stages. Myxosporeans are simple animals but truly multicellular—so if they arose from a transmissible tumor, that tumor would have had to evolve distinct cell types.Multicellularity is thought to have evolved at least 25 times in eukaryotes, the domain of life that includes complex single-celled creatures as well as plants, animals, and fungi. In animals, though, it’s believed to have arisen just once at the very base of our lineage. Some multicellular branches of the eukaryotes have reverted to unicellularity, but no animals have been known to do so (unless, like some scientists, you consider cancer itself to be a kind of reversion). As yet, there don’t seem to be lineages of any kind in which multicellularity was gained, lost, and then gained again, in keeping with the Scandal hypothesis. “We understand that this is a very improbable scenario,” Panchin said.But that doesn’t mean it couldn’t have happened. “I think it’s certainly possible that clusters of cancer cells that are transmissible could evolve to have something like a life cycle,” Aktipis said. “There’s nothing special about the evolutionary process that says you can only evolve a life cycle if you are a branch of the evolutionary tree that didn’t derive from [a part of] another organism.”Following the EvidenceIn the hope of finding more substantive evidence for the Scandal theory, Panchin and his team compared the genomes of a variety of simple species (most of them parasitic) with those of five myxosporeans, three single-celled creatures, and 29 other animals. They looked for hints of a cancerous past by checking for the absence of genes that are often lost when cells turn malignant. These include genes involved in apoptosis, the regulated self-sacrifice that purges abnormal cells from the body. Any organism evolving from a transmissible tumor would presumably lack such genes.Although the scientists had expected other parasites to be the most likely Scandal candidates, only the myxosporeans had lost key tumor-suppressing genes. So they drilled deeper and found that the myxosporeans have lost so many genes related to apoptosis that they probably can’t trigger that death pathway at all. That deficiency stood out: “Even if you look at very simplified parasites which are animals, we don’t see this degree of lack of cancer-related genes,” Panchin said.“I think that the field has been way too cautious about talking about when cancer becomes its own species, or its own kind of organism.”Aktipis thinks that Panchin and his co-authors have presented some intriguing reasons why “we should at least consider the possibility that some of the parasitic organisms that we see today might have evolved from transmissible cancers.” But it’s not case closed, she said. “This paper is a beginning for this work, not a decisive proof of it by any means.”Juliana Naldoni, a parasitologist and myxosporean specialist with the Federal University of São Paulo, isn’t convinced that myxosporeans are Scandals at all. “They are actually much more complex than initially thought and evolve quite intricate [and specific] mechanisms of interaction with their hosts,” she said. Some species also have complex features, such as cells organized into structures resembling muscles for movement, for example. She just doesn’t find it plausible that such complexity arose from a cancer.Adrian Baez-Ortega, a doctoral student and bioinformatician with Murchison’s Transmissible Cancer Group at the University of Cambridge, agrees with Naldoni. “It is a thought-provoking paper, if not a very convincing one,” he wrote in an email to Quanta. He isn’t terribly impressed by the loss of apoptosis genes, for example. “In the context of such a dramatic genome reduction, the claim that the lack of genes specifically related to apoptosis points to a cancerous origin seems rather cherry-picked,” he explained.But mostly he’s skeptical that a transmissible cancer could last long enough to evolve multicellularity. Cancer cells have incredibly unstable genomes. Although this allows them to mutate rapidly and elude their host’s defenses, Baez-Ortega pointed out that on an evolutionary timescale, “this is a very detrimental strategy. As time goes on, a good portion of a cancer’s genome becomes nonfunctional or abnormal, and this might impede not just survival, but also the development of sophisticated traits like multicellularity.” The way he sees it, “even if a transmissible cancer could have survived for millions of years, it would be much more likely to remain a unicellular parasite.”That said, he thinks the Scandal hypothesis is worth further investigation. “There is almost nothing evolution cannot do,” he said. Rather than focusing on specific missing genes, he would like researchers to scan candidate species for the diverse genomic changes that occur in cancers, from point mutations to large-scale chromosome rearrangements. “If a cancer were to become a long-lived species, all these modifications would be preserved in its genome,” he said.Even Panchin and his colleagues aren’t going all-in on the hypothesis that myxosporeans are Scandals. “I think that’s fair to say it’s probably not true,” he said. It’s just that, with the work they’ve done so far, they can’t rule it out. “We’ve been trying to refute it with the means that we have.”He added, “We are going to try to falsify the hypothesis through looking at the Malacosporea genome.” Malacosporeans are cnidarian parasites and the closest known relatives of myxosporeans, but they are so much more complex that they are clearly not cancer derived. If they, too, turn out to lack apoptosis genes, it would suggest that the myxosporean loss doesn’t stem from a cancerous past.Even if, in the end, the data suggest myxosporeans aren’t evolved cancers, Panchin noted that Scandals could still be out there waiting to be discovered. “We are hoping that maybe some zoologists who have been investigating some other peculiar kind of animal at some point will say, ‘Probably those guys are wrong about Myxosporea, but this [animal], he’s obviously a cancer.’”Christie Wilcox is an award-winning science writer who currently edits and writes scripts for SciShow. Her award-winning writing has been featured in multiple anthologies, and her bylines include National Geographic, Popular Science, and The Washington Post. "
  },
  {
    "imageUrl": "http://static.nautil.us/16159_116f7fad0fcd533f862692fddb0f0d3a.jpg",
    "title": "What Is the Human Microbiome, Exactly?",
    "description": "Posted by Margaret E. Farrell on June 27, 2019  Are you an ecosystem? Your mouth, skin, and gut are home to whole communities of microscopic organisms, whose influence on your body ranges from digesting your food to training your immune…",
    "category": "Biology",
    "content": "Are you an ecosystem? Your mouth, skin, and gut are home to whole communities of microscopic organisms, whose influence on your body ranges from digesting your food to training your immune system and, possibly, impacting your mood and behavior. What are these tiny tenants, and how do they change the way we think about human health, disease, and even identity?The human body is made up of trillions of cells—well, trillions of human cells. Around the beginning of the 21st century, scientists learned that in fact the human body contains many trillions more microbial cells—possibly three times as many. This is the microbiome: the collection of microorganisms (bacteria, fungi, and other microbes) living in and on the human body. It is an especially curious discovery—it has been with us, evolving, interacting, and helping to determine our fate as organisms, since before the emergence of the human species itself. And while scientists have known about the existence of some microorganisms on and in the human body since the discovery of E. coli, the magnitude and importance of the microbiome has only recently begun to come to light.The cells in the microbiome contain as many as 2 million genes—compare that to the approximately 22,333 genes in your own human DNA. The National Institute of Health funds The Human Microbiome Project, whose purpose is to study the various microbial communities in the human body and their roles in human health and disease. We now know that the microbiome contributes a substantial amount to human growth, development, and function. Perhaps the most popular is the gut microbiome, which impacts human digestive health (this is the science behind your daily probiotic yogurt). Aside from digestive health, some scientists are studying the relationship between the composition of the microbiome and the development of the central nervous system, and some psychologists want to take this a step further to investigate the relationship between the microbiome and phenomena like emotion, learning, and social behavior.But what is the microbiome, exactly? Two biologists, Nicolae Morar and Brendan Bohannan, of the University of Oregon, recently surveyed the metaphors scientists use to talk about the microbiome (an “organ” or a “part of the immune system”) and the human-microbiome complex (a “superorganism,” a “holobiont,” or an “ecosystem”). These metaphors influence scientific understanding and can shape medical treatment. For example, some physicians support fecal microbiota transplantation (FMT); that is, swallowing a pill full of someone else’s poo to treat malfunction of the gut microbiome. FMT follows the same basic principles as an organ transplant, and the treatment is arguably a consequence of viewing the microbiome as an organ.But this is a limited perspective, because we tend to think of organs as relatively robust to change. Barring significant interruptions, a heart will develop more or less the same in each person and remain the same over time. But the microbiome is shifty, and it responds to tiny changes in our diet, environment, and behavior—like a superorganism. It is “a collection of organisms,” the researchers write, “that interact closely and from which functions emerge that do not exist at the level of individual organisms.” This view is attractive because the human body and its microbiota work to do things that neither could do alone. For example, humans could not get nearly as much energy from digestion without the help of their microbiota, and the microbiota could not survive without a host.The superorganism view also highlights the history of coevolution between humans and microbiota. Because each needs the other to survive, they evolve in response to one another. But on the other hand, this view tends to minimize focus on the competition that can occur between bacterial populations and a human host.In general, the problem with each metaphor is that it only captures a part of what the microbiome is and does, and so the researchers conclude that there is no best metaphor, and we need all of them to truly appreciate and understand the complexity of the microbiome and its role in our bodies.But why might scientists have expected there to be one “best” metaphor for the microbiome? Well, these are not merely metaphors. They are conceptual frameworks, rigorous ways of understanding the microbiome, rooted in its nature. Think about other parts of the human body—your heart, your bones, your blood. Each of these is understood as one thing. An organ, a tissue, a fluid in the circulatory system. Even though each has multiple functions, there is no controversy over whether the heart is an organ or not. So if scientists have discovered something so essential to human health and development that it truly is a part of the human body, it is reasonable to expect that we will be able to think about it the same way we think about other parts of the body.But the microbiome also seems like a new sort of thing entirely—partially because it really is not one thing. It is trillions and trillions of things, and each of them is an organism on its own. It has many relationships with our bodies, and each is dynamic. What’s more, the microbiome is involved in almost all the bodily structures and processes that we already thought we understood—and so maybe it is not so surprising that there are so many ways to think about the microbiome.A proponent of scientific perspectivism would say that because scientists put together models and theories of a phenomenon for specific purposes, and because none of those models or theories can capture all of the details of the phenomenon at once, each one is necessarily partial. A pluralist philosopher of science would say we should use all these perspectives to put together theories about the biological world that are patchwork in a way, but all the better for it.This sounds a lot like the case of the microbiome—but these views are much more far-reaching. Scientists make models and representations of the world when they make their theories, and arguably, the inability to capture everything at once applies in many if not all scientific fields. “We believe that it is time to move beyond metaphors, and we propose a scientifically pluralist approach that focuses on characterizing fundamental biological properties of microbiomes such as heritability, transmission mode, rates of dispersal rates, and strength of local selection,” Morar and Bohannan write. “Such an approach will allow us to break out of the confines of narrow conceptual frameworks, and to guide the exploration of our complexity as chimeric beings.”The fact that there still isn’t one “best” metaphor for grasping the microbiome might tell us something much deeper about the world—that perhaps the most promising approach to understanding it is to play with a variety of perspectives, prizing none over the others.Margaret E. Farrell is a Ph.D. student in the Department of Logic and Philosophy of Science at the University of California, Irvine. Her research is focused on the history and philosophy of biology.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16142_868d5b697b168fd2f49f9c38787021da.jpg",
    "title": "Our Aversion to A/B Testing on Humans Is Dangerous",
    "description": "Posted by Scott Koenig on June 24, 2019  Facebook once teamed up with scientists at Cornell to conduct a now-infamous experiment on emotional contagion. Researchers randomly assigned 700,000 users to see on their News Feeds, for…",
    "category": "Ideas",
    "content": "Facebook once teamed up with scientists at Cornell to conduct a now-infamous experiment on emotional contagion. Researchers randomly assigned 700,000 users to see on their News Feeds, for one week, a slight uptick in either positive or negative language or no change at all, to determine whether exposure to certain emotions could, in turn, cause a user to express certain emotions. The answer, as revealed in a 2014 paper, was yes: The emotions we see expressed online can change the emotions that we express, albeit slightly. Conversations about emotional contagion were quickly shelved, however, as the public disclosure of the study sparked an intense backlash against what many perceived to be an unjust and underhanded manipulation of people’s feelings. Facebook would later apologize for fiddling with users’ emotions and pledge to revise its internal review practices.Yet there’s reason to think the backlash wasn’t entirely reasonable. The magnitude of the researchers’ manipulation was small; what users saw on their altered feeds wasn’t much different, if at all, from what they would normally see in a given week. What’s more, Facebook had universally implemented a radical change in the way people express themselves, with unknown effects on mental well-being, in merely launching their platform, said Michelle Meyer, an assistant professor of bioethics at Geisinger Health System. “But nobody was saying that they should be federally investigated, no one was suing them over this, no one was saying they were monsters or maybe had driven people to kill themselves. There just wasn’t that sort of outsized reaction,” she told me. “But as soon as you randomize people to see essentially the same thing that they were going to see anyway, given some reasonable period of time, people freaked out.”Facebook’s emotional contagion study is just one of several examples of businesses trying to better understand their products and services via randomized experiments, or “A/B tests”—that is, experiments in which subjects are randomly assigned to receive one of two different treatments and then compared on some outcome measure. Like the emotional contagion study, many of these experiments caused public outcry for reasons that may seem intuitive and obvious, such as the lack of informed consent, or the idea that it’s wrong to knowingly treat people unequally. But a recent series of studies from Meyer and her colleagues casts doubt on these explanations. Their research suggests that people have an irrational aversion to A/B tests, which could limit the extent to which important institutions like hospitals, legislatures, and corporations base their decisions on objective evidence.Across 16 studies, Meyer and her colleagues randomly assigned 5,873 participants to read vignettes of a leader deciding how best to achieve a goal. The leader could land on one of three possible decisions: to universally implement one policy, to universally implement another policy, or to conduct an A/B test to compare the two policies before choosing one. Participants were randomly assigned to see one of the three possible decisions and rate its appropriateness. The vignettes spanned a wide range of topics, from the regulation of self-driving cars to poverty reduction to increasing enrollment in employee retirement plans. And, importantly, for each topic, the researchers were careful to select two untested policies that were roughly equal in appropriateness, so that neither of the two choices would be obviously better than the other. One vignette, for example, described a hospital director’s effort to reduce infection rates by reminding doctors of standard safety protocols. The director’s options were to A) print the protocols on the back of doctors’ badges, B) print the protocols on posters to be placed in rooms where the doctors worked, or A/B) randomly assign patients to be treated by a doctor who either wore the badge or worked in a room with a poster, and then compare rates of infection between the two groups.Meyer and her team found that in nearly every situation they tested, the decision to conduct an A/B test was deemed least appropriate by a considerable margin. In other words, people preferred those in power to universally implement untested policies at their discretion instead of testing them first. This so-called “A/B effect” held up even in participants with a STEM education or with high levels of scientific literacy.When asked to justify their ratings, many participants who were shown the A/B tests expressed concern that the decision-makers in the vignettes never asked people for consent before conducting tests on them. Yet the issue of consent was raised by fewer than 1 percent of the participants who were assigned a vignette where the leader universally implemented an untested policy. This inconsistent standard for consent puzzled Meyer and her colleagues because, as they wrote in their article, “in all cases, people were subjected without their consent to one of the same two untested policies with unknown effects.” Why were the A/B tests singled out?One possible cause of the A/B effect, according to the researchers, is the “proxy illusion of knowledge,” or the belief that other people know more than they actually do. Participants may have found it unsettling to see people in power admit a need to conduct tests—admit, in other words, that they don’t know enough. “It makes you feel a little safer, a little more comfortable in your day-to-day, to imagine the director of the hospital where you’re being treated is omniscient, or knows what’s going to work, or has looked at your case specifically and has already chosen what the best course of action is,” said Patrick Heck, a postdoctoral research fellow at Geisinger Health System and a co-author of the paper. “When in fact that’s quite rarely the case.”It may also be that A/B tests are off-putting because of the cultural baggage associated with science and experimentation. “From the Nazis to Tuskegee to human vivisection to Frankenstein—most of which, by the way, were not actual scientific experiments,” said Meyer, “there’s a colloquial use of the word ‘experiment’ and also ‘random’ that has very negative connotations.”Whatever the cause, there appears to be a persistent distaste for A/B tests across a wide swath of the population. Yet abjuring randomized experiments, Meyer said, can put more power in the hands of a few. “I want to live in a world where practices and policies and treatments are as evidence-based as possible, and not based on the intuitions of people who happen to become CEO of a company or happen to become head of a hospital,” Meyer said. “There are many attributes that lead people into those positions of power, but magically knowing in advance what does and doesn’t work is not one of them.”Meyer, Heck, and the rest of their team are currently focusing their efforts on finding ways to make A/B tests more palatable. One potential strategy is to help people become more familiar and thus more comfortable with the process and purpose of randomized experiments. Also, in view of the negative connotations to words like “experiment” and “random,” it may help to frame A/B tests in different terms, like “trial.”The role of facts and analysis in public life appears to be shrinking, so such efforts to broaden the appeal of randomized experiments have arguably never been more important. “Policy and government and healthcare and all these really high-touch systems and organizations that exist to improve people’s lives are fundamentally connected to science—to social science and especially to the basic sciences,” said Heck. “And of course the workhorse for the basic sciences is randomized evaluation.”With that in mind, it may be wise for us all to shake off the heebie-jeebies and let ourselves become guinea pigs once in a while.Scott Koenig is a doctoral student in neuroscience at CUNY, where he studies morality, emotion, and psychopathy. Follow him on Twitter @scotttkoenig.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16222_dfc30173c2d819d8706f018f2ef5d63e.jpg",
    "title": "Think You Know the Definition of a Black Hole? Think Again",
    "description": "Posted by Brian  Gallagher on July 12, 2019  When I was 12, I made the mistake of watching the Paul W. S. Anderson horror film, Event Horizon. It gave me nightmares for weeks: The movie’s title refers to an experimental spaceship…",
    "category": "Ideas",
    "content": "When I was 12, I made the mistake of watching the Paul W. S. Anderson horror film, Event Horizon. It gave me nightmares for weeks: The movie’s title refers to an experimental spaceship that could create artificial black holes through which to travel, making interstellar trips trivial. But the crew, upon activating the ship’s gravity drive, ended up somewhere like Hell. Possessed by what appears to be the ship itself—it seems to acquire a will of its own—they mutilated themselves and one another. A crew member had the presence of mind to broadcast a final message, amid the screams, to any would-be rescuers: liberatis me (“save me”). Black holes have held me in a state of trembling fascination ever since. I doubt they lead to some demonic dimension but, like the existence of God, such a realm can’t, strictly speaking, be disproven, only judged improbable.I couldn’t put the possibility out of mind as I read Einstein’s Shadow: A Black Hole, a Band of Astronomers, and the Quest to See the Unseeable. The book, by science writer and editor Seth Fletcher, is about Sheperd Doeleman and the other scientists behind the Event Horizon Telescope, the goal of which was to photograph a black hole. Doeleman, Fletcher writes, thought his team “could cause a historical disjuncture—before the first picture of a black hole, and after.” A few months ago, they unveiled the picture of galaxy Messier 87’s central black hole, with gas—in hues of red, orange, and yellow—glowing around it. It was mesmerizing. It showed the shadow cast by the event horizon, confirming a prediction of Einstein’s theory of general relativity.In May, in a conversation with Doeleman, Chris Anderson, of TED, asked, “Where do you end up if you fall into a black hole?” Being in Vancouver, Doeleman joked, “Vancouver. Here we are.” But then went on to call black holes “the central mystery of our age.” Why? “Because that’s where the quantum world and the gravitational world come together. What’s inside is a singularity, where all the forces become unified because gravity finally is strong enough to compete with all the other forces”—the strong, weak, and electromagnetic. But we can’t see the singularity. “The universe has cloaked it in the ultimate invisibility cloak. We don’t know what happens in there.”And yet the mystery goes deeper: What might be more puzzling than the innards of a black hole is the trouble of defining one in the first place. That’s what Erik Curiel found out when he asked theoretical and experimental physicists, mathematicians, and philosophers, “What is a black hole?” Curiel, a physicist and philosopher himself, at the Munich Center for Mathematical Philosophy, reported on his experience chatting with these researchers in a Nature Astronomy paper, titled “The many definitions of a black hole.” One response in particular, he thought, from Beatrice Bonga, who specializes in gravitational waves at the Perimeter Institute, was emblematic of the rest. “Your five-word question is surprisingly difficult to answer,” she told him, “...and I definitely won’t be able to do that in five words.”Isn’t a black hole just a region in space where matter has become so dense that not even light can escape its gravity? Simple? Well, no. That’s the layman’s way of expressing the effect of the event horizon, which was classically defined, Curiel says, as “the boundary of the causal past of future null infinity.” He explains that the definition “tries to take the intuition that a black hole is a ‘region of no escape’ and make it precise.” Curiel thought many of the folks he spoke to would bring this up, but most didn’t. Those who did mentioned it partly to point out its problems. Curiel writes:This definition is global in a strong and straightforward sense: the idea that nothing can escape the interior of a black hole once it enters makes implicit reference to all future time—the thing can never escape no matter how long it tries. Thus, in order to know the location of the event horizon in spacetime, one must know the entire structure of the spacetime, from start to finish, so to speak, and all the way out to infinity. As a consequence, no local measurements one can make can ever determine the location of an event horizon. That feature is already objectionable to many physicists on philosophical grounds: one cannot operationalize an event horizon in any standard sense of the term. Another disturbing property of the event horizon, arising from its global nature, is that it is prescient. Where I locate the horizon today depends on what I throw in it tomorrow—which future-directed possible paths of particles and light rays can escape to infinity starting today depends on where the horizon will be tomorrow, and so that information must already be accounted for today. Physicists find this feature even more troubling.Sean Gryb, a quantum gravity theorist at the Perimeter Institute, told Curiel, “The existence of [a classical event horizon] just doesn’t seem to be a verifiable hypothesis.”Curiel spoke to some researchers who made black holes out to be even more exotic than I thought possible. For example, theoretical physicist Domenico Giulini, of Leibniz University Hannover, was skeptical that black holes were physical things at all. “It is tempting but conceptually problematic to think of black holes as objects in space, things that can move and be pushed around,” he said. “They are simply not quasi-localized lumps of any sort of ‘matter’ that occupies [spacetime] ‘points.’” But others, like Ramesh Narayan, at the Harvard-Smithsonian Center for Astrophysics, are less fanciful. “A black hole is a compact body of mass greater than four solar masses—the physicists have shown us there is nothing else it can be.”Black holes, it cannot be denied, are both simple and complicated. For Chiara Mingarelli, a gravity-wave researcher, it’s those two qualities that make black holes so fascinating. “It’s this interesting duality of them being really simple and really complicated. So they’re really simple. You can describe them by their mass and their spin, right? But what’s past the event horizon? What does the singularity actually look like? Is it actually this point of infinite curvature? It is some sort of quark soup? What does it look like?” She imagines that, as light travels toward the singularity, in all sorts of wonderful orbits, there must be fireworks. “I try to imagine it as like a water fountain, having light coming out and then falling back in on itself as it approaches the event horizon.”Watch the rest of Mingarelli’s interview with Nautilus here. She discusses her reaction to the detection of gravitational waves, how they compress space and time, and whether it is fair for only a few people to win the Nobel Prize for spotting them.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16180_e1924e8eb73074038bbea5e2a0bffa40.jpg",
    "title": "The Dr. Strange of the American Revolution",
    "description": "Posted by Brian  Gallagher on July 04, 2019  I ascribe the Success of our Revolution to a Galaxy,” Benjamin Rush wrote to John Adams, in 1812. He wasn’t invoking the astrological. It was commonplace then to associate a bright…",
    "category": "Culture",
    "content": "I ascribe the Success of our Revolution to a Galaxy,” Benjamin Rush wrote to John Adams, in 1812. He wasn’t invoking the astrological. It was commonplace then to associate a bright assembly of people with the starry band in the night sky that Chaucer called “the Milky Wey.” Yet Rush crossed out “a Galaxy” and wrote in, perhaps for the sake of specificity, “an Illustrious band of Statesmen—philosophers—patriots & heroes.” Historian Jill Lepore has written that, in the “comic-book version of history that serves as our national heritage, where the Founding Fathers are like the Hanna-Barbera Super Friends, Paine is Aquaman to Washington’s Superman and Jefferson’s Batman.” And Rush? I posed this question to Stephen Fried, author of the recent book, Rush: Revolution, Madness & the Visionary Doctor Who Became a Founding Father. Fried replied, “Dr. Strange.”An apt choice. Rush was a strange, or a strangely gifted, man, and one of the youngest—at 30—to sign the Declaration of Independence. Mentored by his fellow Philadelphian Founding Father Benjamin Franklin, Rush would become a towering intellect with a remarkable admixture of passions. In his twenties, politics was a minor one—medicine and chemistry were his strong callings—but a quiet association he maintained with the Philadelphia Sons of Liberty turned fateful when, one summer day in 1774, he was invited to join in the welcoming of the delegations for the first Continental Congress. He would make lasting impressions on Washington and Adams, among others, participating in conversations, some he hosted at his home, of fervid patriotism, cross-pollinating ideas of how to advance the revolution. It would lead Rush to task Thomas Paine with writing, and he editing, what would be called Common Sense, an anti-monarchical and pro-Independence pamphlet.During the Revolutionary period and afterward, Rush was one of the most preeminent physician-scientists alive (Rush University Medical Center, in Chicago, is named after him). He lent his fierce pen to a variety of social causes, among them emancipation for America’s slaves and rigorous education for girls and women. Though a Christian, Rush was religiously tolerant of all faiths and a champion of secularism. He encouraged Thomas Jefferson to finish what would become the Jefferson Bible, a non-supernatural reworking of the Old and New Testaments. In Rush, Fried confirms what earlier biographical writings on the doctor had observed: that history misunderstood him, “had not taken him seriously enough as a founder, a writer, a teacher, and a revolutionary in politics, medicine, religion, public health, and philosophy.”Rush was a founder of American psychiatry. As a scientist, he was fascinated by mental illness; as a doctor, he was horrified by its treatment. Where most saw the workings of God or demons in the manners of the mentally ill, Rush saw malfunctioning parts. It was no sin to be deranged. The mentally afflicted deserved sympathy and sophisticated care. They had “diseases of the brain,” he said, not character flaws of failures of will. Rush was a pioneer in removing psychiatric patients from prison conditions. He unchained them, gave them proper lighting, and had them exercise in the hospital gardens.One of Rush’s first psychiatric cases, in the 1780s, involved Philadelphia’s most publicly mad woman, an old widower named Hannah Garrett who harbored royal delusions and made a habit of catching flying insects and dismembering them. She’d keep their bodies in a jar “for the presumption of daring to bite the King’s daughter.” Rush diagnosed her with a “grief-induced madness” triggered, in middle life, “by the loss of her husband.” Rush and his staff, as part of her treatment, regularly spoke to her, to try to understand what Rush viewed as her “errors of thinking” in order to heal her.In 1786, the American Philosophical Society relaunched its annual oration—an occasion for a sterling lecture on a matter of importance—and wanted Rush to give it, in Philadelphia. The city was, at the time, the site of an American Enlightenment. Some of the country’s brightest minds would attend Rush’s talk. Newspapers advertised the event; a thousand tickets were printed. “The subject of the lecture was not announced,” Fried writes. “Dr. Rush himself was the draw.” He knew that nearly everything he’d say would amount to blasphemy for someone he respected, because his subject was “An enquiry into the influence of physical causes on the moral faculty,” the moral faculty being the “power in the human mind of distinguishing and choosing good and evil…virtue or vice.”Rush denounced the prescription of prayer and clerical scoldings for those suffering mental ailments. These are, he said, “only to be cured by medicine…[and] laws for the suppression of vice and immorality will be as ineffectual as the increase and enlargement of jails.” He also speculated on, among other things, suicidal thinking as a kind of contagion “often propagated by means of newspapers,” anticipating contemporary discussions about how media should report on people taking their own lives. His peroration would resonate with anyone today intrigued by the implications of genetics on innate talent and behavior:I am not so sanguine as to suppose that it is possible for man to acquire so much perfection from science, religion, liberty and good government as to cease to be mortal; but I am fully persuaded that from the combined actions of causes, which operate at once upon the reason, the moral faculty, the passions, the senses, the brain, the nerves, the blood and the heart, it is possible to produce such a change in the moral character of man, as shall raise him to a resemblance of angels—nay more, to the likeness of God himself.Decades later, Rush would flesh out this thesis in Medical Inquiries and Observations, upon the Diseases of the Mind. In a letter to John Adams, Rush explained that he’s attempting “to show that the mind and the body are moved by the same causes & Subject to the same laws.” He viewed psychological vigor as a product of the brain, a vulnerable and comprehensible organ, subject to damage and decay that could diminish moral conduct. He expected his peers to give him no shortage of criticism. “But time I hope will do my Opinions justice,” he wrote. “I believe them to be true and calculated to lessen some of the greatest evils of human life.”Rush was right, of course. The brain controls our behavior, moral or not. If it is injured, or if we’re subject to certain stimuli, our ethics can be affected. As Columbia University clinical psychiatrist Carl Erik Fisher once told Nautilus, “What makes someone a pedophile or struggle with sexual impulses if not their brain? And, if the locus is in their brain, then what makes their responsibility any more or less than someone who has a tumor? A hundred years from now we may be able to talk about brain function in a deterministic way, even if there’s not such a clear obvious organic issue.”Rush wasn’t as prescient about other matters. He infamously suggested, for example, that Africans’ dark skin was a hereditary form of leprosy. And in his own time he was excoriated for his practice of bloodletting, which he believed to be efficacious up until his own death, having performed it on himself. But he argued well and early for the benefits of basic sanitation in general, and in hospitals in particular. Maintaining health was always as crucial as upholding dignity. To his hospital’s board, he wrote that using dungeon-like cells “any longer for the reception of mad people will be dishonorable both to the Science and Humanity of the city of Philadelphia.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16205_4f46995ccd92374fee622f3c79daf227.jpg",
    "title": "The Neural Similarities Between Remembering and Imagining",
    "description": "Posted by Jim Davies on July 10, 2019  Imagine a living room. Not yours or your friend’s or one you saw in a home makeover show, but one purely from your imagination—perhaps your ideal living room. You should have no trouble…",
    "category": "Biology",
    "content": "Imagine a living room. Not yours or your friend’s or one you saw in a home makeover show, but one purely from your imagination—perhaps your ideal living room. You should have no trouble doing it: We take this kind of imagination for granted. Rarely do we find ourselves wondering how the mind chooses what objects to put into these novel scenes and which ones to exclude. But it’s worth reflecting on, perhaps especially for creative types, because our visual imagination appears to be constrained by regularities in visual memories. Diversifying what you see may mean enriching what you can imagine. In a recent study Irish neuroscientist Eleanor Maguire, of University College London, had people imagine novel scenes and compared this to people imagining single objects against a white background. She found that different parts of the brain were implicated when imagining a rich scene compared to imagining a single object. One is the hippocampus, which, according to scene construction theory, is important for both consolidation of long-term memory and the imagination. Spatial navigation and planning both rely on the hippocampus and depend on the ability to create coherent spatial scenes. One hypothesis of the theory is that the closer you perceive things to be to each other—a couch and a coffee table, for instance—the more likely you are to later retrieve and put them into a scene when it is imagined, too. It also suggests that, as you create a scene, your brain starts a feedback loop between the hippocampus and the visual cortex.Diversifying what you see may mean enriching what you can imagine.The other is the ventromedial prefrontal cortex (vmPFC), which plays a role in, among other things, deliberation and self-control. Maguire found that the vmPFC was quite active while subjects imagined scenes, but was less so while they imagined objects. She also found that it was activated before the hippocampus. This suggests that the information flow goes from the vmPFC to the hippocampus, rather than the other way around. It looks like the hippocampus takes instructions from the vmPFC for how a scene should look—and this is what happens when you recall a memory. As Maguire and her colleagues concluded, episodic memory and scene imagination “share fundamental neural dynamics and the process of constructing vivid, spatially coherent, contextually appropriate scene imagery is strongly modulated by vmPFC.”In my laboratory, we try to model this kind of imagination in software. My graduate students and I made an imagination engine to imagine scenes the same way people do. We would give it a prompt word, like “mouse,” and it would find objects in its database, or the words for them, that tend to appear in photographs with mice. The engine would then put five things associated with a mouse in an imagined scene. We quickly encountered a problem that humans easily avoid, though: The engine, given the word “mouse,” would generate both a cat and a computer keyboard. People don’t do this—they pick objects associated with one meaning of the term rather mix and match objects associated with both.My graduate student Michael Vertolli sought to solve this problem. In a 2017 study, he used the concept of co-occurrence, which, in the context of a spatial scene, is similar to proximity. He had our imagination engine look not only at the correlation between the prompt word and a retrieved object, but also all of the correlations among the retrieved objects. “Computer keyboard” co-occurs with “mouse,” but it doesn’t appear in the same photographs as mousetraps. So one of them would be swapped out for a different word. This process would repeat until all of the objects in the imagined scene correlated with each other to a certain threshold coherence. Different regions of the hippocampus appear to have different functions—the CA3 subregion gets input for memory, and this is fed into the CA1 subregion, which scientists suspect detects coherent patterns. This might be where the thresholding mechanism takes place.What might seem unintuitive about this is that, when you imagine a visual scene, it doesn’t feel like things are being replaced one by one. Apparently we are not conscious of the candidates our minds reject for placement in a scene—we are only privy to the things that end up there. If you imagine a birthday party, for example, the cake, candles, balloons, and drinks might seem to spring into your mind all at once. But if you look around to see what’s there, you might find that it wasn’t quite as fleshed-out as you thought. The image in your mind, if you can create it at all, needs to fill in where you cast your mind’s eye. You can even tell that, in other parts of the image, things will fade and disappear until you attend to them again.It’s long been common knowledge that imaginings are re-combinations of bits from memory. But now we’re seeing that the act of recalling something that happened to you looks very much like what happens when you imagine something new.Jim Davies is a professor at the Institute of Cognitive Science at Carleton University in Ottawa, and author of Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One with the Universe, and co-hosts the “Minding the Brain” podcast. His sister is novelist JD Spero.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16167_c8b4d673d23385e09789e7db8e3f431a.jpg",
    "title": "Why Our Postwar “Long Peace” Is Fragile",
    "description": "Posted by Brian  Gallagher on July 03, 2019  You could be forgiven for balking at the idea that our post-World War II reality represents a “Long Peace.” The phrase, given the prevalence of violent conflict worldwide, sounds more…",
    "category": "Culture",
    "content": "You could be forgiven for balking at the idea that our post-World War II reality represents a “Long Peace.” The phrase, given the prevalence of violent conflict worldwide, sounds more like how Obi-wan Kenobi might describe the period “before the dark times, before the Empire.”And yet, the “Long Peace” has been a long-argued over hypothesis about the relative absence of major interstate conflict since 1945: Have mechanisms like nuclear deterrence, democratization, economic pacts, and international organizations like the United Nations really fostered an enduring trend of peaceful co-existence, or is this just a statistical fluke—a normal interlude of relative calm before another global-scale conflagration?“This debate has been difficult to resolve because the evidence is not overwhelming, war is an inherently rare event, and there are multiple ways to formalize the notion of a trend,” Aaron Clauset, a computer scientist at the University of Colorado, Boulder, wrote in a paper last year. “Ultimately,” he goes on, “the question of identifying trends in war is inherently statistical.” Which is why he took a deep dive into the data collected by the Correlates of War Project; it has, among other things, information on the onset years and battle deaths (civilian casualties excluded) of 95 interstate conflicts between 1823 and 2003.He came out with some intriguing calculations. In a section titled “The long view,” he says that, if we assume “the long peace does not reflect a fundamental shift in the production of large wars,” then we could estimate the “waiting time for a war of truly spectacular size”—one with a billion battle deaths (World War II, for comparison, had 16.6 million battle deaths). How long would we have to wait? The median forecast is 1,339 years, but the range of waiting times is super variable—there’s a 5 percent chance that such a catastrophic event would happen in under 400 years, and a 95 percent chance it would happen within the next 11,000 years.Nautilus caught up with Clauset to discuss his findings.Do you take a side in the debate about whether the “long peace” is an enduring trend?I’m only willing to believe things that can be convincingly proved using hard-nosed statistics and data. In this case, the hypothesis that the long peace will stick is not on very firm statistical grounds. There are other people who are more optimistic, who believe in their hearts that the long peace will stick. That’s fine. They look at other sources of evidence on peace-promoting mechanisms—for instance, it’s been documented and shown very convincingly that the spread of democracy, the creation of peacetime alliances like NATO, the development of economic ties among nations, and the work of international organizations, like the UN, apparently have a role in reducing the risk of war. But in my analysis of this data, I think the evidential case for optimism is somewhat weak. The long peace may be more fragile than we have thought. We should not rest on our laurels. We have to maintain vigilance in order to make sure that the other processes, geopolitical or social processes that tend to unravel these peace-promoting mechanisms, don’t actually succeed.Why do you think the evidence for the “long peace” is weak?If you replay the statistics of the past 200 years, you would see periods of time that are as peaceful and at least as long as this long peace. This approach is what you would call a change-point analysis, because we know when we think something changed in the history of the production of wars. Can we tell a difference between the statistics on one side of that change-point from the statistics on the other side? Looking at the sizes of wars and then separately at the rate at which they’re produced, we don’t see any evidence of a change.Are you optimistic or pessimistic about there being another big war soon?I think of myself as an optimist. I look at the broader set of evidence, and I see that there are reasons to believe things are changing. The pattern appears consistent with an idea that there is a trend here—that the risk of a large war among some nations is going down in part because there has not been a large war on the European continent since World War II, a clear success of efforts to promote peace. But if those efforts really are having an impact, we may not know for a long time how big that impact is. There’s an upper bound on how big that impact can be as a result of the fact that we can’t tell conclusively that the statistics of war have really changed. The likelihood that any one of us individually will die in a war has gone down—that’s Steven Pinker’s basic thesis—but that doesn’t contradict the finding that the production-rate of wars at different sizes is stable. What makes things so interesting is that so much has changed in the world over this 200 year period. Public health. The world population. The number of nations. Technology has revolutionized everything. A wide variety of completely crazy geopolitical events have unfolded—plenty of nastiness in terms of nations becoming more autocratic along with nations being more democratic, et cetera. And yet, evidently the statistics of war have remained stable despite the changes in either direction. Why? I don’t know. I hope somebody else will look into this and tell me what the answer is.Is there a way to know how much longer the “long peace” would have to last in order to call it a real trend?We can generate a synthetic sequence of war events from a model that says that there’s no change between these two periods—1823 to 1945, and 1945 to now—and then ask whether or not the pattern that we observe in the postwar period is unusual with respect to this natural variation, the stationary model that we produced. This model represents what history says happens and how it should happen, which allows us to sort of replay history. The long peaces are relatively common statistical patterns under this model just because large wars are extremely rare. Because it’s fully specified, we can use that model to ask a question about the future: If that relative decrease in war were to stick around for a much larger period of time, how many more years would it need to hold before you could really say, yes, this postwar period is different from what we would expect from a stationary model where nothing’s changed. That’s where the number of 100-140 years comes out. At that point, we can then say definitively, by only looking at the severities and frequencies of wars, that the long peace is different. If you had a more sophisticated model that included information like where wars were fought, who was fighting in them, and why, et cetera, you could maybe shrink that number to some degree.What does your analysis of war statistics say about the future?Besides looking at the next 100 years, there’s the sort of, “Let’s take this argument to the logical conclusion and see what we can learn from it,” which is, “Let’s think about a huge event, like a billion battle deaths.” No one should use the likelihood of these events as predictions because they are, instead, mathematical calculations that are useful for probing our understanding—the billion-death population is useful because it allows us to think about the concept of stationarity. The statistics of war in the last 200 years appear to be very stable. If that’s true, is it plausible that that stability could extend backwards in time? Well, the billion battle-death calculation suggests that that cannot be true, because the distribution of how long you wait before you see a billion deaths includes a lot of human history. And we did not see an event that was that big during that part of the history. So it suggests that the rules of conflict have changed in human history, just not in the past 200 years. But if the rules of conflict are stable in the future, then it’s possible that a billion-death conflict could be generated by this stable war generating system. The plausibility of this forecast is unknowable. It may be that the mechanisms promoting peace, like peacetime alliances, economic ties, et cetera, really are having an impact. It’s just taking a long time for them to really show that they have an impact on the statistics of war—in which case, the likelihood of a billion-death war is actually much lower than the model would say.What is the Correlates of War Project?It is essentially one of the gold standard sources of data on violent conflicts, especially interstate wars. It has been put together by conflict researchers over many years. It’s high quality, low bias, low variance in terms of its coverage, so it’s essentially a fairly complete list of interstate wars that have happened during this time period. For the very early part of the time period, in the early 1800s, there’s some debate about what constitutes a nation as a member of the international system. But including or excluding those things doesn’t change the results that we have in this paper. It’s pretty easy to count dead bodies and to identify when wars start because we have formal and informal declarations of war—the Vietnam War is included.Why didn’t you factor in civilian deaths in your analysis?I did not look at those because that changes what we mean by war. Different kinds of wars have different kinds of civilian deaths versus battle deaths. So by focusing on just battle deaths alone I have a variable that is consistent across 200 years. It’s relatively consistently measured, even though things inside the wars themselves are changing, like the weapons and the likelihood of dying in a gun battle. It’s possible that if you had the technology of today for the wars of the 1800s, then they might have been bigger. I don’t know. They might have been smaller, too. It’s hard to figure out what that counterfactual would really be like.Why do you focus on only interstate wars?Interstate wars are special. Some civil wars turn into interstate wars, but it’s not that common. Focusing specifically on interstate wars alone gives us a relatively clean sense of a certain important class of conflicts, and it means that we are excluding things like civil wars. The American civil war is not in the data set. The statistics of civil war are currently being hotly debated inside the peace research community, in part because some evidence suggests that the rate of civil wars starting has been going up since the fall of the Soviet Union, and that there are countries that actually retract in a civil war cycle, in which they have a civil war and then there’s a period of peace and the civil wars start again. To get into the interstate war data set, it has to be a war between recognized members of the international system. For interstate-like conflicts, civil wars et cetera, it can be much messier. For instance, Iran’s funding of Hezbollah is an example of a state funding a non-state entity in a different country to carry out things that are like attacks. But in order to make sense of this incredibly complex system of human conflict, it can be useful to drill down deeply into one particular part, and this paper attempts to do that with the interstate wars.Might there be something about war that makes it immune to our attempts to decrease its frequency?Lewis Fry Richardson, a pioneer of mathematical techniques in weather forecasting who took an interest in the statistics of war very early in the 20th century, along with more recent researchers, argue that war—interstate wars especially—have a special kind of statistics in which they’re just rare. Especially the big ones. And, as a result, it’s difficult to convince yourself, if you just look at the statistics of the frequency and severity of these things, that the risk of large wars has actually gone down. Because if they don’t occur very often to begin with, then you might just get lucky.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why the Zen wars were vicious.This classic Facts So Romantic post was originally published in March 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16067_40e244ce06f98f84c452989ca388b8a2.jpg",
    "title": "Game of Thrones and the Evolutionary Significance of Storytelling",
    "description": "Posted by Brian  Gallagher on May 31, 2019  I told myself it was absurd to be discontented with the way Game of Thrones ended. Why should I feel anything for the fate of a fictional world? Even so, I watched with interest, on YouTube,…",
    "category": "Biology",
    "content": "I told myself it was absurd to be discontented with the way Game of Thrones ended. Why should I feel anything for the fate of a fictional world? Even so, I watched with interest, on YouTube, videos of how several of the episodes—particularly “The Long Night,” “The Bells,” and “The Iron Throne”—could have been re-written to heighten thematic consistency, the drama of certain deaths and the coherence of other characters’ story arcs, and the feeling that the conclusion, including Westeros’ geopolitical fallout, made sense. No, I did not sign the Change.org petition to have season eight re-made with “competent writers,” which, as of this writing, has over 1.6 million signatures. But this outrage at not having the story pan out in a supposedly satisfying fashion is telling of how much power the tale possessed. And this sort of fracas is not unprecedented. The New Yorker wrote about how, in 2012, fans pleaded for the game developer Bioware to remake the ending of the last entry in their space-adventure trilogy, Mass Effect 3—and the fans got it.What is it about storytelling that gets people so riled up when they feel it goes wrong? Perhaps the fury stems from the evolutionary burden stories, and storytellers, have had to carry. In the Game of Thrones series finale, “The Iron Throne,” Tyrion Lannister, known as “The Imp,” renowned for his wit and erudition, gives a speech to the heads of the leading houses of the realm. “What unites people?” he asks. “Armies? Gold? Flags?” He shakes his head. “Stories. There’s nothing in the world more powerful than a good story.”Tyrion is quite right. In a 2017 study, researchers wrote, “The universal presence and antiquity of storytelling indicates that it may be an important human adaptation,” one that encouraged group cooperation and made skilled storytellers desirable social partners. The study was led by Daniel Smith and Andrea Bamberg Migliano, anthropologists at University College London. Along with their colleagues, Smith and Migliano analyzed the content and purpose of the stories the Agta people tell. They are a hunter-gatherer group, indigenous to the Philippines, believed to be the descendents of those who colonized the area 35,000 years ago. They’re distinguished from their non-Agta neighbors by their short physique and relatively dark skin, number around 1,250, and live in seclusion in a protected forest, inaccessible by car.Over several nights, a few elders told the researchers four stories, ones they normally tell children and each other. They all involved anthropomorphized animals and celestial objects, and related norms and principles about cooperation and socializing, particularly about sex equality, egalatarianism, friendship, and group identity. “In these stories,” the researchers wrote, “the ending reflects a reconciliation of individual interests and differences, while also exemplifying various mechanisms of social norm enforcement, such as emphasizing the benefits to cooperation over competition, examples of punishment for breaking norms, and reverse dominance hierarchies to prevent individual accumulation of power.”The Agta live in camps that house, on average, 49 people, and the researchers asked the Agta to name the best storytellers in each camp, and then ranked each camp in storytelling ability by the proportion of good storytellers present. After having 290 Agta people from the camps play an experimental resource-allocation game, which allows players to choose how much they’d like to keep or share, the researchers found that higher camp-storytelling skill was significantly associated with increased cooperation in the game. Their model implied that a one percent increase in camp nominations of good storytellers was associated with an increase in game donations by 2.2 percentage points. “This association,” the researchers wrote, “is consistent with skilled storytellers spreading cooperative norms and promoting cooperation in camps.”The researchers also found that skilled storytellers were desirable social partners. They asked 291 Agta from 18 camps to name people with whom they’d prefer to live, and found that—after controlling for kinship, reciprocal nominations, distance, as well as age and sex variables—skilled storytellers were nearly twice as likely to be picked compared to less skilled Agta. Storytelling wasn’t the only facet of people’s reputation that the researchers measured. They also assessed skill in hunting and fishing, as well as medicinal knowledge and camp influence. When the researchers asked the Agta to also consider these attributes in choosing a living partner, storytelling skill came out on top, “with skilled storytellers again having roughly double the odds of being nominated relative to non-skilled storytellers,” they wrote, “an effect much larger than that of possessing a good fishing reputation, the second-best reputational predictor.” Clearly the Agta find storytellers attractive—but does that lead storytellers to have more sex? It’s possible. The researchers found that skilled storytellers had 0.53 more living offspring compared to non-skilled storytellers, “indicating,” they wrote, “that storytelling skill is associated with increased fitness.”Not only that, skilled storytellers, due to the valued service they provide, may also, according to a 2003 paper, experience increased fitness by benefitting from more social support from others. A 2007 paper showed how social support can have this effect in primates. In line with this idea, Smith and Migliano showed that, among the Agta, skilled storytellers are more likely to receive resource swaps in their experimental game. The researchers suspect, as a result, that storytellers may be compensated, in a way, “for their public good by other camp mates who benefit from the increased cooperation which storytellers may promote, in what may be mutually beneficial trade-like relationships.” But of course, as the researchers note, people may be rewarding storytellers for simply spinning an entertaining yarn.Storytelling is no doubt one of the most highly coveted skills in modern society. Smith and Migliano suggest that the notable ability of some people to concoct and convey compelling tales evolved early on in our species’ history as a precursor “to more elaborate forms of narrative fiction, such as moralizing high-gods,” the emergence of which, according to Harvard evolutionary anthropologist Joseph Henrich, may have spurred societal complexity. “From simple storytelling to complex religion, and later formal institutions such as nation states, the evolution of storytelling,” Smith and Migliano conclude, “may have been pivotal in organizing and promoting human cooperation.”Perhaps it should come as no surprise, then, that neuroscientists are recognizing the importance of narrative in brain activity. “Stories are deep-rooted in the core of our nature,” Jonas Kaplan, a researcher at the University of Southern California’s Brain and Creativity Institute, has said. Kaplan co-authored a 2017 brain-imaging study that found there’s something universal about what occurs in the brain when it is processing stories, since researchers could predict, by brain activity, which of a range of stories subjects were reading, even when the same story was rendered in English, Mandarin, or Farsi. Neurologist Robert Burton highlighted, in Nautilus, the symmetry between storytelling and scientific understanding. “Once we see that stories are the narrative equivalent of correlation,” he wrote, “it is easy to understand why our brains seek out stories (patterns) whenever and wherever possible.”So, Tyrion was right in saying that nothing unites people like a good story. In his speech, he goes on to say that “nothing can stop” a good story, “no enemy can defeat it.” Which is true. The story wasn’t defeated, it just managed to unite a whole lot of people in oppositional longing for alternatives. Stories work in many ways.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\n*An earlier version of this post claimed that researchers determined that skilled Agta storytellers had more sex. Strictly speaking, they were only found to have had more living offspring than non-skilled storytellers. "
  },
  {
    "imageUrl": "http://static.nautil.us/16116_adc32ce9b954ad17e491a0bc426dbb71.jpg",
    "title": "A Close Look at Newborn Planets Reveals Hints of Infant Moons",
    "description": "Posted by Joshua Sokol on June 18, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Astronomers have spent decades, if not centuries, hoping to see embryonic planets. As of a year ago, the closest they…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Astronomers have spent decades, if not centuries, hoping to see embryonic planets. As of a year ago, the closest they had come was the discovery of gaps, thought to be caused by budding planets, in the spinning disks of gas and dust that surround young stars. But they weren’t sure how to interpret these indirect clues.What a difference a year makes. Increasingly detailed observations of a star called PDS 70, which is a little smaller than the sun and some 370 light-years from Earth, have revealed not just one newborn planet, but evidence of a second. These planets are so young that they appear to still be growing. What’s more, a recent paper argues that one of these planets is surrounded by its own swirling disk of gas and dust, the kind of structure thought to hatch large moons. If confirmed, this would be a spectacular validation of long-held ideas about how planets and their moons form.The first discovery came last June, when a team of astronomers published an image of a planet orbiting inside the disk around PDS 70, a young star roughly 5 million years old (our own sun is roughly 1,000 times older). A month later, another study showed that the planet was emitting a deep red light as hot hydrogen atoms fell down onto it, which suggested that the world is still beefing up.Then just last week, astronomers announced that PDS 70 hosts a second giant baby planet still sucking up ambient hydrogen. “What makes PDS 70 superlative is that it’s absolutely unambiguous that there’s at least one planet there, and the evidence for a second planet is very convincing,” said Kate Follette, an astronomer at Amherst College.Most of these discoveries come from the Very Large Telescope (VLT), which is actually a set of four enormous telescopes perched on a mountaintop in Chile’s Atacama Desert. To find the second planet and provide even further confirmation for the first one, a team led by Sebastiaan Haffert at Leiden University in the Netherlands used MUSE, a new instrument on the VLT, to search for emission from hydrogen. They got their data on just one clear night in the summer of 2018, when MUSE was still being tested. “Given how robustly they detected both planets, it’s really exciting for the future,” Follette said.So far, the two planets seem to match predictions by previously untested theories of planet formation, said Zhaohuan Zhu, an astrophysicist at the University of Nevada, Las Vegas. Back in 2012—eons ago in this fast-moving field—astronomers first saw an empty, dust-free band in the disk of PDS 70. It starts from a position equivalent to where Uranus orbits in our solar system and extends to about three times that distance.The first planet, dubbed PDS 70b, prowls right outside the inner edge of this gap in the disk. But one planet alone couldn’t explain why the gap was so wide. Two planets, however, would in theory open up a much wider gap. That’s exactly what the new data show. With a second planet found near the outer edge of the gap, circling the star once for every two complete orbits by the inner planet, “everything seems to start to connect,” Zhu said.Because it has bright planets far from their star, PDS 70 is perfectly tuned to give up its secrets. “PDS 70 is really becoming a benchmark system,” said Julien Girard, an astronomer at the Space Telescope Science Institute who collaborated on the discoveries of both the second planet and the possible moon-forming disk.The more tentative detection of the moon-hatching disk also came from the VLT, through another instrument called Sinfoni. Once the initial discovery of PDS 70b was made last year, Valentin Christiaens at Monash University in Australia looked for the same point of light in his own separate measurements of the system. “When we eventually saw the blob, I got very excited,” he said.These measurements showed that the light from PDS 70b included more red light than would be expected from just a planet. Christaens and his team argue that the data reveal a disk of dusty material around the planet that absorbs heat, then rereleases it in infrared wavelengths.In models of how solar systems form, smaller disks like these coalesce into satellites such as the four Jovian moons discovered by Galileo. If the initial observations are confirmed by other observatories, they’ll help to validate these ideas. “It’s nice confirmation that you could have a circumplanetary disk around giant planets, that Jupiter is not that special,” Zhu said.He also points out that in our own solar system, two of the most promising targets in the search for life, Europa and Enceladus, orbit giant planets. If infant gas giants often host disks that can make moons, Zhu said, “maybe Europa or Enceladus could be common in our galaxy.”Joshua Sokol is a freelance science journalist in Boston. His work has appeared in New Scientist, The Atlantic, The Wall Street Journal and elsewhere. He has a bachelor’s degree in astronomy and in English literature from Swarthmore College, and a master’s degree in science writing from the Massachusetts Institute of Technology. In between, he worked as a data analyst for the Hubble Space Telescope’s Advanced Camera for Surveys.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16035_d6368eb1ada0146fbabee40e79f3b4e4.jpg",
    "title": "What Ancient Romans Used Instead of Toilet Paper",
    "description": "Posted by Stephen E. Nash on May 17, 2019  We’ve all been caught unawares by our digestive tract at one time or another. It happened to the Nash family several months ago. We were nearing the end of an extended road trip, driving…",
    "category": "Culture",
    "content": "We’ve all been caught unawares by our digestive tract at one time or another.It happened to the Nash family several months ago. We were nearing the end of an extended road trip, driving down a secondary highway through a sparsely populated area of Colorado at night, when one of my 9-year-old twin sons had to use the bathroom. Despite my pleading, he said he couldn’t make it to the next town. (He had to poop.) So we pulled over and headed for the bushes. After he took care of his business, we realized that we didn’t have toilet paper with us.The whole dramatic episode got me thinking, and for the next couple of hours, I pondered toilet paper and the cultural nature of bathroom routines. (Cut me some slack. It was a long drive.)Toilet paper is now such a routine part of our lives that we rarely give it any thought. That boring reality, however, should make us think—because toilet paper is an artifact, a technology, and is therefore grounded in culture.As we finally re-entered Denver—my wife and kids blissfully asleep—I saw the Colorado state capitol building, beautifully lit on the horizon. I started thinking about the ancient Romans. With tall columns, colonnades, and a high, golden dome, the capitol is nothing if not a Roman temple to civics.Modern American society, and Western societies more generally, tend to look back on ancient Rome as the pinnacle of Western civilization. We emulate their institutions and cultural practices. Why? Are they worth it?Every week a crew took hefty pots of pee to a laundromat. Why? Because ancient Romans washed their togas and tunics in pee!When I thought more about their everyday habits, I realized that, despite all of their accomplishments, ancient Romans engaged in some practices that many people today would find thoroughly revolting. Take a minute to consider, for example, what many of those supposedly “civilized” people did when they had to go to the bathroom.When Mt. Vesuvius erupted on August 24 in A.D. 79, Pompeii, Herculaneum, and other Roman settlements were sealed as time capsules. They were first excavated in the 18th century, and since then these sites have given us a wonderful view into ancient Roman society.Many of the bathrooms uncovered at Pompeii and elsewhere were communal. In many cases, they were beautiful, with frescoes on the walls, sculptures in the corners, and rows of holes carved into cold, Italian marble slabs.Roman toilets didn’t flush. Some of them were tied into internal plumbing and sewer systems, which often consisted of just a small stream of water running continuously beneath the toilet seats.In the same way that we use an American-style toilet, a Roman user would sit down, take care of business, and watch number two float blissfully away down the sewer system. But instead of reaching for a roll of toilet paper, an ancient Roman would often grab a tersorium (or, in my technical terms, a “toilet brush for your butt”). A tersorium is an ingenious little device made by attaching a natural sponge (from the Mediterranean Sea, of course) to the end of a stick. Our ancient Roman would simply wipe him- or herself, rinse the tersorium in whatever was available (running water and/or a bucket of vinegar or salt water), and leave it for the next person to use. That’s right, it was a shared butt cleaner. (And of course, there were other means of wiping as well, such as the use of abrasive ceramic discs called pessoi.)Okay, so ancient Roman pooping habits seem strange, but what about their customs around pee?As best we can tell from historic and archaeological data, ancient Romans peed in small pots in their homes, offices, and shops. When those small pots became full, they dumped them into large jars out in the street. Just like with your garbage, a crew came by once a week to collect those hefty pots of pee and bring them to the laundromat. Why? Because ancient Romans washed their togas and tunics in pee!Human urine is full of ammonia and other chemicals that are great natural detergents. If you worked in a Roman laundromat, your job was to stomp on clothes all day long—barefoot and ankle deep in colossal vats of human pee.(Frankly, I wonder why we haven’t emulated this aspect of Roman culture in our age of green, eco-friendly, and sustainable businesses. I’m thinking of opening a chain called Urine-Urout All-Natural Laundromat. It’s a sparkling business opportunity!)As peculiar as personal hygiene practices in ancient Rome may seem to us, the historical fact is that many Romans successfully and sustainably used tersoria and washed their clothes in pee for several centuries—far longer than we’ve used toilet paper. Indeed, toilet paper is not a universal technology even today, as any trip to India, rural Ethiopia, or remote areas of China will make abundantly clear.The memorable stop we made for my son in rural Colorado will always remind me of our culture’s widespread dependence on toilet paper. We’ve become so accustomed to the stuff that we are loath to consider widely used alternatives. (Heck, even the elegant bidet gets short shrift in our society.)As an archaeologist, this is surprising to me, especially because toilet paper was formally introduced in this country only in 1857, a comparatively short time ago. At that time, New York entrepreneur Joseph Gayetty first created commercial toilet paper; each individual paper sheet bore his name. He claimed that, in addition to their novel utilitarian function, they were medicinal and prevented hemorrhoids.In 1890, Clarence and E. Irvin Scott developed the first toilet paper on rolls; their brand thrives today. (It happens to be my favorite. Too much information?) Like Gayetty’s sheets, Scott tissue was originally marketed as a medicinal product. In the late 1920s, Hoberg Paper Company marketed Charmin brand toilet paper to women, with an emphasis on softness (thank goodness) and femininity, rather than medicinal properties that didn’t actually work.Today, toilet paper is ubiquitous in Western cultures; it’s a US$9.5 billion-a-year industry in the United States. Americans, in their typical excess, use more than 50 pounds per person per year! About 1.75 tons of raw fiber are required to manufacture each ton of toilet paper. That doesn’t seem sustainable, and frankly, I’m surprised that people haven’t protested more as a result.Given these numbers and the marketing efforts behind them, it’s hard to argue that the use of toilet paper is somehow natural. On the contrary, toilet paper is nothing more than a technology. So the next time you’re enjoying a morning constitutional, think about the fact that defecation and urination are more than biological functions; they are cultural activities that involve artifacts and technologies that change through time.Speaking of which, it’s high time that we consider changing how we clean ourselves after we use the toilet. Tersorium, anyone?Stephen E. Nash is a historian of science and an archaeologist at the Denver Museum of Nature & Science. He has published numerous books, most recently Stories in Stone: The Enchanted Gem-Carving Sculptures of Vasily Konovalenkoand and An Anthropologist’s Arrival: A Memoir.\n\tThe newest and most popular articles delivered right to your inbox!\nCorrection:An earlier version of this piece used the term “tersoriums” for the plural form of “tersorium.” After scrubbing for more information, we confirmed that “tersoria” is the correct plural form of the term. The text has also been updated to clarify that the butt-brush “cleaning” ritual varied and to note that tersoria were certainly not the only means ancient Romans used to clean themselves after defecation. We don’t mean to be abrasive, but it’s impossible to convey all the minutiae of personal hygiene practiced by the ancient Romans. (And would you really want to know more?)This work first appeared on SAPIENS under a CC BY-ND 4.0 license. Read the original here. "
  },
  {
    "imageUrl": "http://static.nautil.us/16050_cf247d6dafc3aadcdf0b2cd608ac8f72.jpg",
    "title": "What an Extinct Bird Re-Evolving Says About “Species”",
    "description": "Posted by Margaret E. Farrell on May 24, 2019  You may have heard the news of what sounds like a resurrection story on the small island of Aldabra, off the coast of Madagascar. Around 136,000 years ago, the island was submerged in water…",
    "category": "Biology",
    "content": "You may have heard the news of what sounds like a resurrection story on the small island of Aldabra, off the coast of Madagascar. Around 136,000 years ago, the island was submerged in water and a layer of limestone captured the rails—a species of flightless bird—living there. The birds (and all other land species living on the island) went extinct. Recently, though, scientists reported that the bones of these fossilized rails are virtually indistinguishable from rails living on the island today. They are calling this an instance of iterative evolution—where the same species evolves multiple, distinct times.How could the same species evolve more than once? Both the fossil and modern rail are descendants of flying rails that made their way to Aldabra from the nearby mainland. In both cases, the rails adapted to the island over time. And each time, the species gradually lost its ability to fly, possibly due to a lack of predators on the island (which removes a significant selection pressure for flight). This raises an interesting question: Can birds on different branches of the evolutionary tree really be part of the same species? The answer depends on what you mean by “species.” This is the species problem, one of philosophy of biology’s persistent demons. As it turns out, it is difficult (and some would argue, impossible) to conceptualize species in a way that fully, and without exception, captures what it is that makes a group of organisms one species and not another.This is a case of vertical convergence—possibly one of Stephen Jay Gould’s wildest dreams.Most biologists prefer what is called the biological species concept, meaning organisms of the same species must be able to interbreed to produce viable offspring. This concept does not apply to many plants and most microorganisms, though, because they reproduce asexually. And even for sexually reproducing animals it is often difficult in practice to determine whether two groups of organisms could potentially interbreed. The Aldabra rails give a case in point since the fossil rails are, well, fossils.The authors of the original study seem to use a different species concept to argue that the rails are a case of iterative evolution. I should point out that the authors actually call the flightless rail D. c. subsp. aldabranus—a subspecies of Dryolimnas cuvieri. But this just pushes the problem a step back—what makes them the same subspecies? We do get an answer from the authors: Their comparison between the fossil and modern rail is based on the shape and size of fossilized humerus (in the wing) and tarsometatarsus (in the leg) bones. This points to what is called the morphological species concept, which demarcates species based on morphological similarity—similarity between bodily structures.But this conflicts with what the phylogenetic species concept—probably the second most-used of the species concepts—would tell us. In this conception, a species is the smallest group of organisms that share an ancestor and can be grouped together based on some core characteristics, like the end of a branch on a tree of life. On the phylogenetic species concept, the fossil rails and modern rails are different subspecies, since by dint of having split off from the mainland, rail species at different times, they have independent evolutionary origins. The modern rail is part of a different lineage than the fossil rail.Part of the motivation for calling these the same subspecies based on morphology is that the bones of the fossil and modern rail are more similar in size than either is to their close ancestors—as the authors note, “Like D. c. aldabranus, it also differs considerably in size from D. c. cuvieri and D. c. abbotti.” It would probably make most evolutionary biologists that favor the phylogenetic species concept happy to say instead that these are two extremely similar subspecies. But the fact that these rails are so similar that the question of species distinction even arises is a reflection of something even more fascinating.This reappearance is a near-perfect example of something evolutionary biologist Stephen Jay Gould imagined—a thought experiment he called “replaying life’s tape.” He wondered about the repeatability of evolution—if we rewound the universe to the origins of life, would evolution produce the same species we see today? Maybe not identical, but closely similar in form and function? Gould and other philosophers of biology pursued this question and, for the most part, acknowledge the very fine-grained contingency of evolutionary processes. They are largely skeptical of the proposal that life, if rewound, would result in the same outcome.But this question has surfaced again as ecologists and evolutionary biologists study an evolutionary phenomenon called convergence. The fins of whales and dolphins have evolved to be structurally similar and work in much the same way, despite having independent evolutionary origins. Another example is the camera eye possessed by both humans and octopuses. These examples, like most of the studies of convergence, can be called horizontal—they involve organisms with distant evolutionary relationships who have ended up in adaptively similar environments and have, over time, evolved similar traits. But none of these cases come as close to replaying life’s tape as does that of these flightless birds. This is a case of vertical convergence—possibly one of Stephen Jay Gould’s wildest dreams.As the authors write, “Conditions were such on Aldabra, the most important being the absence of terrestrial predators and competing mammals, that a Dryolimnas rail was able to evolve flightlessness independently on each occasion.” The tape resets when the island reemerges with much the same environmental conditions as before and the mainland rail species recolonizes. It replays to reach the same (or, if you prefer, very similar) subspecies.Margaret E. Farrell is a PhD student in the Department of Logic and Philosophy of Science at the University of California, Irvine. Her research is focused on the history and philosophy of biology.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16103_8f5c01673292dca6425ee6f028fc2ffa.png",
    "title": " The Math Trick Behind MP3s, JPEGs, and Homer Simpson’s Face",
    "description": "Posted by Aatish Bhatia on June 10, 2019  Over a decade ago, I was sitting in a college math physics course and my professor spelt out an idea that kind of blew my mind. I think it isn’t a stretch to say that this is one of the…",
    "category": "Numbers",
    "content": "Over a decade ago, I was sitting in a college math physics course and my professor spelt out an idea that kind of blew my mind. I think it isn’t a stretch to say that this is one of the most widely applicable mathematical discoveries, with applications ranging from optics to quantum physics, radio astronomy, MP3 and JPEG compression, X-ray crystallography, voice recognition, and PET or MRI scans. This mathematical tool—named the Fourier transform, after 18th-century French physicist and mathematician Joseph Fourier—was even used by James Watson and Francis Crick to decode the double helix structure of DNA from the X-ray patterns produced by Rosalind Franklin. (Crick was an expert in Fourier transforms, and joked about writing a paper called, “Fourier Transforms for birdwatchers,” to explain the math to Watson, an avid birder.)You probably use a descendant of Fourier’s idea every day, whether you’re playing an MP3, viewing an image on the web, asking Siri a question, or tuning in to a radio station. (Fourier, by the way, was no slacker. In addition to his work in theoretical physics and math, he was also the first to discover the greenhouse effect.)So what was Fourier’s discovery, and why is it useful? Imagine playing a note on a piano. When you press the piano key, a hammer strikes a string that vibrates to and fro at a certain fixed rate (440 times a second for the A note). As the string vibrates, the air molecules around it bounce to and fro, creating a wave of jiggling air molecules that we call sound. If you could watch the air carry out this periodic dance, you’d discover a smooth, undulating, endlessly repeating curve that’s called a sinusoid, or a sine wave. (Clarification: In the example of the piano key, there will really be more than one sine wave produced. The richness of a real piano note comes from the many softer overtones that are produced in addition to the primary sine wave. A piano note can be approximated as a sine wave, but a tuning fork is a more apt example of a sound that is well-approximated by a single sinusoid.)Now, instead of single key, say you play three keys together to make a chord. The resulting sound wave isn’t as pretty—it looks like a complicated mess. But hidden in that messy sound wave is a simple pattern. After all, the chord was just three keys struck together, and so the messy sound wave that results is really just the sum of three notes (or sine waves).Fourier’s insight was that this isn’t just a special property of musical chords, but applies more generally to any kind of repeating wave, be it square, round, squiggly, triangular, whatever. The Fourier transform is like a mathematical prism—you feed in a wave and it spits out the ingredients of that wave—the notes (or sine waves) that when added together will reconstruct the wave.If this sounds a little abstract, here are a few different ways of visualizing Fourier’s trick. The first one comes to us from Lucas V. Barbosa, a Brazilian physics student who volunteers his time to make incredible math and science animations for Wikipedia, where he goes by “LucasVB.”So let’s take a squarish looking wave, pass it through Fourier’s prism, and see what comes out the other side.In these images (click through to Wikipedia to see it as an animation), the red squarish wave is distilled into a set of pure notes (the blue sine waves). Think of these blue waves like a mathematical ingredient list for the red wave. Pressing this analogy, the Fourier transform is a recipe—it tells you exactly how much of each note you need to mix together to reconstruct the original wave. The vertical blue lines in the animation are essentially a graph visually representing the amount of each note.Here’s another way to think about this, provided by Matthew Henderson, or “Matthen,” a Ph.D. student at Cambridge University who also creates animated GIFs of mathematical curiosities. Matthen explains Fourier’s trick using circles instead of sine waves. This involves a set of circles of different sizes, each one centered on the edge of a bigger circle. Then the circles begin to spin, the big circles swinging the smaller ones around, and the smaller ones spinning faster than big ones. If you trace the motion of one point on the smallest circle, you can reconstruct a wave of any shape, as shown in the animation and the stills below. Again, the Fourier transform tells you how to build the wave: which circles, moving at which speeds. If you’re old enough to have played with a Spirograph, this idea of tracing complex patterns using wheels within wheels might be familiar to you. Here’s an interactive version of the same animation, created by LucasVB, where you can mess around and change the sizes of the circles.To summarize, the Fourier transform tells you how much of each ingredient “note” (sine wave or circle) contributes to the overall wave. Here’s why Fourier’s trick is useful. Imagine you were talking to your friend over the phone and you wanted to get them to draw this squarish wave. The tedious way to do this would be to read out a long list of numbers that represent the height of the wave at every instant in time. With all these numbers, your friend could patiently stitch together the original wave. This is essentially how old audio formats like WAV files worked. But if your friend knew Fourier’s trick, you could do something pretty slick: You could just tell them a handful of numbers—the sizes of the different circles in the picture above. They can then use this circle picture to reconstruct the original wave.And this isn’t just some obscure mathematical trick. The Fourier transform shows up nearly everywhere that waves do. The ubiquitous MP3 format uses a variant of Fourier’s trick to achieve its tremendous compression over the WAV (pronounced “wave”) files that preceded it. An MP3 splits a song into short segments. For each audio segment, Fourier’s trick reduces the audio wave down to its ingredient notes, which are then stored in place of the original wave. The Fourier transform also tells you how much of each note contributes to the song, so you know which ones are essential. The really high notes aren’t so important (our ears can barely hear them), so MP3s throw them out, resulting in added data compression. Audiophiles don’t like MP3s for this reason—it’s not a lossless audio format, and they claim they can hear the difference.This is also how the smartphone app Shazam can recognize a song. It splits the music into chunks, then uses Fourier’s trick to figure out the ingredient notes that make up each chunk. It then searches a database to see if this “fingerprint” of notes matches that of a song they have on file. Speech recognition uses the same Fourier-fingerprinting idea to compare the notes in your speech to that of a known list of words.You can even use Fourier’s trick for images. Here’s a great video that shows how you can use circles to draw Homer Simpson’s face. The online encyclopedia Wolfram Alpha uses a similar idea to draw famous people’s faces. This might seem like a trick you’d reserve for a very nerdy cocktail party, but it’s also used to compress images into JPEG files. In the old days of Microsoft Paint, images were saved in bitmap (BMP) files which were a long list of numbers encoding the color of every single pixel. JPEG is the MP3 of images. To build a JPEG, you first chunk your image into tiny squares of 8 by 8 pixels. For each chunk, you use the same circle idea that reconstructs Homer Simpson’s face to reconstruct this portion of the image. Just as MP3s throw out the really high notes, JPEGs throw out the really tiny circles. The result is a huge reduction in file size with only a small reduction in quality, an insight that led to the visual online world that we all love (and that eventually gave us cat GIFs).How is Fourier’s trick used in science? I put out a call on Twitter for scientists to describe how they used Fourier’s idea in their work. The response astounded me. The scientists who responded were using the Fourier transform to study the vibrations of submersible structures interacting with fluids, to try to predict upcoming earthquakes, to identify the ingredients of very distant galaxies, to search for new physics in the heat remnants of the Big Bang, to uncover the structure of proteins from X-ray diffraction patterns, to analyze digital signals for NASA, to study the acoustics of musical instruments, to refine models of the water cycle, to search for pulsars (spinning neutron stars), and to understand the structure of molecules using nuclear magnetic resonance. The Fourier transform has even been used to identify a counterfeit Jackson Pollock painting by deciphering the chemicals in the paint.Whew! That’s quite the legacy for one little math trick.Aatish Bhatia is a science writer and physicist working at Princeton University on the Council on Science and Technology, where he is an Associate Director and Lecturer. He writes the award-winning science blog Empirical Zeal and is on Twitter as @aatishb.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in November 2013. "
  },
  {
    "imageUrl": "http://static.nautil.us/16053_09504c80bbd8bb4b75f97df27a9feaa3.jpg",
    "title": " This Famous Aging Researcher Doesn’t Want Us to Live Forever",
    "description": "Posted by Brian  Gallagher on May 29, 2019  In the Netflix anime series Knights of Sidonia, humankind is marooned in a spaceship 500,000-strong, refugees constantly on the run from shapeshifting aliens who destroyed Earth over 1,000…",
    "category": "Biology",
    "content": "In the Netflix anime series Knights of Sidonia, humankind is marooned in a spaceship 500,000-strong, refugees constantly on the run from shapeshifting aliens who destroyed Earth over 1,000 years ago. Both the patriarchy and poverty have been smashed. Advances in genetic engineering have allowed androgynous individuals to proliferate and asexual reproduction to become commonplace. Everybody (except the protagonist, a clone of his grandfather) can photosynthesize, drastically reducing the need to eat.A plot twist near the end of the first season, released in 2014, revealed the existence of a shadow government called the Immortal Ship Committee which, in the last several centuries, had grown in number, from less than 10 members to just under 30, becoming more and more corrupt and self-serving. One of them, a 600-year-old woman named Kobayashi, captains the ship behind a mask, concealing her unaging, youthful complexion. She helps orchestrate fake news of alien encounters to justify the emergency powers of the committee’s undemocratic puppet government front.The potential for undying tyrants or tyrannical bodies is one reason Leonard Hayflick, one of the world’s preeminent experts on aging (he was a founder of the Council of the National Institute on Aging), is against slowing down or eliminating the aging process. He has other reasons, too, like avoiding the father-daughter situation in Interstellar, where, due to the time-dilation effects of traveling in different gravitational fields, the daughter caught up to her father in years, and eventually died of old age first. “To slow, or even arrest, the aging process in humans is fraught with serious problems in the relationships of humans to each other and to all of our institutions,” he told Jordana Cepelewicz, a former editorial intern at Nautilus. “By allowing antisocial people—tyrants, dictators, mass murderers, and people who cause wars—to have their longevity increased should be undesirable…I would rather experience the aging process as it occurs, and death when it occurs, in order to avoid allowing the people who I just described to live longer.”Despite his reservations about radical life-extension, Hayflick is a big proponent of studying aging at a more fundamental level, he said. “Most studies are either descriptive, studies on longevity determinants, or studies on age-associated diseases. None of this research will reveal information about the fundamental biology of aging. Less than 3 percent of the budget of the National Institute on Aging in the past decade or more has been spent on research on the fundamental biology of aging.” He’s a bit annoyed, for instance, that about a half of the National Institute on Aging’s budget goes toward researching Alzheimer’s disease. “The resolution of Alzheimer’s disease as a cause of death will add about 19 days onto human life expectancy,” he said. “I have suggested that the name of the institute be changed to the National Institute on Alzheimer’s Disease. Not that I support ending research on Alzheimer’s disease, I do not, but the study of Alzheimer’s Disease and even its resolution will tell us nothing about the fundamental biology of aging.”Hayflick also has some advice on what we should teach scientists and the public about aging. “That education must include an understanding that the massive amount of research funds spent on studying the leading causes of death will not advance our understanding of the basic biology of aging,” he said. “It also must include an understanding that the study of longevity determinants (anabolic processes) will not reveal information about the basic biology of aging (catabolic processes). Finally, we need to educate scientists and the public, to support research on the differences between young cells and old cells that make the latter more vulnerable to age-associated diseases.”You can watch the whole of our interview with Hayflick here.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in April 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16042_fdc3507c5193fe2896b618f1d154360b.jpg",
    "title": "The Problem with Using the Term “Fake News” in Medicine",
    "description": "Posted by Clayton Dalton on May 20, 2019  Here’s one way to rid society of “fake news”—abandon the term altogether.That’s what a U.K. committee recommended that Parliament do last fall. It argued that the concept has…",
    "category": "Culture",
    "content": "Here’s one way to rid society of “fake news”—abandon the term altogether.That’s what a U.K. committee recommended that Parliament do last fall. It argued that the concept has lost any clear meaning, since it has been used to describe everything from genuine error to frank duplicity, or even just to slander. It’s an important point to make, in an era when our sprawling connectivity abets the spread of ever more misinformation. As a group of scientists at the Massachusetts Institute of Technology demonstrated in a landmark study published last year in Science, falsehoods shared on social media tend to spread much further, and much faster, than truths do. And, while misinformation can sway elections and threaten public institutions, medical falsehoods can threaten people’s health, or even their lives.A few months ago, several physicians made this case in the Journal of the American Medical Association, arguing that doctors and scientists should play a more proactive role in combating the dissemination of scientific misinformation in online spaces. “What is needed is a campaign, engaging the platforms that patients use,” they wrote. “In some cases, fake news could be seen as a teachable moment, and an opportunity for researchers to clarify scientific findings.” I’m all in. Scientists and physicians, like me, do hold a unique position from which to help improve scientific literacy and comprehension. But if we are going to combat this phenomenon, we need to know what, exactly, we’re speaking about. Is “fake news” deliberate disinformation, intended to exploit? Is it simply inaccuracy? Or is it a pejorative meant to discredit? As the Parliamentarians noted, the term has been used in all of these ways.Given the lack of clarity in how the term has been deployed, we should be thoughtful and precise in addressing these different manifestations of “fake news,” and in our own use of the term. Unfortunately, I’ve seen just the opposite happen. Physicians, scientists, journalists, and others are dropping the “fake news” label like a trending hashtag, without giving serious effort to navigating its nuances. Instead of addressing confusion, they’ve added to it.Most common seems to be the application of the “fake news” moniker to articles that are simply inaccurate or poorly reported.In a Daily Beast article titled “The Fake News Epidemic in Health,” physician Nina Shapiro writes about the problem of lax reporting on health and medicine: “Claims that routinely circulate are frequently overblown (‘diet cures cancer’), misleading (‘coffee enemas detox the body’), based on substandard research (‘fish oil supplements are good for you’), or are completely false (‘vaccines cause autism’).” Shapiro is right to speak out against specious reporting, but the title is problematic. Although she doesn’t use the term “fake news” directly, framing her discussion in this way implies a fraudulent intent, when in truth these reporters probably just didn’t do their homework.Physicians, scientists, journalists, and others are dropping the “fake news” label like a trending hashtag…Academic journals are misusing the term, too. Going back to 2017, we find an article in the medical journal Canadian Family Physician titled simply, “Fake Medical News.” The piece was a critique of a study suggesting that patients fared better when treated by female physicians. Standard fare for an academic journal. Not really “news,” and certainly not fake. Another, this time in an emergency medicine journal, was titled “Medically Clear: Fake News Thrives in Medicine.” The author describes receiving a solicitation to contribute to a medical journal that turned out to be fraudulent. A sort of fake, yes, but not fake news.The problem with such cavalier use of the “fake news” label is twofold. First, it implies duplicitous writing or reporting, with an intent to deceive, when in reality the examples above suffer mostly from imprecise language and a lack of rigor. And second, indiscriminate use of the term erodes its power in calling out things that are, actually, fraudulent.This winter, STAT published an article called “Solving the Fake News Problem in Science.” The piece was actually about the lack of replication and confirmation of published scientific results, but it makes a pretty sweeping claim—“...science, like news and social media, has a ‘fake news’ problem.” The author is right that there’s a problem with “fake news” in science, but it doesn’t have anything to do with confirmation or replication.The real problem is health and medical news that is truly fake, as the original Webster’s had it—a trick, a swindle, a fraudulent manipulation. The internet is full of scammers generating content that masquerades as news, and distributing it via social media. The motive is usually money, whether by click-driven ad revenue or by sale of weird tonics. Here’s a stunning example reported by Proto Magazine: A fabricated story suggesting that flu shots were killing people attracted 500,000 clicks in January 2018, more than any other story from NPR, ABC, CBS, CNN, or Fox News.To me, this variety of disinformation is by far the most dangerous, and any use of the term “fake news” as it relates to health or medicine should be relegated to this sphere, if we are to use it at all. It is precisely this sort of fraudulent disinformation that physicians and scientists should speak out against.There’s one more iteration. It’s the use of the “fake news” label as a smoke screen, as a pejorative meant to discredit. As a weapon. The term entered the public lexicon largely in this way, courtesy of Donald Trump. During his first two years in office, not a single month passed without a Trump tweet about “fake news.” During some periods, such tweets appeared daily. The apparent intention has been to smear unflattering coverage of his administration. This is important, because such a dominant cultural association can be hard to dissociate from the term’s use elsewhere.We should be especially careful, then, in how we use the phrase, as the authors of the JAMA piece noted: “Opponents of the content of a report or a message need only decry it as ‘fake news’ to invoke a conspiracy against that content,” they wrote. “This single phrase almost seems to initiate an anamnestic response among those disinclined to accept or believe the content, automating cascades of disbelief and dismissal.”So I was surprised when a report in the New York Times seemed to use the term in just this way. The reporter, Gina Kolata, wrote about a controversial treatment for stroke. Many physicians have misgivings about the drug, called tissue plasminogen activator, which can cause catastrophic hemorrhage. The evidence supporting its use is not definitive, and a spirited debate regarding its safety is ongoing. But rather than addressing the nuance of this debate, the reporter declared that physicians who question the drug’s safety are peddling the “medical version of fake news.”I was stunned to see scientific discourse passed off as “fake.” We’re not talking about hucksters selling colloidal silver. At best, the reporter used the term for buzz. At worst, it’s a maneuver intended to discredit those she disagrees with. The scientific community should be proactive in engaging with sources of misinformation and disinformation in online spaces. But when we blithely throw around a term like “fake news” without considering its implications, we do a profound disservice to the goal of comprehension and clarity. We ourselves become part of the problem.Clayton Dalton is an emergency medicine resident at Massachusetts General Hospital in Boston. He has published stories and essays with NPR, Aeon, and The Los Angeles Review. Read his last feature in Nautilus, “Iron Is the New Cholesterol.” \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16020_b301bbff368b7a75043a2b9925a530ff.jpg",
    "title": "The English Word That Hasn’t Changed in Sound or Meaning in 8,000 Years",
    "description": "Posted by Sevindj Nurkiyazova on May 13, 2019  One of my favorite words is lox,” says Gregory Guy, a professor of linguistics at New York University. There is hardly a more quintessential New York food than a lox bagel—a century-old…",
    "category": "Culture",
    "content": "One of my favorite words is lox,” says Gregory Guy, a professor of linguistics at New York University. There is hardly a more quintessential New York food than a lox bagel—a century-old popular appetizing store, Russ & Daughters, calls it “The Classic.” But Guy, who has lived in the city for the past 17 years, is passionate about lox for a different reason. “The pronunciation in the Proto-Indo-European was probably ‘lox,’ and that’s exactly how it is pronounced in modern English,” he says. “Then, it meant salmon, and now it specifically means ‘smoked salmon.’ It’s really cool that that word hasn’t changed its pronunciation at all in 8,000 years and still refers to a particular fish.”How scholars have traced the word’s pronunciation over thousands of years is also really cool. The story goes back to Thomas Young, also known as “The Last Person Who Knew Everything.” The 18th-century British polymath came up with the wave theory of light, first described astigmatism, and played a key role in deciphering the Rosetta Stone. Like some people before him, Young noticed eerie similarities between Indic and European languages. He went further, analyzing 400 languages spread across continents and millennia and proved that the overlap between some of them was too extensive to be an accident. A single coincidence meant nothing, but each additional one increased the chance of an underlying connection. In 1813, Young declared that all those languages belong to one family. He named it “Indo-European.”Today, roughly half the world’s population speaks an Indo-European language. That family includes 440 languages spoken across the globe, including English. The word yoga, for example, which comes from Sanskrit, the language of ancient India, is a distant relative of the English word yoke. The nature of this relationship puzzled historical linguists for two centuries.In modern English, well over half of all words are borrowed from other languages. To trace how language changes over time, linguists developed an ingenious toolkit. “Some parts of vocabulary are more stable and don’t change as much. The linguistic term [for these words] is ‘a core vocabulary.’ These are numbers, colors, family relations like ‘mother,’ ‘father,’ ‘sister,’ ‘brother,’ and basic verbs like ‘walk’ and ‘see,’ says Guy. “If you look at words of that sort in different languages, it becomes fairly clear which ones are related and which ones are not. For example, take the English word for number two, which is dva in Russian and deux in French, or the word night, which is nacht in German and noch in Russian.”“The sounds that change across time are unpredictable, and differ from language to language, and some may not happen to change at all.”Analyzing the patterns of change that words undergo, moving from one language to another, showed how to unwind these changes and identify the possible originals. “Reconstructed vocabulary of Indo-European is based on a comparison of descendant languages,” explains Guy. “You collect words that mean more or less the same thing in all the languages, and if they look like each other in terms of their pronunciation, then it’s a good candidate for a descendant from a common ancestor.” The English word honey is madhu in Sanskrit and myod in Russian. Sanskrit and Russian haven’t shared a common ancestor since Indo-European, so these words had to come from the same source. (There are also the words mead in English, met in German and mjød in Danish that refer to an alcoholic drink made from honey.)After discovering a word that might have existed in the Indo-European, linguists compared how its pronunciations changed from language to language. For example, sound [k] changes to [h] from Latin to Germanic, and the Latin word casa transforms into the English house while the French word cœur transforms into the English heart.* With hints like that, linguists could undo the sound changes and trace the original pronunciation. In several thousand years, most words change beyond recognition, like the word wheel, which initially might have sounded “kʷékʷlos.” But there were some remarkable exceptions—like the timeless lox.The family tree of the Indo-European languages sprawls across Eurasia, including such different species as English and Tocharian B, an extinct language once spoken on the territory of Xinjiang in modern China. In Tocharian B, the word for “fish/salmon” is laks, similar to German lachs, and Icelandic lax—the only ancestor all these languages share is the Proto-Indo-European. In Russian, Czech, Croatian, Macedonian, and Latvian, the [k] sound changed to [s,] resulting in the word losos. This kind of millennia-long semantic consistency also appears in other words. For example, the Indo-European porkos, similar to modern English pork, meant a young pig. “What is interesting about the word lox is that it simply happened to consist of sounds that didn’t undergo changes in English and several other daughter languages descended from Proto-Indo-European,” says Guy. “The sounds that change across time are unpredictable, and differ from language to language, and some may not happen to change at all.”The word lox was one of the clues that eventually led linguists to discover who the Proto-Indo-Europeans were, and where they lived. The fact that those distantly related Indo-European languages had almost the same pronunciation of a single word meant that the word—and the concept behind it—had most likely existed in the Proto-Indo-European language. “If they had a word for it, they must have lived in a place where there was salmon,” explains Guy. “Salmon is a fish that lives in the ocean, reproduces in fresh water and swims up to rivers to lay eggs and mate. There are only a few places on the planet where that happens.”In reconstructed Indo-European, there were words for bear, honey, oak tree, and snow, and, which is also important, no words for palm tree, elephant, lion, or zebra. Based on evidence like that, linguists reconstructed what their homeland was. The only possible geographic location turned out to be in a narrow band between Eastern Europe and the Black Sea where animals, trees, and insects matched the ancient Indo-European words.In the 1950s, archaeological discoveries backed up this theory with remnants of an ancient culture that existed in that region from 6,000 to 8,000 years ago. Those people used to build kurgans, burial mountains, that archaeologists excavated to study cultural remains. In that process, scholars not only learned more about the Proto-Indo-Europeans but also why they were able to migrate across Europe and Asia.In turned out that, in the past, the grassy plains of steppe that run from Western China to the Black Sea had large herds of wild horses. Early humans hunted them for food, but the Proto-Indo-Europeans were probably the first people who domesticated the ancestors of modern-day domestic horses. That brought them an enormous advantage, allowing them to move a lot faster than any other human group. Then, they adopted—or, less likely, invented—wheeled vehicles and attached these to horses. “That’s probably the moment when they suddenly managed to expand into the Middle East, into India, and across Europe,” says Guy. “Within the next thousands of years, they expanded like no other human group that we know about in history. Because [now] they had mobility, which nobody else had.”In his book The Power of Babel, Columbia University linguist John McWhorter wrote, “Everything about a language is eternally and inherently changeable, not just the slang and the occasional cultural destination, but the very sound and meaning of basic words, and the word order and grammar.” It’s nice to know, though, that some words never change—lox being one of the most surprising.Sevindj Nurkiyazova is a journalist from Kyrgyzstan. Follow her on Medium @calempir.\n\tThe newest and most popular articles delivered right to your inbox!\n*This sentence was changed so as not to imply that Germanic languages descend from Latin ones. "
  },
  {
    "imageUrl": "http://static.nautil.us/16074_1d18c59d33149e93443c87bf4d1ab535.jpg",
    "title": "Why Working-Class New Yorkers Drop Their “Rs”",
    "description": "Posted by Sevindj Nurkiyazova on June 04, 2019  In George Bernard Shaw’s play Pygmalion, professor Henry Higgins says: “You can spot an Irishman or a Yorkshireman by his brogue. I can place any man within six miles. I can place him…",
    "category": "Culture",
    "content": "In George Bernard Shaw’s play Pygmalion, professor Henry Higgins says: “You can spot an Irishman or a Yorkshireman by his brogue. I can place any man within six miles. I can place him within two miles in London. Sometimes within two streets.” British English is famous for its variety of accents, and Higgins had real prototypes, linguistic Sherlock Holmeses who could discern a speaker’s origin from a phrase or two. They knew that tiny idiosyncrasies of speech could be fraught with personal information. The first person to reveal this scientifically was an American linguist, William Labov.When Labov moved to New York City in the early 1960s, he noticed that some working-class New Yorkers sounded different from middle and upper-class dwellers. One feature stood out to him: the pronunciation of the r sound, which working-class people tended to drop. Labov suggested that the difference in pronunciation can be linked to a speaker’s class, as our social differences manifest in speech. To show this, he chose three department stores—a high-end Saks Fifth Avenue, a middle-class Macy’s, and a now-defunct discount store S. Klein’s—and spent several hours pestering employees with questions.“He’d go up to employees working in the store and say something like: ‘Excuse me, can you tell me where to find men’s ties?’ ” explains Cecelia Cutler, professor of linguistics at the City University of New York. “The employees would say ‘fourth floor.’ Or they might say ‘fauth flaw’—with no r.” Labov would rush around the corner, pull out a notebook, and scribble, marking whether the person used or dropped the sound. He always chose questions that lead to a single answer, “fourth floor,” and in two days, surveyed 264 people. “He found that the working-class store had the greatest rates of r dropping,” says Cutler. People in the discount store dropped the r sound eight times more often than in the luxury store.The Social Stratification of English in New York City, Labov’s 1966 book, was groundbreaking. Labov took something as small as one letter and showed how a subtle detail of our language could tell who we are. Today, Labov is known as a father of sociolinguistics, the study of how language and society influence each other. In time, sociolinguists discovered that a particular pronunciation could reveal not only a person’s class and origin but education, ethnicity, age, and even sexual orientation. A whole new field of science came out of an observation about the peculiarity of the r sound in New York. But why that sound?“It could be any sound. It’s not the sound in particular that necessarily matters,” says Cutler. In English, there is no “perfect” articulation for the r sound. Different dialects show at least seven alternative pronunciations. It is also true for all other sounds—there is no standard for each phoneme in any language. Usually, we grasp all the sounds of our native language by the age of eight, and when we use a particular tongue shape, it creates distinctive acoustic features that can indicate our background. “If you look very carefully at almost any language, you’ll find linguistic features that become socially meaningful to people,” says Cutler. “This is the way we distinguish one another. In terms of r itself, there’s no particular reason—except that there’s a lot of variation in r.”Languages and their dialects are in constant flux. If Shakespeare time-traveled to 2019, the modern North American accent could sound more British to him than the contemporary British at least in terms of the r sound: In the 17th century, England had yet to lose it. The empire hadn’t gone r-less until the late 18th century, and some of its earlier colonies, like Canada and the US, speak another variety of English. The British accent, including its newfangled r-lessness, was considered prestigious in the States. It influenced eastern and southern American port cities, inspiring peculiar Boston and New York accents. After the Civil War, the prestige started shifting toward so-called rhotic accents of non-coastal parts of the country. The shift accelerated after the Second World War—Labov’s survey captured it right in its peak, when working-class New Yorkers haven’t yet grasped the change that middle and upper-class people already had.By estimates, R is the fourth most frequent consonant in the English language. What makes the r sound stand out is that it is salient to our ear. An urban legend claims that the French got their guttural R because the King, Louis the XIV, couldn’t pronounce a strong R. (Today, to nail that pronunciation, foreigners exercise gurgling water in the back of the throat.) In Latin, R was nicknamed canina litera—“dog’s letter”—for its resemblance to a snarl, and in Spanish, Italian, and Russian, the sound is still trilled. In some East Asian languages, like Japanese and Korean, the r sound is so close to l that there is a single symbol for both. Sometimes, R can even transform into a vowel, like in North American pronunciation of English words “nurse” and “butter.”There is a separate branch of linguistics devoted to this phoneme, called rhotics, and even a conference, where for three days in a row, scientists from 17 countries talk about all things R. “People actually debate whether it is a true class of sounds that should be identified with each other across languages or we group them mostly because of orthography. They’re written with a letter R across languages, so we consider them all the same sound. But if you look at them acoustically, articulatory they have a lot of differences between them,” says Tara McAllister Byun, a linguist and speech-language pathology professor at New York University. “The English r sound is notoriously difficult. It is extremely late to emerge in development. Developing children up to the age of eight may still be arriving at an adultlike pronunciation of the r sound, whereas most phonemes come in much earlier in development.”For the past five years, she has been leading an interdisciplinary team developing an app—staRt, “Speech Therapist’s App for r Treatment”—that uses visual-acoustic biofeedback to treat rhotacism, mispronouncing the r sound. Rhotacism is one of the most common and challenging speech errors: no verbal, visual, or tactile cues can clearly explain how to enunciate the r. Sometimes, children don’t hear the difference between correct and incorrect r sounds, which makes it even more challenging to improve.“You are twisting your tongue into a complex shape, and that is motorically more complex than producing a single major constriction for a sound like a or u,” says McAllister Byun. In other words, r is tricky for the tongue because we have to twist in a fancy way. It’s like a complicated dance move that could take weeks to learn—like moonwalking.By now, I used R about 300 times in this article. The combination of frequency and diversity makes the R a perfect social differentiator, a marker of our identity. Our speech holds dozens, if not hundreds, of markers like this. Imagine if we could read them all—it’d be like a superpower. All it takes to uncover it is to start paying attention to small details, the r sound being the most conspicuous.Sevindj Nurkiyazova is a journalist from Kyrgyzstan. Follow her on Medium @calempir.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15981_a1c9a80b51fb1a982b13329a1dc58f95.jpg",
    "title": "The Problem with the Way Scientists Study Reason",
    "description": "Posted by Sacha Altay on April 29, 2019  In March, I was in Paris for the International Convention of Psychological Science, one of the most prestigious gatherings in cognitive science. I listened to talks from my field, human…",
    "category": "Culture",
    "content": "In March, I was in Paris for the International Convention of Psychological Science, one of the most prestigious gatherings in cognitive science. I listened to talks from my field, human reasoning, but I also enjoyed those on ethology, because I find studies on non-human animals, from turtles to parrots, fascinating. Despite their typically small sample sizes, I found the scientific reasoning in the animal-studies talks sounder, and their explanations richer, than the work I heard on human reasoning.The reason is simple: Ethologists evaluate their experimental paradigm, or set-up, in light of its ecological validity, or how well it matches natural surroundings. An animal’s true habitat, and its evolutionary history, have always centered the discussion. In contrast, most experimental paradigms in human reasoning, such as the Cognitive Reflexion Test (CRT) or syllogisms, are based on logic or mathematics. One of the most famous tasks of the CRT is the bat and ball problem: A bat and a ball cost $1.10. The bat costs $1 more than the ball. How much does the ball cost? Most participants fail at this task. The correct answer is not 10 cents, but 5 cents. Perhaps the ultimate tool psychologists use to study reasoning is the syllogism: For example, “Major premise: All men are animals. Minor premise: Some animals are aggressive. Conclusion: Some men are aggressive.” (Does this conclusion follow?)As I listened to talks relying on these methods, I wondered: Do people think like that in everyday life? Probably not. Did our Pleistocene ancestors? Very unlikely. Then, how should I interpret these results? Is using abstract logic on humans like asking a turtle to climb stairs?Nikolaas Tinbergen, the founder of behavioral ecology, famously stated that ethology is the art of interviewing animals in their own language. This principle is simple but powerful. And there is no reason why it should not be applied to humans. Psychologists studying reasoning extensively rely on logic and philosophy, and neglect psychology’s more natural ally: biology. The neglect stems in part from the ease with which humans can seem to understand one another. Our psychology is equipped with specialized cognitive systems, like theory of mind, that help us negotiate social life. We spontaneously attribute intentions, reasons, and beliefs to others. These heuristics help us to predict behavior, but they also parasitize our scientific understanding of the mind, blinding us to the necessity of using biology when studying ourselves. With turtles, there’s no problem, because we have only weak intuitions about their behaviors, and it’s difficult to ask them what they think.Humans are, in other words, too familiar with one another. Fundamental laws of biology, like evolution by natural selection, are falsely believed to have weak constraints on human psychology—particularly for high-level cognitive functions, like reasoning. But the human brain, just like the turtle brain, has been shaped by millions of years of evolution. Reason is unlikely to have escaped its influence. What does it mean, then, to interview humans in their own language?In a more ecological setting, when we can argue and reason collectively, the right answer spreads like wildfire.Let’s take a concrete example. One of the most discussed tasks in the psychology of reasoning is the Wason selection task, named after English psychologist Peter Wason: “Each card has a number on one side, and a patch of color on the other. Which card or cards must be turned over to test the idea that if a card shows an even number on one face, then its opposite face is red?”Most of us will turn 8 and the red card (even though this one is useless), neglecting the orange card that could falsify the rule (if, on its opposite face, we found an even number). Humans are very bad at this task. But what happens when we use ecological stimuli? Psychologists Leda Cosmides and John Tooby created a social version of the Wason selection task: “Each card has an age on one side, and a drink on the other. Which card(s) must be turned over to test the idea that if you are drinking alcohol then you must be over 18?”If you are like most participants, it now seems obvious that you need to flip the beer and the 16 cards. You solved the problem effortlessly even though the task is, logically, the same as before. Our big brains likely evolved to solve tasks related to social interactions, not abstract logical problems. The Cosmides-Tooby selection task was ecologically valid; the first one wasn’t. Using the wrong experimental design, whether it’s the task itself or the stimuli, exposes researchers to many problems—the main one being that the results become hard to interpret. You don’t know if what you found reveals an interesting feature of the human mind—such as that human deductive reasoning is biased in the classical Wason selection task—or if it’s just a methodological artifact because the stimuli were not ecological.This is why it is important, when analyzing a biological mechanism, to consider an animal’s evolutionary history—the environment in which its ancestors evolved, and the recurrent problems they had to solve. Four broad questions can be answered: “How does it work?”; “How does it develop?”; “Why does it work like that?”; and “How did it develop in the course of evolution?” The first two questions offer proximate explanations, whereas the two last questions offer ultimate explanations.As an illustration, imagine you are asked to experiment with the strengths and weaknesses of this object:You could use it as, say, a hammer, and realize that it is terrible at hammering nails. Maybe it’s some kind of hole puncher? While smashing paper sheets, a colleague asks you, confused: “Why would you try to punch holes in paper sheets with a cherry pitter?!” Wow—all of a sudden, everything becomes clear. You know exactly how to use it. You are now able to predict what it might be good (or bad) at. You feel awful for calling a cherry pitter a bad hammer…It was inappropriate.This logic of reverse engineering, at the heart of evolutionary biology, is rarely applied to reasoning. Instead, scholars attribute to reasoning the role of correcting one’s intuition and solving problems. This role is often left implicit because it’s rarely considered an object worthy of scientific discussion.1 In fact, it has long been a black box that very few have had the courage to open with the adequate biological tools.But Hugo Mercier, who I work with at École Normale Supérieure, and Dan Sperber recently ventured there in their 2017 book, The Enigma of Reason. According to them, reasoning is not a capacity to correct false intuitions or solve problems. Nature is full of problems that organisms have to solve (like finding a mate, or food for dinner) and they constantly update their priors, or beliefs, about their environment in a broadly rational fashion. For example, in the Sahara lives Cataglyphis fortis, a species of ant using a “celestial compass” and an “odometer” to find the shorter way back to the colony after finding food. These complex inferences rely on a specialized cognitive system—falling outside the domain of reasoning—updating the ants’ priors about the environment and allowing them to solve a difficult task: finding their way home in the desert.Mercier and Sperber say that reason is a tool that evolved to solve particular problems related to communication, like evaluating information provided by others, convincing family or tribe members with arguments, and justifying one’s behavior to protect and improve one’s reputation in a complex social world. Their theory makes novel and testable hypotheses, like that reason works best when people argue with each other rather than reason alone, and that we evaluate arguments more objectively than we make them.In light of their theory, the failure and success of reasoning tasks makes more sense. For example, alone, we are mediocre at solving the bat and ball problem, but in a more ecological setting, when we can argue and reason collectively, the right answer spreads like wildfire, as Mercier and his colleagues showed in a 2017 study, “Argumentation and the Diffusion of Counter-Intuitive Beliefs.”Instead of trying hard to find biases in human cognition using weird tasks with little ecological validity, the psychology of reasoning would be more productive if more researchers follow Tinbergen’s simple lesson: Interview animals in their own language. Otherwise, we might be lost in translation for a while.Sacha Altay is a Ph.D. student in cognitive science at École Normale Supérieure in Paris. He works on argumentation, misinformation, and how we evaluate communicated information. Follow him on Twitter @Sacha_Altay.\n\tThe newest and most popular articles delivered right to your inbox!\nFootnote1. There are notable exceptions, like the work of Gerd Gigerenzer and his colleagues, on the adaptive role of biases. "
  },
  {
    "imageUrl": "http://static.nautil.us/15949_afab9e1401c7c4b559e2723add38e05a.jpg",
    "title": " Taking Another Person’s Perspective Doesn’t Help You Understand Them",
    "description": "Posted by Brian  Gallagher on April 17, 2019  No moral advice is perfectly sound. The Golden Rule—do unto others as you would have them do unto you—is only as wise as the person following it.A more modern-sounding tip—take the…",
    "category": "Culture",
    "content": "No moral advice is perfectly sound. The Golden Rule—do unto others as you would have them do unto you—is only as wise as the person following it.A more modern-sounding tip—take the perspective of others—can seem like an improvement. It was Dale Carnegie’s eighth principle in How to Win Friends and Influence People (it is “a formula that will work wonders for you”), and Barack Obama trotted it out at the United Nations when discussing Israel and Palestine (“the deadlock will only be broken when each side learns to stand in each other’s shoes”). Perspective-taking avoids the Golden Rule’s flaw—its effect doesn’t hinge on the integrity of the person considering it. And it’s an inducement to selflessness, in that you’re encouraged to exchange your frame of reference for that of another. Perspective-taking increases the odds you’ll emotionally empathize with the person whose shoes you’re stepping into, rely less on your own biases and group-based stereotypes, and avoid automatic expressions of racial bias.In a recent study, authors Tay Eyal, Mary Steffel, and Nicholas Epley say these results, among others, “suggest that being told to put oneself into another’s perspective may result in increased interpersonal accuracy,” an understanding of the thoughts and desires of someone else. It doesn’t. After testing the impact perspective-taking had on the accuracy of interpersonal judgments in 25 experiments, the researchers concluded, “If anything, perspective-taking decreased accuracy overall while increasing confidence in judgment.” Even romantic partners together for a decade on average couldn’t get perspective taking to work when quizzed on their significant other’s preferences or views. They thought, on average, that 13 of their 20 guesses would be accurate after being quizzed. Only five were.“Another person’s mind comes through their mouth.”This is a counterintuitive finding, Epley explained in a piece for NPR. “The vast majority of people we surveyed predicted that actively adopting another person’s perspective would help them understand another person better in a variety of ways, from understanding another person’s reaction when looking at a picture to predicting movie preferences,” he wrote. “Perspective-taking may work some wonders for your social life, but understanding another person better does not seem to be one of those wonders.”The lesson for Epley can seem “painfully obvious.” To understand someone, we should not imagine their point of view but make the effort to “get” their perspective. “True insight into the minds of others is not likely to come from honing your powers of intuition,” Epley wrote, “but rather by learning to stop guessing about what’s on the mind of another person and learning to listen instead.”This takeaway undermines the idea that moral progress depends on perspective-taking, a thesis that George Eliot built into her classic novel, Middlemarch. “Middlemarch is deeply ethical,” Rebecca Newberger Goldstein, a philosopher, novelist, and author, most recently, of Plato at the Googleplex: Why Philosophy Won’t Go Away, said. “The differences between her characters are ethical differences which are shown as differences in the limits of their capacity for sympathetic imagination. All of her characters are ... after their own wellbeing, but, for some of them, their characters are such that they are able to imagine themselves into others. They are the characters who undergo moral progress and moral expansion. She makes the limits of imagination—not the limits of reason—essential to how much moral progress a character can make.”Epley, by contrast, doesn’t seem as taken by the ability to imagine the inner reality of others, since the data indicates that what we picture is often spurious. He’s advocated conversation over imagination for years. In 2015, Epley told Nautilus’ Kevin Berger, “Another person’s mind comes through their mouth.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in June 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/16009_91dacc0f5b8d3192aa89bc86db450dc1.png",
    "title": "The “Emodiversity” of Star Wars",
    "description": "Posted by Brian  Gallagher on May 06, 2019  This past “Star Wars Day,” May 4, I watched some of the original trilogy a bit mournfully: Peter Mayhew, who played Chewbacca, passed away the day before. When The Empire Strikes Back…",
    "category": "Culture",
    "content": "This past “Star Wars Day,” May 4, I watched some of the original trilogy a bit mournfully: Peter Mayhew, who played Chewbacca, passed away the day before. When The Empire Strikes Back took us to the Yoda-dwelling Dagobah, I recalled what the exiled Jedi Master had told premonition-plagued Anakin Skywalker decades earlier, about how to deal with the fear of losing loved ones. “Death is a natural part of life,” he tells Anakin. “Rejoice for those around you who transform into the Force. Mourn them do not. Miss them do not. Attachment leads to jealousy—the shadow of greed, that is.”Yoda is often held up as an avatar or icon of sagacity. Take a recent example. The psychologist Igor Grossman, who heads the Wisdom and Culture Lab at the University of Waterloo, published a study in January with colleagues on the role of emotion in reasoning wisely. In their study—which involved observational, diary, and experimental methods and almost 4,000 participants—the researchers found that the variety and relative abundance of emotions that humans experience, or “emodiversity,” was linked to wisdom. “The positive association between emodiversity and wisdom-related characteristics occurred consistently for daily challenges, unresolved interpersonal conflicts, as well as political conflicts,” they concluded.The University of Waterloo promoted the study with a press release titled, “In test of wisdom, new research favors Yoda over Spock.” What the hyper-rational Vulcan represents, Grossman said in the release, is “uniform emotional down-regulation,” whereas the hallmark of Yoda’s judgment style is his ability to “recognize and balance a wide range of emotions.” Spock, in other words, suppresses his feelings while Yoda evens them, and it’s this evenness that aligns with wise reasoning—and well-being. A 2014 study Grossman cites found emodiversity to be an “independent predictor of mental and physical health—such as decreased depression and doctor’s visits—over and above mean levels of positive and negative emotion.”You could be excused, though, for thinking the language of Yoda—and even the Jedi Code—is more Spockian than Grossman seems to believe. Yoda was right to remind Anakin that we’re born into a losing struggle against death. But don’t mourn or miss the dead? To guard against jealousy and greed, avoid attachments altogether? Yoda seems to be laboring under a sort of cognitive dissonance, recognizing the naturalness of death, but not of emotional bonds and responses. But that might be an uncharitable reading. Yoda, after Anakin asks him what he should do about his feelings of impending loss and death, tells him, “Train yourself to let go of everything you fear to lose.” It’s not that Yoda would rather eliminate the influences of emotion; his aim is to master them. This stems, no doubt, from the Jedi Code, the first line of which is: There is no emotion, there is peace.Even an act of kindness may have more severe repercussions than you know, or can see.The Jedi approach to emotion and wisdom echoes what Grossman titled his study, “Wise reasoning benefits from emodiversity, irrespective of emotional intensity.” Several psychological studies in the last decade, Grossman notes, have converged on what wise reasoning means: intellectual humility, acknowledgment of different perspectives, appreciation of the context within which an issue unfolds, sensitivity to the fact that social relations may change, acknowledgment of the likelihood of multiple outcomes of a conflict or action, a self-transcendent viewpoint on an issue, and preference for compromise in resolving opposing viewpoints. He and his colleagues didn’t uncover any mechanisms explaining the relationship between these characteristics and emodiversity, but offered one idea: More varied emotional experiences could encourage greater conceptual knowledge, which might promote wise reasoning. “This idea would imply,” they wrote, “that the central element linking emodiversity to wise reasoning concerns contextually sensitive knowledge,” or “knowledge of how a specific strategy fits a given situation.”An example might be whether it would be wise to indulge in a spontaneous act of charity. At one point in Star Wars Knights of the Old Republic II: The Sith Lords, a 2004 role-playing video game set 4,000 years before the film A New Hope, you travel to Nar Shaddaa, a crime-ridden moon-metropolis. Right after docking, a beggar confronts you, and you can either kindly give him “five credits” (the Light-Side option) or say to him, “Get out of here before I kill you” (the Dark-Side option). What’s made this encounter an iconic set piece of the game is that, no matter which option you choose, one of your companions—a powerful old woman named Kreia who is, as far as you know, a sort of lapsed Jedi—will offer withering criticism.Charity, for instance, prompts her to say: “Why did you do such a thing? Such kindnesses will mean nothing. His path is set. Giving him what he has not earned is like pouring sand into his hands. What if by surviving another day he brings greater darkness upon another? The slightest push, the smallest touch, sends echoes throughout life. Even an act of kindness may have more severe repercussions than you know, or can see. By giving him something he has not earned, perhaps all you’ve helped him become is a target.” The game then cuts to a scene showing the beggar being shaken down by another man, and struck down. “Seeing another elevated,” Kreia continues, “often brings the eyes of others who suffer, and perhaps in the end, all you’ve brought is more pain. And that is my lesson to you: Be careful of charity, and kindness, lest you do more harm with open hands than with a clenched fist.” (You can watch her response to the other choice here.)Kreia is not against helping others—she’s scornful of reflexive and blind compassion. The context demands shrewdness. Nar Shaddaa, to borrow the words of Obi-Wan, is a wretched hive of scum and villainy. Handing out money, in view of so many violent and impoverished people, may not be the best idea. This cold wisdom, and her critique of both the Jedis’ self-abnegating altruistic tendencies and the Siths’ self-destructive power-worshipping, have given people to wonder whether she may be the best-written female character in Star Wars, or all of gaming.In any case, her wise remarks anticipated a lesson in future Star Wars movies. The lesson being that it is wise to realize that allegiance to one ideology, or set of ideals, isn’t optimal, which is consistent with Grossman’s characterization of wise reasoning, namely the importance of considering multiple perspectives. “One quickly learns that the Jedi Code does not give all the answers,” Kreia says in The Sith Lords. “If you are to truly understand then you will need the contrast, not adherence to a single idea.” Luke Skywalker, in the latest film, The Last Jedi, says, “I only know one truth. It’s time for the Jedi to end.” And of course, viewers of The Last Jedi will remember that Yoda gleefully destroys ancient Jedi texts with a bolt of lightning.Which is in keeping with how Grossman sees Yoda. As he wrote in his study, “Yoda is known to be emotionally expressive, to share a good joke with others, but also to recognize sorrow and his past mistakes.” Perhaps wisdom is being jolly in owning up to folly.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15991_c51c69da77c98612490b49ed6d146a80.jpg",
    "title": " Why Water Is Weird",
    "description": "Posted by Brian  Gallagher on May 01, 2019  One day, frustrated after many hours of meditation and practice, Bruce Lee, still a teenager, went sailing. His martial arts teacher, Yip Man, had been instructing Lee in the art of detachment,…",
    "category": "Matter",
    "content": "One day, frustrated after many hours of meditation and practice, Bruce Lee, still a teenager, went sailing. His martial arts teacher, Yip Man, had been instructing Lee in the art of detachment, a key facet of gung fu. Lee couldn’t let go. “On the sea I thought of all my past training and got mad at myself and punched the water!” he later wrote. “Right then—at that moment—a thought suddenly struck me; was not this water the very essence of gung fu? I struck it but it did not suffer hurt. I then tried to grasp a handful of it but this proved impossible. This water, the softest substance in the world, which could be contained in the smallest jar, only seemed weak. In reality, it could penetrate the hardest substance in the world. That was it! I wanted to be like the nature of water.”For Lee, the budding martial artist, water embodied an ideal of lithe and effortless strength. He learned this from ancient Chinese philosopher Lao Tzu’s Tao Te Ching and updated it, adding, “When heated to the state of steam it is invisible but has enough power to split the earth itself.” It’s striking that water can illustrate and elucidate a martial arts philosophy while also being, to this day, the “least understood material on Earth,” as researchers reported recently.Water can appear to be “fine-tuned” for life.In their 2018 study, Hajime Tanaka, John Russo, and Kenji Akahane—all researchers in the Department of Fundamental Engineering at the University of Tokyo, in Japan—tried to tease apart what makes water unique among liquids. It’s got anomalous properties, like expanding when cooled below 40 degrees Fahrenheit, which explains why lakes freeze downward, from top to bottom, rather than up. Normally frozen solids are more dense than their liquid equivalents, which would mean that frozen chunks would fall to the bottom of a lake instead of staying on top. Water also becomes less viscous compared to other liquids when compressed, and has an uncanny level of surface tension, allowing beings light enough, like insects, to walk or stand atop it. Since it’s these distinctive features among others that power our climate and ecosystems, water can appear to be “fine-tuned” for life.The researchers, with the benefit of supercomputers, were able to tweak and untune a computational model of water, making it behave like other liquids. “With this procedure,” Russo said, “we have found that what makes water behave anomalously is the presence of a particular arrangement of the water’s molecules, such as the tetrahedral arrangement, where a water molecule is hydrogen-bonded to four molecules located on the vertices of a tetrahedron,” a shape of four triangular planes. “Four of such tetrahedral arrangements can organize themselves in such a way that they share a common water molecule at the center without overlapping,” Russo said. As a result, when water freezes, it creates an open structure, mostly empty space and less dense than the disordered structure of liquid water, which is why water props ice up. Both highly ordered and disordered tetrahedral arrangements give water its “peculiar properties.” The paper’s title spells this out: “Water-like anomalies as a function of tetrahedrality.”Nautilus asked Richard Saykally, a chemist at U.C. Berkeley, why these peculiarities make the liquid so ripe for scams and fanciful speculations. The ancient Greeks thought water was one of the four “essential” elements, the others being earth, air, and fire. Homeopathy, which purports to cure illness using small doses of disease-causing substances dissolved in water, evolved out of this, Saykally said. But there are more modern magical claims about so-called “structured” or “hexagonal” water. Some “wellness” practitioners claim humans age in part because we don’t replenish our stock of structured water. Depending on water’s structure, they say, it can penetrate your cell walls more effectively and has all kinds of health benefits.“There’s no scientific basis to that at all,” Saykally said. “You can’t make structured water. Doesn’t make any sense because the hydrogen bond in water lives for a few picoseconds—10-12 seconds—and these hydrogen bond structures of water are rearranging very rapidly so you don’t have water clusters existing as isolated entities in water despite a lot of these claims.”The ancient Greeks may have been wrong about water being an essential element, but Saykally says it’s no coincidence that water is essential for life on Earth. “It’s something intrinsic about water in that the strong tetrahedral hydrogen bond network that water makes is a very flexible environment for chemical processes to happen,” he said. “It has the right properties to dissolve many ions; it has the right properties to cause what we call hydrophobic materials”—like proteins—“to fold up in special ways.”Saykally has invented a new laser to study water clusters, with the ultimate goal of producing “the perfect model for water,” he said. “We want to combine all the information available from studies of water clusters with our terahertz laser spectroscopy—from quantum chemical calculations and from condensed phase measurements—and make a computer model of water that will answer any question you ask. That perfect water model is what we have been calling the universal first principles model of water.”Watch Saykally, below, describe what he’d do with a supercomputer running a universal model of water.You can enjoy the rest of our conversation with Saykally here, in which he says, among other things, whether another form of liquid water is possible.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in April 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15956_281bc81fb21d55e267f37af58d79801a.jpg",
    "title": "Most Tech Today Would be Frivolous to Ancient Scientists",
    "description": "Posted by Sidney Perkowitz on April 19, 2019  Surrounded by advanced achievements in medicine, space exploration, and robotics, people can be forgiven for thinking our time boasts the best technology. So I was startled last year to…",
    "category": "Culture",
    "content": "Surrounded by advanced achievements in medicine, space exploration, and robotics, people can be forgiven for thinking our time boasts the best technology. So I was startled last year to hear Sarah Stroup, a professor of classics at the University of Washington, Seattle, give a speech called “Robots, Space Exploration, Death Rays, Brain Surgery, and Nanotechnology: STEMM in the Ancient World.” Stroup has created a college course integrating classics and science to show how 2,000 year-old Greek and Roman STEMM (science, technology, engineering, mathematics, medicine) underlie and illuminate the sciences today.Stroup starts with robotics. The Greeks made self-acting machinery such as an automaton theater, a first step toward building a real robot, and they imagined a mythological one. Talos, a bronze being made by the god Hephaestus (later the Roman Vulcan) patrolled the island of Crete and threw rocks at threatening ships, anticipating today’s development of intelligent battlefield weaponry that chooses its own targets. In the 4th century BCE, Aristotle foresaw other implications of intelligent machines when he wrote, “If every instrument could accomplish its own work… chief workmen would not want servants, nor masters slaves,” as is now happening when robots and artificial intelligence replace people.Stroup also gives the example of a “death ray,” what the laser was called after its invention in 1960. In 214 BCE, the Greek scientist Archimedes may have created an early version. He is said to have used metal mirrors to focus sunlight that burned the wooden ships of the Roman fleet attacking his city, Syracuse. Though the story is probably apocryphal, modern researchers have shown that metal mirrors could concentrate sunlight sufficiently to set a wooden craft afire.“I became weary of modern STEM sorts, imagining they had invented everything.”Presenting other accomplishments, Stroup notes that the Greeks explored space by naked eye and correctly concluded that the planets orbit the sun. The anatomist Herophilus dissected the human brain to understand its structure, and Hippocrates established medical ethics, expressed today in the Hippocratic Oath that defines a physician’s responsibilities. Centuries before modern nanotechnology, Roman artisans colored glass with embedded gold and silver particles only nanometers across, producing the famous Lycurgus Cup that looks green or red depending on how it is illuminated.Stroup stresses that ancient STEMM both supported warfare and supported the common good. This dual nature exists today, and I was curious to learn how Stroup compared the uses and consequences of technology in ancient times to ours. Not long after we met at her talk, we conducted this interview over email.We use the word “technology” daily. What would the word or concept have meant to an ancient Greek or Roman? “Technology” comes from the Greek τέχνη, techne, which designates art, skill, or cunning. In Greek, it can be applied to sculpture, to metallurgy, to any craft or a method or set of rules for doing anything. The Latin translation of τέχνη would be ars, from which we get our word art. I find it amusing that moderns tend to imagine technology and art as opposites, when in fact the root words—techne and ars—mean exactly the same thing. In terms of τεχνολογία—technologia—it means specifically a systematic treatment of grammar. The modern sense of the word technology is not found in the ancient word. In your presentation, you mentioned ancient technology vs. what you call modern tech. How do they differ?Much modern technology is built off ancient technologies and in many cases, we still don’t understand how they did certain things they did: We’ve not yet regained their knowledge. A major difference between ancient technology and modern tech is that the latter is industry-driven, whereas ancient technologies never were. As a result, modern tech is designed not necessarily for use value—much modern tech is entirely frivolous—but for a consumer market, and is designed for early obsolescence. Luxury tech must become unusable as swiftly as the consumer will tolerate. Can you imagine an aqueduct that needed to be replaced every 18 months?We can and do build durable technologies—medicine; aerospace—but the tech that most people depend on must appeal to our fears and vanities and must require continuous and rapid overturn. If it were truly necessary, the market would demand durability. Much modern tech is little more than current fashion, of which moderns have become the passive consumers.What do students take away from your course?The students generally experience a kind of existential crisis at some point in my class once they realize how much was lost from antiquity and how relatively little the modern period has produced. We’ve rediscovered a good deal, and of course the control of electricity has helped tremendously, but they had developed atomic theory, and I don’t think they were too far away from being able to control electricity.We talk about how moderns have trouble dealing with the advanced technologies of the ancients—we say things like ahead of his time, but if all these guys were ahead of their time, what you really mean is that was their time. As one student said last week, it’s like the typical “undeveloped society sees advanced technology of more sophisticated society, suspects magic,” except in this case, we’re the ones suspecting magic.“Luxury tech must become unusable as swiftly as the consumer will tolerate. Can you imagine an aqueduct that needed to be replaced every 18 months?”Besides the example of Hippocrates, how do your students learn about ethics as it connects to technology?Ethics is a Greek invention. Aristotle was the first philosopher we know who began to write treatises concerned with what came to be known as ethical philosophy. Ethics—and philosophy as a whole—were born along with Greek astronomy, mathematics, science, and engineering. Your students build ancient devices, like catapults, as part of your course. What does that teach them?  They realize that so much ancient engineering, like modern engineering, is about building things to kill people. We all have great fun launching catapults, trebuchets, and ballistas, but we also talk about the fact that each is a weapon, and was designed to kill. The main thing the students learn is how difficult it is to build these things. You spend nine weeks trying to build a catapult—with access to a huge workshop and electric tools, and you can use nylon cord—and it still barely launches a ball more than ten feet. Suddenly the past doesn’t seem so laughably simplistic. Though by the end of the term, they’re all over that misconception, anyway.How do you understand the value of history?History is a roadmap, and if you don’t study it you have only a scrap of paper with “you are here” scrawled on it. The more you learn about history, the larger your map. The larger your map, the more you are able to know where you’ve come from, and so know where you’re going.As a classicist, what inspired you to develop a course in ancient STEMM?My degrees are in philosophy and classics, but my background is in the sciences and that’s what I had planned to study in university. I read in the sciences, I design and build high power rockets, I adore math. As it happened, I stumbled into philosophy and Greek, the first two things to challenge me, and so I ended up staying here.While we know a fair deal about ancient Greek and Roman technological sophistication, there weren’t any courses in ancient technology/science/math when I was an undergrad. “Ancient technology” had never been part of the modern canon “classical studies” and the state of the evidence is even worse than for the rest of our field. Literary masterpieces make it through time, but technological treatises are disposable, and advanced machinery is melted down and reused. I invented my course both to fill this void and to give myself the excuse to study more of this myself. Also, I became weary of modern STEM sorts, imagining they had invented everything.In your talk, you noted the Greeks used a recyclable material, bronze. What would they have thought about our global dependence on plastic, a material used once, then discarded, but never again to go away?I believe they would be absolutely appalled.Sidney Perkowitz, Candler Professor of Physics Emeritus at Emory University, is the editor of Frankenstein: How a Monster Became an Icon, and the author of the forthcoming Physics: A Very Short Introduction and Real Scientists Don’t Wear Ties.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/16018_671792587502028b6cd4be7c4d662d08.jpg",
    "title": "Dark Matter Gets a Reprieve in New Analysis",
    "description": "Posted by Charlie Wood on May 09, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.The galactic center shines too brightly, like the glow of a metropolis at night where maps show only a town. To mend…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.The galactic center shines too brightly, like the glow of a metropolis at night where maps show only a town. To mend their cosmic cartography, astrophysicists have spent years debating what could be powering this excess of energetic light. In 2015 the arguments appeared to swing decisively in favor of a somewhat prosaic explanation—that a large population of dim neutron stars was responsible. But a new examination of that work, posted on the scientific preprint site arxiv.org earlier this month, revealed a likely flaw in those analyses. “There’s something happening in the data we don’t understand,” said Rebecca Leane, a theoretical physicist at the Massachusetts Institute of Technology and co-author of the paper.The new study, along with two others that came out in March, reopens the possibility that space-based instruments have found the first direct evidence of the elusive “dark matter” thought to pervade the universe.The problem first appeared in 2009, when Dan Hooper, an astrophysicist at the University of Chicago, and Lisa Goodenough, then a graduate student at New York University, noticed that NASA’s Fermi Gamma-ray Space Telescope appeared to be picking up too many of the energetic photons known as gamma rays.He suggested the anomaly could originate from a theoretical jumbling throng of dark matter particles in the galaxy’s center. While dark matter doesn’t shine or fraternize with known particles, in the right sort of collision these particles could annihilate in a shower of familiar matter and antimatter that would then go out with a puff of gamma rays. A measurement of these offshoots would represent the first evidence of dark matter that wasn’t exclusively gravitational in nature.“If you told me the background model we were using is actual reality, I’d be running around screaming ‘dark matter’ right now.”Yet dark matter wasn’t the only thing that could be generating the excess gamma rays. They could shine from cosmic lighthouses known as millisecond pulsars—magnetically charged neutron stars that make a thousand turns each second. A group of undiscovered pulsars too dim to be picked out individually could be bathing the center of the galaxy in extra gamma rays.Ultimately, two studies released in 2015 leaned toward the mundane. The Fermi data looked grainy. It had bright pixels suggestive of multiple millisecond pulsars, and dim pixels suggestive of no pulsars. If dark matter was the culprit, it should have colored all pixels more evenly. The dark matter interpretation, it seemed, was dying.Now Leane and Tracy Slatyer, a theoretical physicist at MIT and a co-author of one of those 2015 pulsar studies, have breathed some life back into the dark matter option.First, the duo cooked up a digital representation of the Milky Way galaxy. They added the known ingredients: stars, gas, dust and known pulsars. Then they included the hypothetical ingredients—not only a sizable helping of dark matter but also some additional small pulsars that were unaccounted for in the initial mockup. They studied this fake Milky Way using the same methods from 2015, to see if they could find the dark matter. Ultimately they learned that the unexpected pulsars threw off the analysis, leading to the erroneous conclusion that this dark matter–stuffed galaxy had little dark matter.The digital Milky Way served as a proof of concept for how surprising additions could shift results, but to see if something similar might be affecting the real galaxy, Leane and Slatyer added mock dark-matter data to actual Fermi data. But again, they found that their data analysis did not correctly identify the added dark matter. Instead it gave too much credit to grainy, pulsar-like points. And the more dark stuff they added, the more the model mistook that dark matter for pulsars. The technique failed to sniff out any of the fake dark matter until the researchers had injected enough to account for the observed gamma-ray glow five times over.Their conclusion: The earlier analysis was never going to find dark matter, regardless of whether it’s there. “Something about our understanding of the gamma rays is missing at this stage,” Leane said. “It’s possible to hide a dark matter signal, if it were really there.”Alessandro Cuoco, an astrophysicist at Aachen University in Germany who has worked with the same technique in other parts of the galaxy, isn’t surprised to hear that the pulsar verdict may be premature. The method relies on estimations of exactly where the gamma rays should be coming from, and if you’re off, the whole house of cards comes tumbling down. “This is an extremely delicate analysis. You have to take into account very tiny details,” he said, “and they can considerably change your conclusions.”Researchers emphasize that this result contains no new evidence for dark matter. Rather, it weakens the competing explanation for the galaxy’s gamma ray glow. “It puts the dark matter explanation into slightly better shape,” said Martin Winkler, a physicist at Stockholm University.In addition, two preprints published in March—one co-authored by Cuoco, another by Hooper—offer complementary support for the dark matter hypothesis. Both analyzed recent data from the International Space Station’s Alpha Magnetic Spectrometer (AMS) experiment. The new studies concluded that the AMS experiment has detected higher than expected levels of antiprotons — another possible remnant of dark matter collisions.Moreover, the kind of dark matter particles that would be needed to produce the AMS data are approximately the same kind of dark matter particles that would be needed to produce the observed gamma-ray glow in the center of the galaxy. The overlap has encouraged some astrophysicists to believe that they may be looking at a two-for-one explanation.Yet the AMS data are far from conclusive. The universe has a lot of ways of making antiprotons—most of those that come zipping by Earth are from humdrum cosmic rays crashing into gas clouds—and any analysis has to precisely account for all the known sources. The trick is figuring out whether AMS’s small uptick implies the existence of dark matter, or whether it’s just a rounding error, said Tim Linden, an astrophysicist at Ohio State University who co-authored the second of the recent papers. His group found moderate support for a dark matter interpretation, but only if they correctly calculated how cosmic rays travel, how protons get trapped in supernovae and how hard the solar wind blows. The math is probably right, Linden said, but the devil is in the assumptions: “If you told me the background model we were using is actual reality, I’d be running around screaming, ‘Dark matter!’ right now.”Cuoco, Linden and Winkler say details about the AMS instrument and precision proton measurements from particle accelerators will help refine their models. A clearer point for dark matter, however, would be if AMS or another high-altitude experiment found heavier, lumbering antimatter particles, which cosmic rays rarely produce. “That would be a smoking gun,” said Winkler.Experts express optimism that with these inputs, the current modeling wars could settle down in a matter of years. And now that the gamma ray glow is back on the table, hopes for dark matter look somewhat brighter. “If the galactic center excess is back in the game,” Leane said of the antiproton measurements, “potentially we are seeing the first signal of dark matter.”Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15946_8248f16fa738b0bfe6013edf69d873bf.jpg",
    "title": "Viruses Have a Secret, Altruistic Social Life",
    "description": "Posted by Viviane Callier on April 16, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Social organisms come in all shapes and sizes, from the obviously gregarious ones like mammals and birds down to the…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Social organisms come in all shapes and sizes, from the obviously gregarious ones like mammals and birds down to the more cryptic socializers like bacteria. Evolutionary biologists often puzzle over altruistic behaviors among them, because self-sacrificing individuals would at first seem to be at a severe disadvantage under natural selection. William D. Hamilton, one of the 20th century’s most prominent evolutionary theorists, developed a mathematical theory to explain the evolution of altruism through kin selection—for instance, why most individual ants, bees and wasps forgo the ability to reproduce and instead pour all their efforts into raising their siblings. Bacteriologists developed game-theory models to explain why bacteria in groups produce metabolites for their neighbors, even though some cheaters take advantage of the situation.But until recently, no one had considered that simple viruses, too, have social lives that influence their fitness and their evolution. “From a theoretical perspective, there is clearly huge potential for viruses to interact socially, leading to possibilities for cooperation and conflict,” wrote Stuart West, a biologist at Oxford University who studies the evolution of social behaviors, in an email to Quanta. “However, there has been relatively little attempt to tackle this empirically.”In a recent study published in Nature Microbiology, Rafael Sanjuán, an evolutionary geneticist at the University of Valencia in Spain, and his colleagues used a combination of theory and experiments to explore viral cooperation and conflict. They found that the spatial structure of a viral infection—the way that different sets of viruses can be isolated in separate compartments of the infected body—matters tremendously. In an evenly mixed system, altruistic viruses fall victim to “cheaters” that take advantage of their sacrifices, but if pockets in the body can isolate and shelter the altruists, they have a shot at survival.Consider the vesicular stomatitis virus (VSV), a less dangerous member of the same viral family as rabies. Viral infections usually stimulate the cells of their mammalian hosts to produce interferons, the signaling proteins that raise neighboring cells’ antiviral defenses and interfere with viral replication. The wild-type VSV has evolved ways to suppress its host’s innate immune system, but at the cost of reproducing more slowly. Still, that ability enables the population of suppressive viruses to thrive—unless a “cheater” variant comes along.“From a theoretical perspective, there is clearly huge potential for viruses to interact socially, leading to possibilities for cooperation and conflict.”The cheater does not have the ability to suppress its host’s defenses; in fact, its presence stimulates the release of interferons. But it still reaps the benefit of a lowered immune response because of the nearby VSVs that suppress interferon release. Because the cheaters don’t pay the reproductive cost of interferon suppression, they can outcompete the wild-type virus in the short term. From a social behavior standpoint, as Sanjuán and his colleagues pointed out in their paper, the wild-type VSV’s suppression of interferon qualifies as an altruistic act because in effect the wild type sacrifices itself for the cheater.Eventually, the host’s interferon response overwhelms both types of viruses and kills them. It might seem like natural selection would therefore always weed out the ability to suppress interferon because its altruism would perversely leave viruses that had it at a disadvantage.Sanjuán’s modeling study shows, however, that is not necessarily the case: The altruistic interferon-suppressing virus can still evolve and thrive if it and the cheater are physically segregated. Structures and barriers in the body can create havens where the interferon-suppressing viruses can survive, safe from the damage that cheaters would otherwise bring down upon them.To model the specific conditions in which innate immune suppression can occur, the researchers used the theoretical framework that Hamilton developed. According to Hamilton’s rule, altruism evolves when r × B > C, where B is the benefit to the recipient, r is the recipient’s relatedness to the giver, and C is the cost to the giver.The researchers also used a parameter to indicate that the benefit, B, depends on whether a virus is surrounded by wild-type or cheater neighbors. Applying Hamilton’s rule to well-mixed and spatially segregated combinations of the two VSV variants, they could empirically estimate the parameters in Hamilton’s equation.“For innate immune suppression to evolve, you need spatial structure,” Sanjuán said. Because both the virus and the host’s interferon response spread out from cell to cell, it’s actually quite difficult to avoid the emergence of spatial structures during infection. Limitations on the rate of diffusion of viral particles and interferon molecules, as well as physical barriers in the body’s tissues, easily create spatial heterogeneity, thus allowing for the evolution of innate immune suppression.In animals with complex behaviors and in bacteria with relatively complex communication systems, the outcomes of evolutionary scenarios are influenced by many factors. In the case of viruses, “it’s much simpler,” Sanjuán said. “Everything is dictated by spatial structure. There is no other known process that can affect the outcome of the system. If the viruses are mixed, then this altruism cannot evolve, and if they are segregated, then the altruism can evolve.”Another aspect of social evolution of viruses that Sanjuán is investigating is why multiple viral particles sometimes gather and infect a cell together. The trade-off is that, if the viral particles assemble, there are fewer units to infect different cells. So “in principle, this is costly because it limits diffusion capability,” Sanjuán said. But his team found, to their surprise, that the aggregated viruses grow faster and produce more progeny. This result was dependent on cell type: In tumor cells that have no innate immunity, being aggregated was costly. But in normal cells, which do mount an innate immune response, being aggregated was beneficial for the viruses because it allows the viruses to overwhelm the innate immune response, Sanjuán suggested.Although the strategy of aggregation for infection seems beneficial for the virus, it too can lead to the evolution of cheaters. For example, if one virus in the aggregate loses a few genes, then it can replicate more quickly, and with that advantage it can outcompete the other viruses in the aggregate. These gene-deficient viruses are known as defective interfering particles (DIPs): Many of them lack about 90 percent of the viral genome and survive as just a small piece of RNA that can replicate very quickly inside a host (they cannot ordinarily infect a new host because they are so incomplete). In cell cultures with a high density of viral infections, the DIPs take over and soon represent more than 99 percent of the viral population, Sanjuán said.The existence of DIPs may touch on another puzzle: Do viruses adjust their manipulations of one another in keeping with the needs of their life cycle? Raul Andino, a virologist at the University of California at San Francisco, points out that early in its invasion of a host, a virus might want lots of company because multiple simultaneous infections would increase its odds of success.“But then they want to reduce the possibility of high multiplicity of infection at a later stage, to reduce the possibility of production of these defective particles,” he said. “This is something we don’t fully understand, but it is a really interesting problem.”Viviane Callier, a biologist by training, works as a science writer at the National Eye Institute and freelances for various science news publications.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15973_d8502c4f5e4b6b2b61d6d833be5a18cf.jpg",
    "title": "Nurture Alone Can’t Explain Male Aggression",
    "description": "Posted by Steve Stewart-Williams on April 26, 2019  A young bank teller is shot dead during a robbery. The robber flees in a stolen van and is chased down the motorway by a convoy of police cars. Careening through traffic, the robber runs…",
    "category": "Biology",
    "content": "A young bank teller is shot dead during a robbery. The robber flees in a stolen van and is chased down the motorway by a convoy of police cars. Careening through traffic, the robber runs several cars off the road and clips several more. Eventually, the robber pulls off the motorway and attempts to escape into the hills on foot, the police in hot pursuit. After several tense minutes, the robber pulls a gun on the cops and is promptly killed in a hail of gunfire. It is later revealed the robber is a career criminal with a history of violent crime stretching all the way back to high school.Now tell me: Are you picturing a male or a female robber? If you look back at the last paragraph, you’ll notice that I didn’t actually specify the robber’s sex. Nonetheless, I’d be willing to bet that you were picturing a man. Don’t worry—you weren’t being sexist; you were simply playing the odds. Most men are not especially violent, but most people who are especially violent are men. And rare though they might be, men such as our fictitious robber are the extreme of a more general trend, namely that men are more violent than women, more in-your-face aggressive, and more prone to taking risks.Why? Where do these all-too-familiar sex differences come from? A recent New York Times opinion piece weighed in on this difficult question, and came to a fairly common conclusion. The headline captured the gist: “It’s Dangerous to Be a Boy: They smoke more, fight more and are far more likely to die young than girls. But their tendency to violence isn’t innate.” (Emphasis added.) In other words, sex differences in aggression come entirely from the environment: from culture rather than biology, nurture rather than nature. Let’s call this the Nurture Only position.Now, there’s no real doubt that social forces help to shape violence and aggression. Decades of research have shown that people’s behavior—aggression included—is responsive to incentives and training. The question, then, is not whether social forces matter, but whether social forces are the whole story. And the answer, in a nutshell, is “almost certainly not.” Biology matters as well. Here’s how we know…An initial line of evidence is that it’s not only in the West that we find sex differences in aggression. Wherever in the world we look, men are more violent and aggressive than women, especially with other men. The clearest and most persuasive evidence for this comes from homicide statistics: In every country, without fail, men commit the vast majority of homicides (and are more likely to be the victims of homicide as well). If the sex difference in aggression is just an arbitrary product of culture, why does it rear its ugly head in every human group?That was a rhetorical question, of course, but I should mention that some sociocultural theorists think they have an answer to it. The eminent psychologists Alice Eagly and Wendy Wood, for instance, argue that, although men clearly do engage in higher rates of violence, this isn’t a result of evolved differences in men and women’s minds. Instead, it’s an indirect effect of evolved differences in men and women’s bodies: namely that men are larger, stronger, and faster than women, and that women get pregnant and produce milk for the young. Because of these non-negotiable physical differences, men in every culture are funneled into social roles involving aggression and physical strength, whereas women are funneled into roles involving childcare. Like the ex-soldier who keeps on polishing his boots every day for the rest of his life, the roles we play in the world have enduring effects on our behavior and personalities. Over time, men actually become more aggressive and women become more caring.In short, Eagly and Wood provide a non-evolutionary-psychological explanation for the cross-cultural trend: The psychological sex differences are found in every culture because the physical sex differences are found in every culture. The physical differences are direct products of evolution; the psychological differences are not. If we re-engineered society’s gender roles, argue Eagly and Wood, the psychological differences would quickly fall away.It’s a clever argument, and one worth taking seriously. On balance, though, I don’t think it flies. To begin with, the Eagly–Wood theory raises some awkward questions. Why wouldn’t natural selection create psychological sex differences as well as physical ones? The mere existence of the physical differences tells us that human males have been subject to stronger selection for aggression and violence than females. Why would this selection pressure shape our muscles, our skeletons, and our overall body size, but draw the line at our brains? And why would natural selection give men the physical equipment needed for violence but not the psychological machinery to operate it? This would make about as much sense as giving us teeth and a digestive system, but not a desire to eat.Why wouldn’t natural selection create psychological sex differences as well as physical ones?On top of that, if sex differences in aggression were all down to gender roles, the differences would be larger in cultures with stricter gender roles and greater gender inequality. That’s not what we find, though. On the contrary, it seems to be the other way round. A recent large-scale, multinational study revealed, for instance, that sex differences in adolescent physical aggression are smaller, rather than larger, in less gender-equal nations. Culture clearly matters when it comes to sex differences in aggression—but the effect of culture is apparently very different than the social role theory would lead us to expect.Not only does culture affect us in unexpected ways, many of the social forces invoked by the cultural theorists have an unfortunate habit of not existing. Consider the claim that society encourages males to be aggressive. This is probably true in some ways; we do sometimes give boys the message that they ought to be tough and not cry. Overall, though, we spend a lot more time discouraging male aggression than female aggression. Why? Because males are more aggressive! Or consider the claim that we tell girls to be quiet and passive. Again, we probably do this sometimes. More often, though, we tell boys to be quiet and passive. Why? Same reason: Boys are louder and more disruptive!In the first piece of research I did as a graduate student, I found that people judged an aggressive act performed by a man to be less acceptable than the same act performed by a woman. This perception apparently translates into real-world behavior. In childhood, boys are punished more often and more severely for aggression. Similarly, in adulthood, male defendants get harsher sentences for the same crimes, even controlling for criminal history. Males, it seems, are more aggressive despite culture, not because of it. And this isn’t just the case in the West. In most cultures, boys are taught not to be aggressive, but in all cultures, boys and men are more aggressive anyway.(To preempt a common misunderstanding, this isn’t to deny that culture has any effect; we’ve already seen that it does. The point is simply that culture couldn’t be the entire explanation for the sex difference in aggression, because the sex difference in aggression appears even when culture pushes harder against male aggression than female.)Some argue that, even if culture doesn’t create the aggression sex difference out of nothing, it does still amplify a relatively trivial inborn difference. Often, though, culture may do the reverse: By clamping down on male aggression, culture may make the sex difference in aggression smaller than it would otherwise have been.Other evidence pushes us toward the same conclusion. The evolutionary psychologist John Archer points out that if the gender gap in aggression were due solely to socialization, it would presumably be smallest in the very young, then grow steadily as the years went by. After all, the longer we live, the more time the forces of socialization have to sink their claws into our minds and behavior, and thus the bigger any gender gaps should be.That’s the theory; the reality, however, is quite different.First, the sex difference in aggression appears very early in life—usually before children take their first bite of their first birthday cake. From the moment they can move around under their own their steam, boys engage in more rough-and-tumble play than girls. The same sex difference is found in other juvenile primates, and appears to be related to testosterone exposure in the womb. In humans, the sex difference shows up long before kids understand that they’re boys or girls, so it can’t just be that they’re conforming to social expectations about how boys and girls ought to act. In any case, children are terrible at conforming to social expectations, as any parent who’s tried to persuade their progeny to sit nicely and quietly in a restaurant will readily confirm. And not only does the sex difference in aggression emerge early, it remains static until puberty. Absolute levels of aggression trend downward for both sexes; however, the gap between the sexes barely budges. If socialization creates the sex difference, why doesn’t continued socialization before puberty pry the sexes apart?If we could cryogenically freeze all the males in this age bracket, we would instantly eliminate most of the crime and violence that plagues human societies.Second, as with many sex differences, the sex difference in aggression suddenly swells at puberty, and is larger among adolescents and young adults than among any other group. Like bull elephants in musth, human males often go a little crazy at this stage in the life cycle. The evolutionary psychologists Margo Wilson and Martin Daly dubbed this the young male syndrome: Males in the grip of this syndrome are more likely than any other demographic to be imprisoned, to kill someone, or to be killed by someone else—most often another young male. The behavioral geneticist David Lykken summed up the situation well when he observed that, if we could cryogenically freeze all the males in this age bracket, we would instantly eliminate most of the crime and violence that plagues human societies.How would the Nurture Only approach explain the violence gap that opens up between the sexes at puberty? Is there a sudden surge in gender socialization—a surge which, for some unknown reason, happens at exactly the same stage of life in every culture and in many sexually dimorphic species? Is it just a coincidence that this alleged surge in socialization comes at the same time as the massive surge in circulating testosterone that accompanies puberty in males?Third, after the violence and mayhem of early adulthood, male aggression steadily nosedives through the remainder of the lifespan. The socialization hypothesis offers no particular reason to expect this. But the decline in violence coincides almost perfectly with the decline in testosterone found in men throughout the adult years, and mirrors the decline found in males of other species. Once again, this is much easier to explain in evolutionary than in sociocultural terms.A final line of evidence that sex differences in aggression have biological underpinnings is that these differences are not unique to human beings. Indeed, in some cases, the parallels across species are striking. Consider humans and chimpanzees. Among humans, males commit around 95 percent of homicides, and are around 79 percent of homicide victims. Among chimps, on the other hand, males commit around 92 percent of “chimpicides,” and are around 73 percent of chimpicide victims. In short, the sex difference in lethal aggression in the two species is remarkably similar in size.And chimps are just the beginning. Sex differences in aggressive behavior are found in a great diversity of species, including most mammals. Why would differences that clearly have an evolutionary origin in other species have an entirely different genesis in our own? In the absence of a convincing answer to this question, the default assumption should be that they wouldn’t have a different genesis, and thus that the sex differences we see in our species have the same root cause as those in our nonhuman kin.None of this implies, by the way, that we’re necessarily stuck with male aggression, or stuck with aggression in general. As the psychologist Steven Pinker demonstrated in The Better Angels of Our Nature, levels of violence and warfare have fallen steadily over the decades, centuries, and millennia, despite the fact that aggression is part of human nature. In various ways, from policing and government to trade and moral norms, we’ve managed to pull ourselves, to a significant extent, out of the vortex of violence and bloodshed that characterized our species for the bulk of its tenure on Earth.If we want to continue on this trajectory, however, or ideally to hasten our progress, our best bet is presumably not to delude ourselves about the true causes of our behavior. As policy wonks like to say: Wrong diagnosis; wrong cure. Let’s get the diagnosis right so that we can maximize our chances of curing the scourge of human violence.Steve Stewart-Williams is an associate professor of psychology at Nottingham University Malaysia Campus and the author of The Ape That Understood the Universe: How the Mind and Culture Evolve (Cambridge University Press, 2018), excerpts from which formed the basis of this article. Follow him on Twitter @SteveStuWill.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15941_5717711e6607ece2b6b847da2133f700.png",
    "title": "An Astrophysicist on What the Black-Hole Image Reveals",
    "description": "Posted by Pankaj S. Joshi on April 12, 2019  The great irony of black holes is that, in all the decades that we astrophysicists have talked about them, we never had any direct observational evidence for them. When astronomers said…",
    "category": "Matter",
    "content": "The great irony of black holes is that, in all the decades that we astrophysicists have talked about them, we never had any direct observational evidence for them. When astronomers said they had “found black holes” in this or that location in a faraway location in the universe, what this really meant was a very compact object—an enormous concentration of mass, far greater than that of any conventional star or planet.On Wednesday all that changed. The key characteristic of a black hole, what makes it black and a hole, is the existence of an event horizon—a one-way membrane or, if you like, a boundary in spacetime. Because of it, black holes gulp everything but do not allow any matter within, or even light, to escape. The Event Horizon Telescope (EHT) has now precisely offered that evidence in the form of a shadow that the black hole necessarily casts.Strictly speaking, the system did not see an event horizon, which cannot be seen by definition. Furthermore, although an event horizon necessarily implies a shadow and silhouette, the converse is not true. Nonetheless the observations are still so precise that whatever is casting the shadow must be exotic. No ordinary body could be so small and yet so dark and so massive. A black hole is now the most conservative conclusion. If it is not a black hole, it might be a naked singularity, a type of immensely dense object that I have studied, and that would make a black hole look rather mundane.The EHT is not a telescope in the normal sense of the word, but actually it is a network of several of the existing radio telescopes, spread across several continents. Each of these telescopes receive radio waves from the central region of the galaxy Messier 87. The data from the individual instruments is then compiled and processed. At the moment the system is restricted to the largest black holes. The images now making the rounds are of a supermassive black hole, with the mass of 6.5 billion suns, lying at the center of the galaxy Messier 87. The EHT is also seeking to observe the black hole at the center of our galaxy, which is roughly comparable. The angle such an object presents to us on the sky diminishes with mass. For now, black holes equal in a mass to a single star are beyond reach.Beyond confirming the existence of the black hole, EHT tells us that all its features of the hole match the predictions of Einstein’s general theory of relativity. The object is rotating clockwise. A black hole, unless a hole in your jeans, spins. This rotation drives the jets that the hole ejects from its environs into deep space. Interestingly, the image from the telescope is asymmetric, which might be a further clue to the spin and mass.Clearly, though, these are still very early days. The scientists and team leaders readily acknowledged that the image was somewhat fuzzy. They are planning to add more telescopes to their current network of some ten radio telescopes. My own hope is that we can go beyond general relativity and directly measure the quantum properties of gravity, which we think may be the rock-bottom level of physical reality. Many have said such a goal is hopelessly ambitious. But people used to say the same about black holes, too.Pankaj S. Joshi is an astrophysicist who specializes in compact objects such as black holes. He is currently a vice chancellor and founding director of the International Center for Cosmology at the Charusat University in Anand, India.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15959_f542f98100aab2e01bcf4f253f88c107.jpg",
    "title": " The Problem with Mindfulness",
    "description": "Posted by Brian  Gallagher on April 24, 2019  Should we be mindful of how popular “mindfulness” now is? Carl Erik Fisher says we should. Fisher is a professor of clinical psychiatry at Columbia University and a practicing psychotherapist…",
    "category": "Ideas",
    "content": "Should we be mindful of how popular “mindfulness” now is? Carl Erik Fisher says we should. Fisher is a professor of clinical psychiatry at Columbia University and a practicing psychotherapist who integrates meditation in his practice, and meditates himself. But he worries some popular meditation practices, which stress salvation through a clear mind, undermine the genuine benefits of meditation. Recent studies in psychology show mindful meditation has been detrimental to practitioners.“The overselling of mindfulness can lead to this idea that we should always be rigidly focused on what’s in front of us and our minds should be totally clear of any sort of input or thought,” Fisher told Nautilus. “That’s a total misrepresentation. Mindfulness doesn’t mean the eradication of thoughts, in any tradition. In any sort of basic, secular, clinical application, it just means paying attention to the present moment…Maybe we need to clarify what we mean by mindfulness before we slap it on a bunch of posters in every school and every workplace.”Every year, at least 1 million new meditators arise in the United States alone. “Meditation Has Become a Billion-Dollar Business,” one Fortune headline announced. Willoughby Britton, director of the Clinical and Affective Neuroscience Laboratory at Brown University, and colleagues, wrote in a 2017 paper, “With more than 20 mindfulness phone apps, mindfulness is a major contributor to the billion-dollar meditation industry that serves more than 18 million meditators.” In a piece in Wired, Robert Wright, author most recently of Why Buddhism Is True: The Science and Philosophy of Meditation and Enlightenment, argued “How Mindfulness Meditation Can Save America.”One worry is the mindfulness movement’s heavy focus on positive, health-related perks, like stress or anxiety reduction. It turns meditation into a tool for mental hygiene. The reasoning goes like this, Fisher said: “Most of us spend at least four to five minutes a day brushing our teeth, so if we’re going to do that for our teeth, we might as well do it for our minds.” This, Britton and her colleagues write, “represents only a narrow selection of possible effects that have been acknowledged within Buddhist traditions both past and present.”In a 2014 study, for example, Tim Lomas, a lecturer in positive psychology at the University of East London, and colleagues, found that a quarter of the 30 male meditators they interviewed had troubling episodes—some encountered hard-to-manage thoughts and feelings; some exacerbated their depression and anxiety; and some became psychotic. One guy, a beginner, tried out an advanced method of deconstructing the self. “I crashed, lying on the floor sobbing,” he said. “I had a really strong sense of impermanence without the context, without the positivity. The crushing experience of despair was very strong…You just feel like you don’t exist, you’re nothing. It’s nihilistic, pretty terrifying.” Some negative experiences were less intense. “Doing mindfulness, you don’t like yourself sometimes,” another man said. “You just become aware, ‘Actually, I’m a bit of a shit.’” Lomas and his colleagues concluded, “Our paper raises important issues around safeguarding those who practice meditation, both within therapeutic settings and in the community.”Britton and her colleagues, in their 2017 paper, had the ambition of organizing meditative experiences in a single codebook, partly as a way to highlight—in a phrase recalling William James’ seminal book on religious experience—the “varieties of contemplative experience.” After interviewing 60 Buddhist meditation practitioners and teachers, including clinicians who implement meditation-based therapies, Britton and her colleagues came up with seven domains of experience, each containing at least five categories of changes meditationers felt, along with the percentage of those who felt it. Their sampling—43 percent female, 57 percent male, mean age 48—skewed toward the strange and strenuous. “In order to better understand the types of experiences that tend to be under-reported in scientific research, scholarship, and the media,” the researchers wrote, “the [Varieties of Contemplative Experience] study intentionally queried experiences that practitioners found unexpected, difficult, distressing, or functionally impairing.”“You just feel like you don’t exist, you’re nothing. It’s nihilistic, pretty terrifying.”In the cognitive domain, which has 10 categories, the two most-reported ones were “change in worldview” (48 percent) and “delusional, irrational, or paranormal beliefs” (47 percent). The most-reported category of experience in the perceptual domain (42 percent) was “Hallucinations, visions, or illusions.” The most-reported experience (82 percent) came from the affective domain: “Fear, anxiety, panic, or paranoia.” The other domains are: “somatic,” involving bodily feelings; “conative,” involving motivation and goal-related behavior; “sense of self”; and “social.” Half of the meditators Britton and her colleagues interviewed experienced “changes in self-other or self-world boundaries” and “social impairment.”These wouldn’t be surprising experiences for those familiar with what it takes to become an arhat, a “perfected person,” in Theravada Buddhism. (The Jain equivalent, an arihant, is a conqueror of the mind, writes Raj Pruthi, in Jainism and Indian Civilization, who “destroys his inner enemies like anger, greed, passion…”) Walk into the courtyard of Shanyuan Temple, in China, about an hour’s flight away from Pyongyang, North Korea, and you’ll be greeted by a formidable formation of 500 unique, life-sized, red-robed statues of arhats, all arrayed around another figure several times larger, in similar garments—Siddhārtha Gautama, the Buddha. Lore has it that, when the Buddha died, in about 400 BCE, these men convened the First Buddhist council to consolidate his teachings.To become an arhat involves following the Noble Eightfold Path—“right view,” “right conduct,” “right effort,” and so on. Being a good person isn’t the point—it’s a prerequisite, writes Peter Harvey, author of An Introduction to Buddhist Ethics. “With virtue as the indispensable basis for further progress, some meditation may be attempted. With appropriate application, this will lead to the mind becoming calmer, stronger, and clearer.” This sparks a virtuous feedback loop. Clarity and tranquility of mind aids virtue, the experience of acting well over time deepens wisdom, and meditative skill profits. “With each more refined development of the virtue–meditation–wisdom sequence,” Harvey goes on, “the Path spirals up to a higher level…to Arahatship.”You might say Fisher is still spiraling. The part of the Eightfold Path that he’s always struggled with, “even on the meditation cushion,” he said, is “right effort.” “Whether it’s some sort of basic compassion intention like, May all beings be happy; or whether it’s just simply staying with the breath, and following my awareness: What’s the right amount of effort? Do I hold onto it tightly and really try to make sure I don’t miss a second? Or do I relax and let my mind go where it will?” Fisher said. He’s pretty non-judgemental about it now—“There’s a balance of right effort”—and seems to suggest that it’s a mistake to privilege one way over the other. “There’s a whole spectrum of responses, and you’ll go across the entire spectrum over the course of a meditation session,” he said.For Fisher, meditation retreats in South Korea, right after college as a Luce Scholar, were an “invaluable” experience, he said, that “seeded that idea that mindfulness and more rigorous psychiatry and neuroscience could be blended together.” Still, he could find himself lapsing in his regular practice. “I would beat myself up and say I was a bad meditator and I’ll always be suffering and my life will be shit,” he said. “And I’ve let go of that a little bit.” Oddly, being around ambitious New Yorkers may have helped. “Their problem is generally not being too easy on themselves. Most people in the city are too hard on themselves!” Fisher said. “So, not that I am trying to apply my own experience to everybody, but I find a lot of commonality and a lot of common ground with my patients along those lines.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in March 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15891_206316442fb9a23e1665ccd7a4f151d2.jpg",
    "title": "When the Link Between Space and Time Will Be Intuitive",
    "description": "Posted by Brian  Gallagher on April 01, 2019  The celebrated English writer Ian McEwan has, in his stories and essays, shown a fondness for science. For two years he shadowed a neurosurgeon to write his 2005 novel Saturday, and his…",
    "category": "Culture",
    "content": "The celebrated English writer Ian McEwan has, in his stories and essays, shown a fondness for science. For two years he shadowed a neurosurgeon to write his 2005 novel Saturday, and his 2010 novel Solar focused on a Nobel Prize-winning physicist’s solar-energy solution to climate change. His forthcoming novel, Machines Like Me, delves into artificial intelligence. “[T]here actually is a special pleasure to be shared when a scientist or science writer leads us towards the light of a powerful idea which in turn opens avenues of exploration and discovery leading far into the future,” he wrote.On Sunday, The Guardian had theoretical physicist Carlo Rovelli answer queries from famous fans and Guardian readers pertaining to his latest book, The Order of Time (which Nautilus excerpted). It was no surprise that McEwan came first. McEwan wanted to know: Will humans ever experience the intertwining of space and time as intuitive, as simply and obviously inseparable? Or are we bound by our nature to always feel the connection as alien?Rovelli replied that he believed the counterintuitive phenomena—for example, of time moving slower for faster travelers—will, slowly, become intuitive. “It has happened with the fact that the Earth is a sphere (clarified two millennia ago) and the fact that it spins (clarified a few centuries ago). At first these were extremely counterintuitive ideas; nowadays we accept them as comprehensible. But it takes time. I think, for instance, that the day when we will have spaceships travelling very fast, and we experience directly things like meeting our children older than us on our return home…” he said. “[W]hen we experience this, the elasticity of time will become obvious to us. Of course, all this is assuming our civilization survives long enough and we do not destroy ourselves with conflicts and stupidity, which is something we humans seem to be very good at and not ready to move away from.”Those who saw the Christopher Nolan film Interstellar will already be familiar with the concept. Matthew McConaughey plays Cooper, an astronaut tasked with finding humanity a new home. A heart-wrenching moment comes, before he departs, when he has to tell his young daughter, Murphy, that, by the time he gets back, she may be older than him—and this infuriates her, because it means her father has no idea when he’d be returning. By the time they see each other again, he’s aged negligibly, and she’s an old woman, about to pass, the result of his spending too much time too deep in the vicinity of a black hole, named Gargantua. Perhaps, with the aid of compelling, scientifically pristine stories illustrating the entanglement of space and time, people’s apprehension of those concepts will become intuitive more quickly than they otherwise would. Maybe we won’t require the onset of a space-faring civilization, with the capability to warpspeed about the galaxy, to “get” Einstein’s physics as if it were second nature.In any case, we have nature itself to thank for appearing the beautiful way it did—in the form of a black hole—because if it didn’t look so stunning, the special effects team on Interstellar would have had to fake it. That’s what the theoretical physicist Kip Thorne, who worked on the film, told Chiara Mingarelli, a scientist studying gravity waves and black holes. “The way that they visualized Gargantua was by taking Einstein’s field equations and actually solving them on supercomputers in such a way that us scientists would have a really difficult time doing. That’s because we don’t have enough money to buy all these supercomputer hours, but this movie did!” Mingarelli told Nautilus’ Michael Segal. “But Kip told us that if it didn’t look nice, that it would’ve just been thrown out and completely discarded. This beautiful piece of science that we couldn’t afford to do on our own, if it didn’t look good it’s out the window! So I’m really glad that it looked nice.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15887_178946dc0cfb8cbcb7b4e672930affd5.jpg",
    "title": " The Self Is Other People",
    "description": "Posted by Brian  Gallagher on March 29, 2019  An oft-repeated line in A Series of Unfortunate Events, a Netflix TV show recently adapted from a book series, feels apt for the moment. “In a world too often governed by corruption…",
    "category": "Ideas",
    "content": "An oft-repeated line in A Series of Unfortunate Events, a Netflix TV show recently adapted from a book series, feels apt for the moment. “In a world too often governed by corruption and arrogance,” it goes, “it can be difficult to stay true to one’s philosophical and literary principles.” The story, by Lemony Snicket, follows the Baudelaire orphans—Violet (14 years old, when the series begins), Klaus (12), and Sunny (a baby)—who are shuttled from guardian to guardian, each one as incompetent as the next, as they’re chased by Count Olaf, a distant relative of theirs dead set on acquiring the children’s considerable fortune by any means necessary. The saying resonates with Violet and Klaus, who are keen intellects, because their desire to be noble conflicts constantly with what ensuring their own safety (and that of their friends) demands.The story can be read as an illustration of one potential measure of the self—how much you’re able to stay true to your principles despite what the world throws at you. It’s an ancient idea. Ryan Holiday, author of The Daily Stoic, wrote in the New York Times not long ago about the ancient Roman philosopher Seneca, who could very well have uttered the quote above, in 49 C.E., when he was asked to serve Emperor Nero, a man soon on his way to becoming a twisted tyrant. “Seneca was torn,” Holiday wrote. “To the Stoics, contributing to public affairs was a critical duty of the philosopher. Could Seneca decline to serve because he disagreed with the emperor? Could he leave a deranged Nero unsupervised? In time, Seneca would also come to the conclusion that when ‘the state is so rotten as to be past helping, if evil has entire dominion over it, the wise man will not labor in vain or waste his strength in unprofitable efforts.’”In January 2017, Holiday was offered a possible post as a communications director for a cabinet member of the Trump Administration; that opportunity, in retrospect, given the turmoil at the White House, reminded Holiday of Seneca’s plight, one he did not want to navigate. Holiday, unlike the Baudelaires, was able to avoid a situation that may have easily frustrated his ability to stay true to his philosophical principles (being truthful presumably being among them).But the self isn’t just the difference between what we might be, if we lived up to our ideals, and what we are. The philosopher Jean-Paul Sartre made this point in his play, No Exit, says Ken Perlin, an artist, engineer, and professor of computer science at N.Y.U., where he directs the Games For Learning Institute. Perlin has said that Sartre has influenced his ideas about how virtual reality will shape the self in the near future.“The characters in No Exit are characters in Hell, and they’re in Hell because they’re rotten people. Their punishment is to spend eternity in a room with each other, and it’s only Hell because they’re stuck with other rotten people. So it’s a kind of existential irony that I’m in Hell only because of who I am,” Perlin told Nautilus. “Sartre’s larger point—when the character says, “Hell is other people,”—is: None of us are alone, none of us are completely defined by our own view of ourselves. We’re constantly bouncing off other people and looking at other people as a mirror of us. Our very sense of who we are is intertwined with what we see when we see other people look at us.”For Perlin, this view of the self imposes an ethical obligation on technological development. It’s why he has taken to calling the Holodeck—the fictional virtual-reality platform from Star Trek—“other people.” He said, “I think we shouldn’t just go off and develop technologies: We should understand what is this interaction between people that we’re trying to support—specifically, not just information passing but emotional affirmation, our views of each other, our ability to relate to each other—all of those are central to any communication technology.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.This classic Facts So Romantic post was originally published in April 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15936_558e3aa9f7f9b7049d1fadda292d23a1.jpg",
    "title": "The Day Feynman Worked Out Black-Hole Radiation on My Blackboard",
    "description": "Posted by Nautilus Editors on April 11, 2019  The amazing image of a black hole unveiled Wednesday, along with data from the Event Horizon Telescope, may not substantiate Stephen Hawking’s famous theory that radiation, an example…",
    "category": "Matter",
    "content": "The amazing image of a black hole unveiled Wednesday, along with data from the Event Horizon Telescope, may not substantiate Stephen Hawking’s famous theory that radiation, an example of spontaneous emission at the quantum level, is emitted by a black hole. But the news did remind us of a story that physicist and writer Alan Lightman told Nautilus: Richard Feynman came up with the idea for spontaneous emission before Hawking. Here is Lightman in his own words:One day at lunch in the Caltech cafeteria, I was with two graduate students, Bill Press and Saul Teukolsky, and Feynman. Bill and Saul were talking about a calculation they had just done. It was a theoretical calculation, purely mathematical, where they looked at what happens if you shine light on a rotating black hole. If you shine it at the right angle, the light will bounce off the black hole with more energy than it came in with. The classical analogue is a spinning top. If you throw a marble at the top at the right angle, the marble will bounce off the top with more velocity than it came in with. The top slows down and the energy, the increased energy of the marble, comes from the spin of the top. As Bill and Saul were talking, Feynman was listening.We got up from the table and began walking back through the campus. Feynman said, “You know that process you’ve described? It sounds very much like stimulated emission.” That’s a quantum process in atomic physics where you have an electron orbiting an atom, and a light particle, a photon, comes in. The two particles are emitted and the electron goes to a lower energy state, so the light is amplified by the electron. The electron decreases energy and gives up that extra energy to sending out two photons. Feynman said, “What you’ve just described sounds like stimulated emission. According to Einstein, there’s a well-known relationship between stimulated emission and spontaneous emission.”Spontaneous emission is when you have an electron orbiting an atom and it just emits a photon all by itself, without any light coming in, and goes to a lower energy state. Einstein had worked out this relationship between stimulated and spontaneous emission. Whenever you have one, you have the other, at the atomic level. That’s well known to graduate students of physics. Feynman said that what Bill and Saul were describing sounded like simulated emission, and so there should be a spontaneous emission process analogous to it.We’d been wandering through the campus. We ended up in my office, a tiny little room, Bill, Saul, me, and Feynman. Feynman went to the blackboard and began working out the equations for spontaneous emission from black holes. Up to this point in history, it had been thought that all black holes were completely black, that a black hole could never emit on its own any kind of energy. But Feynman had postulated, after listening to Bill and Saul talk at lunch, that if a spinning black hole can emit with light coming in, it can also emit energy with nothing coming in, if you take into account quantum mechanics.After a few minutes, Feynman had worked out the process of spontaneous emission, which is what Stephen Hawking became famous for a year later. Feynman had it all on my blackboard. He wasn’t interested in copying down what he’d written. He just wanted to know how nature worked, and he had just learned that isolated black holes are capable of emitting energy when you take into account quantum effects. After he finished working it out, he brushed his hands together to get the chalk dust off them, and walked out of the office.After Feynman left, Bill and Saul and I were looking at the blackboard. We were thinking it was probably important, not knowing how important. Bill and Saul had to go off to some appointment, and so they left the office. A little bit later, I left. But that night I realized this was a major thing that Feynman had done and I needed to hurry back to my office and copy down the equations. But when I got back to my office in the morning, the cleaning lady had wiped the blackboard clean.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why black holes fascinate the public. "
  },
  {
    "imageUrl": "http://static.nautil.us/15923_fcf77efc6ffefda49699bc9188e0a933.jpg",
    "title": "In Quantum Games, There’s No Way to Play the Odds",
    "description": "Posted by Kevin Hartnett on April 10, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.In the 1950s, four mathematically minded U.S. Army soldiers used primitive electronic calculators to work out the…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.In the 1950s, four mathematically minded U.S. Army soldiers used primitive electronic calculators to work out the optimal strategy for playing blackjack. Their results, later published in the Journal of the American Statistical Association, detailed the best decision a player could make for every situation encountered in the game.Yet that strategy—which would evolve into what gamblers call “the book”—did not guarantee a player would win. Blackjack, along with solitaire, checkers, or any number of other games, has a ceiling on the percentage of games in which players can expect to triumph, even if they play the absolute best that the game can be played.But for a particularly strange variant of games, it’s impossible to compute this maximum-win probability. Instead, mathematicians and computer scientists are trying to determine whether it’s possible even to approximate maximum-win probabilities for these games. And whether that possibility exists hinges on the compatibility of two very different ways of thinking about physics.These “nonlocal” games were conceived in the 1960s by the physicist John Stewart Bell as a way to understand the bizarre quantum phenomenon called entanglement. While quantum entanglement is complicated, nonlocal games are not. You have two players, each of whom is asked a simple question. They win the game if their answers are coordinated in a certain way. Unfortunately they can’t communicate with each other, so each has to guess how the other is going to answer. Bell proved that if the players were able to share pairs of entangled quantum particles, they could enhance the correlations between their answers and win the games at higher than expected rates.“These algorithms are completely mysterious.”Over the past few years, researchers have elaborated on Bell’s setup, as I wrote in the recent article “The Universe’s Ultimate Complexity Revealed by Simple Quantum Games.” A 2016 paper by William Slofstra and a 2018 paper by Andrea Coladangelo and Jalex Stark proved that for some nonlocal games, the more pairs of entangled quantum particles the players share, the better they can play. This relationship holds indefinitely, meaning that players need infinite pairs of entangled particles (or entangled pairs with an infinite number of independent properties) to play nonlocal games the very best they can be played.One consequence of these results is that it’s impossible to compute the maximum-win probability for some nonlocal games. Computers can’t work with infinite quantities, so if the perfect algorithmic strategy requires an infinite number of entangled particles, then the computer can’t calculate how often that strategy pays off.“This is no general algorithm that, if you just put in a description of a game, will output the maximum success probability,” said Henry Yuen, a theoretical computer scientist at the University of Toronto.But if we can’t know the maximum-win probability exactly, can we at least compute it within, say, a few percentage points?Mathematicians have been hard at work on the question. Strangely, their approach depends on the compatibility of two very different ways of thinking about physics.Recall that the two players in a nonlocal game need to be kept from coordinating their answers. There are two ways to ensure this. The first is to physically isolate the players from each other—to place them in their own separate rooms or on opposite ends of the universe. This spatial isolation provides a guarantee that they can’t communicate. Researchers analyze this situation using what’s called the “tensor product” model (referring to mathematical objects called tensors).But there’s another way to ensure the players can’t conspire on their answers. Instead of separating them, you impose a different requirement: The order in which the two players measure their entangled particles and give their answers can’t affect the answers they give. “If the order in which they do their measurements doesn’t matter, then they clearly can’t be communicating,” Yuen said.In mathematics, when the order in which things is done doesn’t affect the final answer, you say that the operation commutes: a x b = b x a. This way of thinking about nonlocal games—based on order independence instead of spatial separation—is called the “commuting operator” model.The tensor product and commuting operator models are used in physics, particularly in the study of interactions between subatomic particles in an area of research called quantum field theory. The two models are different ways of thinking about what it means for physical events to be causally independent of each other. And while the tensor product model is more intuitive—our mind’s eye tends to picture causal independence in terms of physical separation—the commuting operator model provides a more coherent mathematical framework. This is because “spatial independence” is a kind of fuzzy idea, while a commuting relationship can be pinned down exactly.“For people who study quantum field theory, this notion of having spatially separate things is not a natural notion,” Yuen said. “At a mathematical level it’s not a given that you can really put two independent things in two separate locations in the universe.”Here’s what this all has to do with nonlocal games.Computer scientists can use the tensor-product model to calculate a floor for the maximum-win probability of nonlocal games. The algorithm they use guarantees that the maximum-win probability is above a certain threshold. Similarly, researchers can use the commuting operator model to establish a ceiling on the maximum-win probability. That algorithm can promise that it lies below some threshold.With these tools in hand, researchers want to squeeze these limits as close together as they can, like two pistons. They know they can’t make these two limits touch to produce a single exact maximum-win probability—recent work by Slofstra, Coladangelo, and Stark proved that the exact maximum-win probability is incalculable—but the closer they can bring them together, the more precisely they can approximate the maximum-win probability.And indeed, the longer these algorithms run, the more the two pistons appear to come together, producing finer and finer approximations around an ineffable middle value that they’ll never actually reach. Yet it’s unclear whether this observed convergence continues indefinitely. “These algorithms are completely mysterious. It’s not a gradual, smooth improvement on the numbers. We just don’t understand how fast they converge,” Yuen said.This piston strategy is premised on the two models being equivalent. It assumes that the ceiling and the floor squeeze a value in the middle. If the two models are in fact equivalent, then the two pistons really are on track to get arbitrarily closer together. (And by implication, if you can prove the pistons are on track to get arbitrarily closer together, you’ve also proven that the two models are equivalent.)But it’s possible the two models are not different ways of representing the same thing. It’s possible that they’re different, incommensurate, and as a result this piston strategy might lead to a situation where the ceiling gets pushed down below the floor. In this case, computer scientists would lose their best strategy for approximating maximum-win probabilities. Unfortunately, no one knows for sure.Over the last couple of years the biggest progress has come in the form of two proofs that have merely established just how hard the problem is to solve.In 2018 Thomas Vidick and Anand Natarajan proved that approximating maximum-win probabilities for nonlocal games is at least as hard as solving other notoriously difficult puzzles such as the traveling salesman problem. Also in 2018, Yuen, Vidick, Joseph Fitzsimons, and Zhengfeng Ji proved that as the pistons close in on each other, the computational resources required to push them closer together grow exponentially.In yet another twist to the story, the question of whether the two models are equivalent is a direct analogue of an important and difficult open problem in pure mathematics called the Connes embedding conjecture. This puts mathematicians and computer scientists in a three-birds-with-one-stone kind of situation: By proving that the tensor product and commuting operator models are equivalent, they’d simultaneously generate an algorithm for computing approximate maximum-win probabilities and also establish the truth of the Connes embedding conjecture. The achievement would win supreme plaudits across all the related fields.Which is to say, fittingly, all the questions are deeply entangled.Kevin Hartnett is a senior writer at Quanta Magazine covering mathematics and computer science. His work has been collected in the “Best Writing on Mathematics” series in 2013, 2016 and 2017. From 2013-2016 he wrote “Brainiac,” a weekly column for the Boston Globe‘s Ideas section.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15917_7737a2600285afe739fc99b6f0e9fd97.jpg",
    "title": "A Precursor Piece to DNA Was Found in Star Material",
    "description": "Posted by Brian  Gallagher on April 05, 2019  April—National Poetry Month—prompted me to reread some of the work of English-American poet W.H. Auden. In “Funeral Blues,” famously recited in Four Weddings and a Funeral, Auden…",
    "category": "Matter",
    "content": "April—National Poetry Month—prompted me to reread some of the work of English-American poet W.H. Auden. In “Funeral Blues,” famously recited in Four Weddings and a Funeral, Auden pairs musings on the cosmos with those on the human condition. The last four lines of “Funeral Blues” evoke grief over the loss of a loved friend:The stars are not wanted now: put out every one;Pack up the moon and dismantle the sun;Pour away the ocean and sweep up the wood;For nothing now can ever come to any good.Auden’s cosmic touch also comes through in the star-centered poem “The More Loving One,” a sort of stoic reflection on the life-giving importance of stars, their indifference to us, and the human penchant to take their powers for granted:Looking up at the stars, I know quite wellThat, for all they care, I can go to hell,But on earth indifference is the leastWe have to dread from man or beast.How should we like it were stars to burnWith a passion for us we could not return?If equal affection cannot be,Let the more loving one be me.Admirer as I think I amOf stars that do not give a damn,I cannot, now I see them, sayI missed one terribly all day.  Were all stars to disappear or die,I should learn to look at an empty skyAnd feel its total dark sublime,Though this might take me a little time. Stars, of course, not only sustain life, by showering planet Earth in photons, but also can contain, in their protostellar (or very young) form, a precursor material of life: a molecule called glycolonitrile. It is “one of the key precursors towards adenine formation,” scientists reported in January, in the Monthly Notices of the Royal Astronomical Society. Adenine is one of the four nucleobases comprising DNA, alongside guanine, cytosine, and thymine.The researchers trawled through archival data from one of the world’s biggest astronomy telescopes, the Atacama Large Millimeter/submillimeter Array (ALMA), and made the first detection of glycolonitrile, near a protostar about 450 light years from Earth in the Rho Ophiuchi cloud complex, one of the easiest star-forming regions to observe. The study’s lead author, Shaoshan Zeng, a research student at Queen Mary University of London, said in a press release, “We have shown that this important pre-biotic molecule can be formed in the material from which stars and planets emerge, taking us a step closer to identifying the processes that may have led to the origin of life on Earth.”How star formation leads to life formation is one of the puzzles Daniel Wolf Savin, an astrophysicist at Columbia University, has worked on. Involved is a “long chemical chain,” he told Nautilus’ Kevin Berger, “and we’re just focusing on the beginning.” In his lab, to study the genesis of organic chemistry, he’s merged a beam of atomic carbon with a beam of molecular hydrogen. “We watch the carbon and hydrogen molecules interact and form hydrocarbons, the first organic molecules.” He’s also used a similar method to study the formation of water.Watch our whole interview with Savin here.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15922_94bc2d7bde46c9df3a2647a6c78144c0.jpg",
    "title": "Nassim Taleb’s Case Against Nate Silver Is Bad Math",
    "description": "Posted by Aubrey Clayton on April 08, 2019  Since the midterm elections, a feud has been raging on Twitter between Nate Silver, founder of FiveThirtyEight, and Nassim Nicholas Taleb, hedge-fund-manager-turned-mathematical-philosopher…",
    "category": "Numbers",
    "content": "Since the midterm elections, a feud has been raging on Twitter between Nate Silver, founder of FiveThirtyEight, and Nassim Nicholas Taleb, hedge-fund-manager-turned-mathematical-philosopher and author of The Black Swan. It began, late last year, with Silver boasting about the success of his election models and Taleb shooting back that Silver doesn’t “know how math works.” Silver said Taleb was “consumed by anger” and hadn’t had any new ideas since 2001. The argument has gotten personal, with Silver calling Taleb an “intellectual-yet-idiot” (an insult taken from Taleb’s own book) and Taleb calling Silver “klueless” and “butthurt.” Here is a recap of what they’re fighting about so you can know who’s right (Silver, mostly) and who’s wrong (Taleb).The origin of Taleb’s ire can be found in Silver’s success since 2008—and his some-time failures. As I described in Nautilus last month, evaluating probabilistic election forecasts can be conceptually slippery, made especially difficult by the counterintuitive properties of mathematical probability. Historically, Silver has received considerable credit, probably too much, for “calling” elections correctly. As his most recent analysis (incorporating at least some of my suggestions!) shows, the more meaningful questions are: 1) How often does something that he gives an n percent chance to actually happen? and 2) How bold are his predictions, in the sense of probabilities being closer to 100 percent rather than long-run averages? In both respects, his models appear to be doing pretty well.Yet Taleb maintains that Silver (whom he has taken to calling “Bullshit Nate”) is “ignorant of probability,” a “fraud,” and “a total impostor.” Why?Critiquing—often rudely—the ways people think about uncertainty is the essence of Taleb’s brand. A thesis repeated in his books, backed up by his strategies as an investor, is that we understate the uncertainty concerning rare and extreme events in complex systems. By modeling randomness with assumptions gleaned when conditions are mild, we expose ourselves to unanticipated crises like market crashes. But a savvy investor, taking advantage of a market under-pricing those risks, can make a profit betting on the extremes.A “black swan”-like critique of the FiveThirtyEight forecasts is likely possible and even valuable.His complaint is that Silver’s forecast probabilities, particularly in 2016, fluctuated too much leading up to the election. Raising the stakes even more, Taleb has repeatedly hate-tweeted a paper of his, saying Silver’s forecasts were guilty of “stark errors” that, if committed on Wall Street, would have allowed investors to clean him out by means of what’s called “arbitrage.” Taleb, drawing on his financial background, proposed his own model that he claimed was immune to such an attack.Why does Taleb think finance has anything to say about election forecasting? It has to do with the sometimes-complex relationship between probabilities and prices. Imagine we were placing bets on an event, say, Trump being impeached, in the form of lottery tickets that pay $1 if he is. Suppose I’m willing to pay up to $0.25 for such a ticket. Financial theory can’t say whether that price is right or wrong, but it can say that, were I then offered the complementary ticket paying $1 if he isn’t impeached, the only rationally coherent price I could pay for it is $0.75, because if my price were, say, $0.77 instead, then someone could earn a sure profit of $0.02 by selling me both bets for $1.02. Similarly, if I am required to offer the bets at the same price I quote, then any price less than $0.75 would allow someone to earn a riskless profit by buying both from me.Today we would call any one of these combinations of bets an arbitrage, meaning an investment with no probability of loss and some probability of profit—a theoretically infinite money pump. The general assumption is that real arbitrage opportunities are, at most, very short-lived, because market forces act immediately to shut off the pump.Taleb says that Silver’s probabilities would make him a target for arbitrage if they gave the price he would offer on an election bet. Unlike the static version we just considered, the arbitrage would happen as time unfolds. Maybe an investor could “buy” an election forecast when it’s cheap (it claims a low probability of a given candidate being elected) and then “sell” it when the price goes up.Taleb argues the only right way to do election forecasting is to use the derivative-pricing techniques from mathematical finance. Derivatives are certain kinds of bets on the performance of an underlying security, like a stock. In this case, the “stock” is the voter support for a political candidate, and the bet is on whether that support will exceed a winning threshold on election day, similar to a “binary option” in the financial world.So, should Silver be scared of losing millions of dollars if he takes Taleb’s bait? In short, maybe but not certainly.In the 1930s, the Italian statistician and actuary Bruno de Finetti noticed something interesting about these kinds of bets: In order to avoid arbitrage, the prices must obey the same equations as the mathematical rules of probability, meaning such relationships as Price[A] + Price[NOT A] = 1, and so on. Taleb’s arbitrage-through-time was also known to de Finetti. Suppose, for example, on Monday a forecaster assigns a 40 percent probability to the event CLOUDY = “Tuesday’s sky will be cloudy” and a 30 percent probability to the event CLOUDY & RAINY = “Tuesday’s sky will be cloudy, and it will rain,” and these probabilities are interpreted as prices. Then we ask the forecaster/bookie: “If we wake up tomorrow and the sky is cloudy (and we have no other new information), what price will you give us for a bet that it will also rain?” De Finetti showed that the only arbitrage-free price for this conditional bet “RAINY GIVEN CLOUDY” is the one consistent with Bayes’ rule of conditional probability:Price[RAINY GIVEN CLOUDY] = Price[CLOUDY & RAINY] / Price[CLOUDY]Taleb’s wires get crossed, and as a result his argument borders on nonsense.In this example, the price should be (0.3)/(0.4) = 0.75. If the forecaster gave too high a price, say 0.90, then through a clever combination of transactions:BUY a bet of $1 on CLOUDY & RAINYBUY a bet of $0.75 on NOT CLOUDYSELL a bet of $0.15 on CLOUDYIf we see that it is cloudy, SELL a bet of $1 on RAINYwe could make a profit of $0.06 no matter what scenario unfolds. We could just as well make that $6 million, or however much we like, by increasing the amounts until we had emptied the forecaster’s bank account.Derivative-pricing in a consistent way that avoids arbitrage is a difficult problem (worthy of a Nobel prize)! An extended form of de Finetti’s argument is at the heart of the modern solution, in what’s now called the “Fundamental Theorem of Asset Pricing”: if prices are consistent, then they form a set of probabilities, meaning the equations of probabilities are satisfied.Unfortunately, Taleb’s wires get crossed here, and as a result his argument borders on nonsense. The converse to the statement above is also true: if prices form a set of probabilities, then they are consistent. Since Silver’s forecasts begin with probability models, it’s safe to assume they obey all the rules, including Bayes’, and would be arbitrage-free. Taleb’s procedure is therefore sufficient but not necessary, making his choice of model doubly unnecessary.Taleb’s real discomfort with Silver’s bouncing probabilities is less about arbitrage than it is about the relationship between past and future volatility. As I illustrated last month, even for a simple “random walk” model of elections—where a candidate’s support goes up or down every day according to a coin-flip—if the past and future polling volatilities are the same, then a candidate’s chance of winning will have the same amount of variability whether the polls are fairly stable or bouncing around. In both cases, the chance of winning may fluctuate dramatically. Again, since these are exact probabilities computed in the same way Taleb’s are, there is no possibility for arbitrage.In order for Taleb’s desired steady forecast probabilities to manifest, the uncertainty in the future polls would need to always be greater than the observed volatility of polls in the past. This is a valid “black swan”-like criticism that he could have made but didn’t: that we should not understate future uncertainty given our past experience observing mild fluctuations in the polls; otherwise we might get blindsided by an event like former director of the F.B.I. James Comey’s letter to Congress.In mathematical finance, derivatives depend on a traded asset like a stock with a known market price. What’s uncertain is whether that price will reach the threshold necessary for a derivative to pay off. In the world of election forecasting, the “price” before election day can only be guessed at based on polling data, so there is uncertainty both around how voters’ opinions will change and also what those opinions are at any given time. Figuring out the latter is what Silver’s models excel at, and this complication makes the whole framework of derivative-pricing arguably inapplicable here, which is no great loss since there was no need for it in the first place.A “black swan”-like critique of the FiveThirtyEight forecasts is likely possible and even valuable. It may be that investors like Taleb, who think Silver’s probabilities are wrong, could earn a profit on average by betting on them—like taking advantage of a casino offering the wrong odds—but they could not claim those profits were risk-free. But by accusing Silver of arbitrage and trying to apply his financial expertise to elections, Taleb has overplayed his hand this time and is left looking, well, klueless.Aubrey Clayton is a mathematician living in Boston.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: This astrophysicist priced the planet Kepler-186f at $655. "
  },
  {
    "imageUrl": "http://static.nautil.us/15928_c4c991c9060046f09535273ef0f126fd.png",
    "title": "First Black-Hole Image: It’s Not Looks That Count",
    "description": "Posted by Sabine Hossenfelder on April 11, 2019  The Italian 14th-century painter, Giotto di Bondone, when asked by the Pope to prove his talent, is said to have swung his arm and drawn a perfect circle. But geometric perfection is limited…",
    "category": "Matter",
    "content": "The Italian 14th-century painter, Giotto di Bondone, when asked by the Pope to prove his talent, is said to have swung his arm and drawn a perfect circle. But geometric perfection is limited by the medium. Inspect a canvas closely enough, and every circle will eventually appear grainy. If perfection is what you seek, don’t look at man-made art, look at the sky. More precisely, look at a black hole. Looking at a black hole is what the Event Horizon Telescope has done for the past 12 years. Yesterday, the collaboration released the long-awaited results from its first full run in April 2017. Contrary to expectation, their inaugural image is not, as many expected, Sagittarius A*, the black hole at the center of the Milky Way. Instead, it is the supermassive black hole in the elliptic galaxy Messier 87, about 55 million light-years from here. This black hole weighs in at 6.5 billion times the mass of our sun, and is considerably larger than the black hole in our own galaxy. So, even though the Messier-87 black hole is a thousand times farther away than Sagittarius A*, it still appears half the size in the sky.The Event Horizon Telescope (EHT) is not less remarkable than the objects it observes. With a collaboration of 200 people, the EHT uses not a single telescope, but a global network of nine telescopes. Its sites, from Greenland to the South Pole and from Hawaii to the French Alps, act in concert as one. Together, the collaboration commands a telescope the size of planet Earth, staring at a tiny patch in the northern sky that contains the Messier-87 black hole.Black holes bend light so much that it can wrap around the horizon multiple times. The resulting image is too complicated to capture in simple equations.In theory, black holes are regions of space where the gravitational pull is so large that everything, including light, becomes trapped for eternity. The surface of the trapping region is called the “event horizon.” It has no substance; it is a property of space itself. In the simplest case, the event horizon is a sphere—a perfect sphere, made of nothing. In reality, it’s complicated. Astrophysicists have had evidence for the existence of black holes since the 1990s, but so far all observations have been indirect—inferred from the motion of visible stars and gas, leaving doubt as to whether the dark object really possesses the defining event horizon. It turned out difficult to actually see a black hole. Trouble is, they’re black. They trap light. And while Stephen Hawking proved that black holes must emit radiation due to quantum effects, this quantum glow is far too feeble to observe. But much like the prisoners in Plato’s cave, we can see black holes by observing the shadows they cast. Black holes attract gas from their environment. This gas collects in a spinning disk, and heats up as it spirals into the event horizon, pushing around electric charges. This gives rise to strong magnetic fields that can create a “jet,” a narrow, directed stream of particles leaving the black hole at almost the speed of light. But whatever strays too close to the event horizon falls in and vanishes without a trace.At the same time black holes bend rays of light, bend them so strongly, indeed, that looking at the front of a black hole, we can see part of the disk behind it. The light that just about manages to escape reveals what happens nearby the horizon. It is an asymmetric image that the astrophysicists expect, brighter on the side of the black hole where the material surrounding it moves toward us, and darker where it moves away from us. The hot gas combined with the gravitational lensing creates the unique observable signature that the EHT looks out for.The experimental challenge is formidable. The network’s telescopes must synchronize their data-taking using atomic clocks. Weather conditions must be favorable at all locations simultaneously. Once recorded, the amount of data is so staggeringly large, it must be shipped on hard disk to a central location for processing. The theoretical challenges are not any lesser. Black holes bend light so much that it can wrap around the horizon multiple times. The resulting image is too complicated to capture in simple equations. Though the math had been known since the 1920s, it wasn’t until 1978 that physicists got a first glimpse of what a black hole would actually look like. In that year, the French astrophysicist Jean-Pierre Luminet programmed the calculation on an IBM 7040 using punchcards. He drew the image by hand. Today, astrophysicists use computers many times more powerful to predict the accretion of gas onto the black hole and how the light bends before reaching us. Still, the partly turbulent motion of the gas, the electric and magnetic fields created by it, and the intricacies of the particle’s interactions are not fully understood.The EHT’s observations agree with expectation. But this result is more than just another triumph of Einstein’s theory of general relativity. It is also a triumph of the astronomers’ resourcefulness. They joined hands and brains to achieve what they could not have done separately. And while their measurement settles a long-standing question—yes, black holes really do have event horizons!—it is also the start of further exploration. Physicists hope that the observations will help them understand better the extreme conditions in the accretion disk, the role of magnetic fields in jet formation, and the way supermassive black holes affect galaxy formation. When the Pope received Giotto’s circle, it was not the image itself that impressed him. It was the courtier’s report that the artist produced it without the aid of a compass. This first image of a black hole, too, is remarkable not so much for its appearance, but for its origin. A black sphere, spanning 40 billion kilometers, drawn on a background of hot gas by the greatest artist of all: Nature herself.Sabine Hossenfelder is a research fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why black holes fascinate the public. "
  },
  {
    "imageUrl": "http://static.nautil.us/15854_80418728f131ff0a2b48614529fbf9f3.jpg",
    "title": "Fear Is Good for the Forest",
    "description": "Posted by Brian  Gallagher on March 20, 2019  In 2011, the renowned evolutionary biologist E.O. Wilson paid a visit to Gorongosa National Park, in Mozambique. It is one of the few places in the world where you can get a feel for the…",
    "category": "Biology",
    "content": "In 2011, the renowned evolutionary biologist E.O. Wilson paid a visit to Gorongosa National Park, in Mozambique. It is one of the few places in the world where you can get a feel for the Great African Rift Valley, humanity’s evolutionary home. After a couple hundred thousand years, the area is largely as it was—mostly the same flora and fauna, sounds and smells. Which is why Wilson titled his book on his time there, A Window on Eternity: A Biologist’s Walk Through Gorongosa National Park. It is a hopeful, triumphant tale: the story of how the area’s natural beauty and mammals were ravaged during Mozambique’s multi-decade civil war and came roaring back to life on the brink of permanent destruction. The revival of the area might serve as a model for future conservation. “Here, then, in one of the remotest parts of Africa,” Wilson wrote, “a great environmental tragedy has been averted just in time.”During the civil war, the hunger of soldiers stationed in Gorongosa led to the extirpation of the region’s leopards, wild dogs, hyenas, and to a lesser extent, lions. “That’s a tragic thing,” said Robert Pringle, an evolutionary ecologist at Princeton University, in a Princeton news release last week. “But what it does is enable us to study how behavior and ecology changes when the predators are removed.” Pringle co-authored a study on Gorongosa, published this month in Science, that showed that, without any predators, the forest’s herbivorous animals, notably bushbucks, had no one to fear, which allowed them to feast on vegetation with impunity, decimating the landscape. “The elimination of predators broke the rules that ordinarily govern where herbivores go and what they eat, and that has effects all the way through the food chain,” Pringle said. In other words, the researchers write, the absence of predators transformed what were once “landscapes of fear” into “landscapes of fearlessness.”The study shows the reintroduction of predators alone can spark an ecosystem’s revival. “The first pack of African wild dogs introduced are well settled in, hunt very successfully, and mostly prey on bushbuck—especially those on the open floodplain,” said Paola Bouley, associate director of conservation at Gorongosa National Park, in the Princeton news release. “They just hammer the bushbuck drifting out in the open.” The research, Bouley added, is “exciting confirmation we are on the right track in Gorongosa, focusing strongly on top predator recovery to bring an entire ecosystem back in to balance.” As the researchers concluded, “whereas anthropogenic predator extinction disrupted a trophic cascade by enabling rapid differentiation of prey behavior, carnivore restoration may just as rapidly reestablish that cascade.”The efforts to restore the park are funded by the Greg Carr Foundation—Carr is an entrepreneur and philanthropist who was inspired by the nature writings of E.O. Wilson. When Wilson went to Gorongosa for a second time, he spent much of his time with Carr. “Greg and I, bound by a common philosophy and broad overlap in the science of wildlife conservation, were by then close collaborators and fast friends,” Wilson wrote in A Window on Eternity.Not long ago, Nautilus sat down with Carr to discuss his love of Gorongosa. “There’s nothing more rewarding and wonderful than watching a national park, in this case Gorongosa, recover and feeling, wow, you know what, it doesn’t just have to be downhill,” Carr told Nautilus’ John Steele. “We’re not just watching the inevitable destruction of life on Earth. No. We can take a damaged, distressed ecosystem and bring it back to health. That’s an exciting feeling and that gives you a sense of, wow, humans, we could be okay on this planet for thousands of years if we understand what it means to live sustainably.”Watch our whole interview with Carr here.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15894_0388946ad3ee4d2a9a862fd416fb2589.jpg",
    "title": " Make Concrete Roman Again!",
    "description": "Posted by Brian  Gallagher on April 03, 2019  The Roman naturalist Pliny the Elder can be charmingly self-deprecating. He attempted, in the 1st century A.D., to curate all ancient knowledge in his Natural History, yet he described…",
    "category": "Matter",
    "content": "The Roman naturalist Pliny the Elder can be charmingly self-deprecating. He attempted, in the 1st century A.D., to curate all ancient knowledge in his Natural History, yet he described its 37 volumes to his close friend’s son, Titus, the Emperor of Rome, as having “such inferior importance.” To Pliny, they did “not admit of the display of genius,” nor “of anything particularly pleasant in the narration, or agreeable to the reader.” But he reminded Titus of his basic challenge—that it was, “indeed, no easy task to give novelty to what is old, and authority to what is new.” He might be pleased, then, to see an ancient technology that he himself admired taking on a modern relevance and novelty. Who, Pliny wondered, could help but be surprised at the way the Roman harbor infrastructure, made from a mix of lime and volcanic ash, overcomes entropy, “forming a barrier against the waves of the sea, becoming changed into stone the moment of its immersion, and increasing in hardness from day to day—more particularly when mixed with the cement of Cumae,” an ancient city in Naples?The geologist and geophysicist Marie Jackson, of the University of Utah in Salt Lake City, shares Pliny’s admiration for these still-standing structures, and has scoured ancient Roman texts in hopes of uncovering the recipe that makes them so durable. The goal would be to revive Roman concrete, and their building techniques, for the modern age—but, alas, her efforts were to no avail. We have a lot to relearn from the ancients.So she’s been working backwards. In “Unlocking the secrets of Al-tobermorite in Roman seawater concrete,” a study she and her colleagues published in 2013, they confirmed, via trace element analyses at the Advanced Light Source, an X-ray synchrotron at the Lawrence Berkeley National Laboratory, in California, that drill cores of a 2000-year-old block submerged in the Bay of Pozzuoli near Naples came “from Flegrean Fields volcanic district, as described in ancient Roman texts.” The samples contained aluminum tobermorite, a rare mineral and not an ingredient in conventional concrete, which accounted for their great durability and strength. In a 2017 study, the researchers found that the aluminum tobermorite grew out of a silicate mineral common to volcanic ash, called phillipsite, spurred by ocean contact. “We’re looking at a system that thrives in open chemical exchange with seawater,” Jackson said.The tobermorite’s long plate-like crystals grant the concrete an unusual flexibility under stress that increases with time submerged. “It’s a very rare occurrence in nature,” Jackson said. By contrast, modern concrete, made from a mix of Portland cement and coarse aggregate, corrodes in seawater within decades, making the application of Roman concrete, Jackson noted, an enticing option for steel reinforcement-free seawalls that guard against rising sea levels. Nele De Belie, a materials engineer at Ghent University in Belgium, agrees, and sees fly ash—a product of coal-burning similar to volcanic ash but much more abundant—as the key to inventing new forms of concrete with the sort of “self-healing” properties Roman concrete possesses. Investors are taking notice. In a blog post, Trebuchet Capital Partners, a private equity real estate firm in Los Angeles, states that Portland cement, the world’s standard, is popular because it can be “poured on site” and it’s “cheap to produce.” But architectural projects using fly ash-based concrete will be more sustainable—environmentally, structurally, and economically. The E.P.A. concurs. Using fly ash, the agency’s website states, “can produce positive environmental, economic and performance benefits such as reduced virgin resources, lower greenhouse gas emissions, reduced cost of coal ash disposal, and improved strength and durability of materials.” (It’s not clear if it’s specifically Jackson’s research that’s informing these views.)“We are entering a new gilded age.”Since Portland cement production accounts for over 5 percent of human greenhouse gas emissions, in the future, governments might impose a carbon tax on its use. Trebuchet argues that that burden “may be a game changer” for future fly ash use, especially if developers start relying more on automation and 3D printing in concrete manufacturing. Fly ash naturally lends itself better to a manufacturing setting than Portland cement. Developers, Trebuchet predicts, will be able “to make spectacularly detailed molds to form precast friezes and architectural elements…cheaper, faster, [and] with better working conditions.”What this means, at least for investors interested in building and architecture, is that “we are entering a new gilded age.” People, Trebuchet argues, will stop having “to put up with cookie cutter bland disposable architecture,” and there may even be “a return on [investment for] more superfluous and pretty structures in the neo-classical or Beaux Arts style,” inspired by buildings like the Pantheon. That’s an outcome that might appeal to the Pantheon-lover John Oschsendorf, a professor of engineering at the Massachusetts Institute of Technology and a MacArthur “Genius” Fellow who, like Jackson, thinks we have a lot to relearn from the ancients. He told us, “I think the Pantheon is probably the greatest building of all time.” It’s not mere nostalgia. “I’m in awe,” he said, it’s “like a great work of Mozart.” Much of that appreciation stems not just from its aesthetic, but from the fact that we still don’t understand how the Romans did it. That mystery goes beyond construction materials, and into design.“What we can find are relationships—geometrical relationships—that are encoded within the buildings, which show some of the rules that the builders had,” Oschsendorf told us. “[M]any Roman domes have a supporting wall which is about one-seventh of the span of the dome, and that meant that the builders…were able to devise ratios and proportions that were stable” and “largely independent of scale. Unfortunately, in the Industrial Revolution, a lot of this knowledge was lost. In particular, we don’t often build arches or domes today. It’s very common if you tried to build a structure in [concrete] or brick today that the building code and the engineers involved will insist that it requires steel.” This highlights the knowledge gap in modern civil engineering, Oschsendorf says. “We’ve lost centuries of knowledge, which has been replaced by other knowledge about how to build in steel and concrete. But today’s knowledge doesn’t necessarily map easily onto those older structures. And if we try to make them conform to our theories, it’s very easy to say that these older structures don’t work. It’s a curious concept for an engineer to come along to a building that’s been standing for 500 years and to say this building is not safe.” The tendency derives in part from a “progress ideology of engineering,” he says, “Something old doesn’t have value to us as a profession. Something new, something hi-tech has value.”Pliny’s challenge, in composing his Natural History, was giving “novelty to what is old, and authority to what is new.” Modern engineering’s is to see that what is old has some authority, too.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in July 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15876_d8dcb82c57e569275fc708b8419f1fc1.jpg",
    "title": "Glaciers May Have Covered the Entire Planet—Twice",
    "description": "Posted by Laura Poppick on March 26, 2019  This story was originally published by Knowable Magazine.The Earth has endured many changes in its 4.5-billion-year history, with some tumultuous twists and turns along the way. One especially…",
    "category": "Matter",
    "content": "This story was originally published by Knowable Magazine.The Earth has endured many changes in its 4.5-billion-year history, with some tumultuous twists and turns along the way. One especially dramatic episode appears to have come between 700 million and 600 million years ago, when scientists think ice smothered the entire planet, from the poles to the equator—twice in quick succession. Drawing on evidence across multiple continents, scientists say these Snowball Earth events may have paved the way for the Cambrian explosion of life that followed—the period when complex, multicellular organisms began to diversify and spread across the planet. When Caltech geologist Joe Kirschvink coined the term Snowball Earth in 1989—merging ideas that some geologists, climate physicists and planetary chemists had been thinking about for decades—many earth scientists were skeptical that these cataclysmic events could really have occurred. But with mounting evidence in support of the theory and new data that help pin down the timing of events, more scientists have warmed up to the idea. Paul Hoffman, a geologist at the University of Victoria in British Columbia, has helped pioneer Snowball Earth research over the past 25 years. Among other things, he amassed 50 months’ worth of fieldwork in Namibia, where he gathered evidence of ancient glacial activity in rocks that are interspersed with limestone. Since limestone tends to form in the warmest parts of the ocean, this sandwich-like pattern supports the idea that glaciers covered all of the Earth, cold as well as warm spots, during Snowball Earth episodes. Knowable spoke with Hoffman, who recounts his life work in the Annual Review of Earth and Planetary Sciences, about the evolution of the Snowball Earth theory and what questions remain. This conversation has been edited for length and clarity.What did the planet look like during Snowball Earth?The name describes its appearance from outer space—a glistening white ball. The ice surface is mostly coated with frost and tiny ice crystals that settled out of the cold dry air, which is far below freezing everywhere. Gale-force winds howl in low latitudes. Beneath the floating ice shelf, a dark and briny ocean is continually stirred by tides and turbulent eddies generated by geothermal heat slowly entering from the ocean floor.What tipped off geologists to the possibility of a Snowball Earth?Geologists were struggling to understand what they saw in the geologic record—that not too long before the first appearance of complex life, there was unmistakable evidence of glaciation even in the warmest areas of the Earth. Geologists had a very difficult time understanding how this was possible.A majority of geologists working on the problem now accept the Snowball hypothesis.The deposits that glaciers leave behind are very distinctive. They look like cement that has been dumped out of a cement truck. These Snowball ice sheets would have flowed from the continents out onto the ocean, so we have a lot of deposits that formed in the marine environment where you get what are known as dropstones: pebbles or boulders that are out of place. Very often, you see structures related to the impact, as if the stone was somehow dropped and then plunked into the underlying sediment. It’s difficult to imagine what, other than floating ice, could have possibly transported this debris; trees, which can carry soil and stones out to sea in their roots, had not yet evolved.How did you start studying Snowball Earth?I had known about the hypothesis since even before I was interested in working on the problem myself. Joe Kirschvink at Caltech told me about it a few months after he had the idea in 1989, but he never did anything more with it at that time. I liked it because I like ideas, but there was a credibility gap, so before our work, the hypothesis was dormant. The biggest problem was that because the conditions were so different from any other time in Earth’s history, we didn’t understand the implications of the hypothesis well enough to know whether any given bit of geologic evidence was either for or against it. We had to have climate models to see what actually happens under Snowball conditions, and that modeling, developed later, has been extremely important.My main contribution was making the case that it was a credible scientific hypothesis by arguing, from different disciplines within geoscience, that there was a lot of geological evidence consistent with the predictions. As I often like to say, new ideas or hypotheses are like small children: It’s best not to judge them too early because you don’t know what they are going to be like as adults. Very often, the problem with new ideas is not that they are wrong, but that they are incomplete.What triggered the runaway growth of ice on Earth?That’s the “why” question and that’s maybe the most difficult one. I don’t think there is a consensus on this. There are a number of factors that contributed, and it is useful to look at this in two ways. First of all, what was the general condition that made for a colder climate and therefore made the Earth more susceptible to this runaway ice growth phenomenon? And then what was the immediate trigger that tipped it over the edge?When the Snowball events occurred, the supercontinent Rodinia was in the process of breaking up. A supercontinent is a state in which all of the continents are clustered together in one group. The reason why people think there is a connection there is that the breakup of a supercontinent would increase rainfall in the continental areas, and that would increase the weathering of crustal rocks. The weathering of rocks actually consumes carbon dioxide, so that would lead to less carbon dioxide in the atmosphere and therefore a colder climate.As for what actually caused the immediate trigger, attention has focused in recent years on a sequence of very large volcanic eruptions that occurred in what is now the high arctic of Canada. These eruptions occurred around 717 million and 719 million years ago. When you get fire fountains—lava that comes out of one place over a period of weeks or months—you get a strong thermal upwelling in the atmosphere from the heating effect of that lava. These upwellings can loft sulfur aerosols into the stratosphere where they hang around for a significant amount of time. These sulfur gas particles reflect incoming solar radiation and have a strong cooling effect. Because of the coincidence in timing between these eruptions and the onset of the first and longer of the two Snowball Earths, it’s been postulated that that may have been the immediate trigger.What did life on Snowball Earth look like, and how did it change as a consequence of runaway ice growth?There were certainly bacteria and there were also algae and unicellular primitive animals, or protists. There is also evidence that the first multicellular animals originated at this time, probably something like sponges. Why is a matter of speculation: There are a number of ideas on this, but they are difficult to test. One idea is that on Snowball Earth, ecosystems may have been more isolated from one another and this might be a situation that would be helpful for evolving new forms of life, and particularly forms of life that are altruistic—ones with cells that find that there is an advantage in working together rather than working individually. So more isolation of different ecosystems might have allowed certain ecosystems that had a higher proportion of these multicellular altruists to establish a foothold.Very often, the problem with new ideas is not that they are wrong, but that they are incomplete.How was the Snowball theory received by other geologists?I underestimated how emotional people would get about it and how wedded people were to the idea that the Earth has never really been greatly different than it is today. In the 19th century, people had a difficult time believing that most of northern Europe and North America were covered by an ice sheet only 20,000 years ago. That was as hard for a 19th-century geologist to accept as Snowball Earth has been for 20th-century geologists. For a long time we had a lot of evidence for glaciation at low latitude and in the warmest parts of the Earth, but we didn’t really have a good idea of the dates of these events. It was sort of embarrassing. But between 2010 and 2014 that situation dramatically changed. We now have pretty precise estimates from two very different dating techniques, and it’s impressive that they are giving highly consistent results. Working out the timescale has caused a majority of geologists working on the problem to now accept the Snowball hypothesis.Alternative theories have arisen over the years, including what is called the Slushball theory—a less extreme version of Snowball Earth. How does pinning down the dates help sort out these alternative theories?In the Slushball scenario, carbon dioxide would start building up very quickly, so the glaciation would be short-lived and the ice would retreat gradually. This is not what we see in the geologic record. We now know that the first Snowball lasted for 58 million years and that is completely inconsistent with the Slushball idea. Also, we see the Snowball glaciations terminate extremely abruptly and they are followed by clear evidence of a complete and abrupt climate reversal, a very hot period. That is not explained by the Slushball model.I don’t think there are any other alternatives that satisfy the evidence.What other questions about Snowball Earth remain?The dating has created a new set of problems. One thing the dating revealed was that the two Snowball Earths occurred in rapid succession and were very unequal in duration. The first one lasted 58 million years and the second one only lasted 5 million to 15 million years. So we don’t know why there is this great disparity in how long the glaciations lasted. And why was it that there was just this short interval between the two? There’s only about 10 million years when there was no ice at all and then suddenly the planet went back into Snowball Earth. So why two in rapid succession? And why wasn’t there a third one or a fourth one? These are new questions that have arisen as a result of our understanding of the timing.Could Snowball Earth return?I don’t think we are in a very good position to say whether or not it’s likely to happen in the future. The future is a long time. We can say it is not going to happen in the next several tens of thousands of years.Why study Earth history?The history of our planet is one of the greatest stories. Because we live here and we are dependent on this place, it is very important to understand that the Earth has not always been the way it is today. Snowball Earth is an example of the kinds of amazing things that the Earth has been through that we would never have suspected if we didn’t investigate the geologic record. Dealing with Snowball Earth has been fantastic—it’s been the most intense learning experience of my life, and I never anticipated that it would be accepted in my lifetime.And you’re still at it, after 25 years?I’m still doing fieldwork in Namibia, as a 77-year-old. It’s just a large and fascinating problem. It’s hard to pull myself away. Laura Poppick is a freelance science journalist in Portland, Maine—a place that sometimes feels like a large snowball. Prior to her career in journalism, she worked in a laboratory at Princeton University studying Snowball Earth rocks. Twitter: @laurapoppick.\n\tThe newest and most popular articles delivered right to your inbox!\nKnowable Magazine is an independent journalistic endeavor from Annual Reviews. "
  },
  {
    "imageUrl": "http://static.nautil.us/15688_68e4593563e1b425a7717504bca103d6.jpg",
    "title": "The Case for Professors of Stupidity",
    "description": "Posted by Brian  Gallagher on January 30, 2019  On this past International Holocaust Remembrance Day, I reread a bit of Bertrand Russell. In 1933, dismayed at the Nazification of Germany, the philosopher wrote “The Triumph of Stupidity,”…",
    "category": "Ideas",
    "content": "On this past International Holocaust Remembrance Day, I reread a bit of Bertrand Russell. In 1933, dismayed at the Nazification of Germany, the philosopher wrote “The Triumph of Stupidity,” attributing the rise of Adolf Hitler to the organized fervor of stupid and brutal people—two qualities, he noted, that “usually go together.” He went on to make one of his most famous observations, that the “fundamental cause of the trouble is that in the modern world the stupid are cocksure while the intelligent are full of doubt.”Russell’s quip prefigured the scientific discovery of a cognitive bias—the Dunning–Kruger effect—that has been so resonant that it has penetrated popular culture, inspiring, for example, an opera song (from Harvard’s annual Ig Nobel Award Ceremony): “Some people’s own incompetence somehow gives them a stupid sense that anything they do is first rate. They think it’s great.” No surprise, then, that psychologist Joyce Ehrlinger prefaced a 2008 paper she wrote with David Dunning and Justin Kruger, among others, with Russell’s comment—the one he later made in his 1951 book, New Hopes for a Changing World: “One of the painful things about our time is that those who feel certainty are stupid, and those with any imagination and understanding are filled with doubt and indecision.” “By now,” Ehrlinger noted in that paper, “this phenomenon has been demonstrated even for everyday tasks, about which individuals have likely received substantial feedback regarding their level of knowledge and skill.” Humans have shown a tendency, in other words, to be a bit thick about even the most mundane things, like how well they drive.Stupidity is not simply the opposite of intelligence.Russell, who died in 1970 at 97 years of age, probably would not be surprised to hear news of this new study, published in Nature Human Behaviour: “Extreme opponents of genetically modified foods know the least but think they know the most.” The researchers, led by Philip Fernbach, cognitive scientist and co-author of The Knowledge Illusion: Why We Never Think Alone, analyzed survey responses from a nationally representative sample of U.S. adults. They obtained similar results, they write, “in a parallel study with representative samples from the United States, France and Germany, and in a study testing attitudes about a medical application of genetic engineering technology (gene therapy).”Fernbach called their result “perverse.” It was nevertheless consistent with prior work exploring the Dunning–Kruger effect and the psychology of extremism, he said. “Extreme views often stem from people feeling they understand complex topics better than they do.” Now as ever, societies need to know how to combat this.But what exactly is stupidity? David Krakauer, the President of the Santa Fe Institute, told interviewer Steve Paulson, for Nautilus, stupidity is not simply the opposite of intelligence. “Stupidity is using a rule where adding more data doesn’t improve your chances of getting [a problem] right,” Krakauer said. “In fact, it makes it more likely you’ll get it wrong.” Intelligence, on the other hand, is using a rule that allows you to solve complex problems with simple, elegant solutions. “Stupidity is a very interesting class of phenomena in human history, and it has to do with rule systems that have made it harder for us to arrive at the truth,” he said. “It’s an interesting fact that, whilst there are numerous individuals who study intelligence—there are whole departments that are interested in it—if you were to ask yourself what’s the greatest problem facing the world today, I would say it would be stupidity. So we should have professors of stupidity—it would just be embarrassing to be called the stupid professor.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher. "
  },
  {
    "imageUrl": "http://static.nautil.us/15841_e6496f5f04bc759fa7a33e69f604ef17.jpg",
    "title": " Forget Everything You Think You Know About Time",
    "description": "Posted by Brian  Gallagher on March 12, 2019  Last April, in the famous Faraday Theatre at the Royal Institution in London, Carlo Rovelli gave an hour-long lecture on the nature of time. A red thread spanned the stage, a metaphor for…",
    "category": "Ideas",
    "content": "Last April, in the famous Faraday Theatre at the Royal Institution in London, Carlo Rovelli gave an hour-long lecture on the nature of time. A red thread spanned the stage, a metaphor for the Italian theoretical physicist’s subject. “Time is a long line,” he said. To the left lies the past—the dinosaurs, the big bang—and to the right, the future—the unknown. “We’re sort of here,” he said, hanging a carabiner on it, as a marker for the present.Then he flipped the script. “I’m going to tell you that time is not like that,” he explained.Rovelli went on to challenge our common-sense notion of time, starting with the idea that it ticks everywhere at a uniform rate. In fact, clocks tick slower when they are in a stronger gravitational field. When you move nearby clocks showing the same time into different fields—one in space, the other on Earth, say—and then bring them back together again, they will show different times. “It’s a fact,” Rovelli said, and it means “your head is older than your feet.” Also a non-starter is any shared sense of “now.” We don’t really share the present moment with anyone. “If I look at you, I see you now—well, but not really, because light takes time to come from you to me,” he said. “So I see you sort of a little bit in the past.” As a result, “now” means nothing beyond the temporal bubble “in which we can disregard the time it takes light to go back and forth.”Rovelli turned next to the idea that time flows in only one direction, from past to future. Unlike general relativity, quantum mechanics, and particle physics, thermodynamics embeds a direction of time. Its second law states that the total entropy, or disorder, in an isolated system never decreases over time. Yet this doesn’t mean that our conventional notion of time is on any firmer grounding, Rovelli said. Entropy, or disorder, is subjective: “Order is in the eye of the person who looks.” In other words the distinction between past and future, the growth of entropy over time, depends on a macroscopic effect—“the way we have described the system, which in turn depends on how we interact with the system,” he said.“A million years of your life would be neither past nor future for me. So the present is not thin; it’s horrendously thick.”Getting to the last common notion of time, Rovelli became a little more cautious. His scientific argument that time is discrete—that it is not seamless, but has quanta—is less solid. “Why? Because I’m still doing it! It’s not yet in the textbook.” The equations for quantum gravity he’s written down suggest three things, he said, about what “clocks measure.” First, there’s a minimal amount of time—its units are not infinitely small. Second, since a clock, like every object, is quantum, it can be in a superposition of time readings. “You cannot say between this event and this event is a certain amount of time, because, as always in quantum mechanics, there could be a probability distribution of time passing.” Which means that, third, in quantum gravity, you can have “a local notion of a sequence of events, which is a minimal notion of time, and that’s the only thing that remains,” Rovelli said. Events aren’t ordered in a line “but are confused and connected” to each other without “a preferred time variable—anything can work as a variable.”Even the notion that the present is fleeting doesn’t hold up to scrutiny. It is certainly true that the present is “horrendously short” in classical, Newtonian physics. “But that’s not the way the world is designed,” Rovelli explained. Light traces a cone, or consecutively larger circles, in four-dimensional spacetime like ripples on a pond that grow larger as they travel. No information can cross the bounds of the light cone because that would require information to travel faster than the speed of light.“In spacetime, the past is whatever is inside our past light-cone,” Rovelli said, gesturing with his hands the shape of an upside down cone. “So it’s whatever can affect us. The future is this opposite thing,” he went on, now gesturing an upright cone. “So in between the past and the future, there isn’t just a single line—there’s a huge amount of time.” Rovelli asked an audience member to imagine that he lived in Andromeda, which is two and a half million light years away. “A million years of your life would be neither past nor future for me. So the present is not thin; it’s horrendously thick.”Listening to Rovelli’s description, I was reminded of a phrase from his new book, The Order of Time: Studying time “is like holding a snowflake in your hands: gradually, as you study it, it melts between your fingers and vanishes.” Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why the nature of time is such a central issue for theoretical physics.This classic Facts So Romantic post was originally published in August 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15746_6840f4a1c1d164848d46033274dfe8b4.jpg",
    "title": "When Gravity Breaks Down",
    "description": "Posted by Sabine Hossenfelder on February 15, 2019  Albert Einstein’s theory of general relativity is more than a hundred years old, but still it gives physicists headaches. Not only are Einstein’s equations hideously difficult to solve,…",
    "category": "Numbers",
    "content": "Albert Einstein’s theory of general relativity is more than a hundred years old, but still it gives physicists headaches. Not only are Einstein’s equations hideously difficult to solve, they also clash with physicists’ other most-cherished achievement, quantum theory.Problem is, particles have quantum properties. They can, for example, be in two places at once. These particles also have masses, and masses cause gravity. But since gravity does not have quantum properties, no one really knows what’s the gravitational pull of a particle in a quantum superposition. To solve this problem, physicists need a theory of quantum gravity. Or, since Einstein taught us that gravity is really curvature of space-time, physicists need a theory for the quantum properties of space and time.It’s a hard problem, even for big-brained people like theoretical physicists. They have known since the 1930s that quantum gravity is necessary to bring order into the laws of nature, but 80 years on, a solution isn’t anywhere in sight. The major obstacle on the way to progress is the lack of experimental guidance. The effects of quantum gravity are extremely weak and have never been measured, so physicists have only math to rely on. And it’s easy to get lost in math.The reason it is difficult to obtain observational evidence for quantum gravity is that all presently possible experiments fall into two categories. Either we measure quantum effects—using small and light objects—or we measure gravitational effects—using large and heavy objects. In both cases, quantum gravitational effects are tiny. To see the effects of quantum gravity, you would really need a heavy object that has pronounced quantum properties, and that’s hard to come by.Physicists do know a few naturally occurring situations where quantum gravity should be relevant. But it is not on short distances, though I often hear that. Non-quantized gravity really fails in situations where energy-densities become large and space-time curvature becomes strong. And let me be clear that what astrophysicists consider “strong” curvature is still “weak” curvature for those working on quantum gravity. In particular, the curvature at a black hole horizon is not remotely strong enough to give rise to noticeable quantum gravitational effects.Curvature strong enough to cause general relativity to break down, we believe, exists only in the center of black holes and close by the Big Bang. In both cases the strongly compressed matter has a high density and a pronounced quantum behavior which should give rise to quantum gravitational effects. Unfortunately, we cannot look inside a black hole, and reconstructing what happened at the Big Bang from today’s observations, with present measurement techniques, cannot reveal the quantum gravitational behavior.Curvature strong enough to cause general relativity to break down, we believe, exists only in the center of black holes and close by the Big Bang.The regime where quantum gravity becomes relevant should also be reached in particle collisions at extremely high center-of-mass energy. If you had a collider large enough—estimates say that with current technology it would be about the size of the Milky Way—you could focus enough energy into a small region of space to create strong enough curvature. But we are not going to build such a collider any time soon.Besides strong space-time curvature, there is another case where quantum effects of gravity should become measurable that is often neglected: By creating quantum superpositions of heavy objects. This causes the approximation in which matter has quantum properties but gravity doesn’t (the “semi-classical limit”) to break down, revealing truly quantum effects of gravity. A few experimental groups are currently trying to reach the regime where they might become sensitive to such effects. They still have some orders of magnitude to go, so not quite there yet.Why don’t physicist study this case closer? As always, it’s hard to say why scientists do one thing and not another. I can only guess it’s because, from a theoretical perspective, this case is not all that interesting.I know I said that physicists don’t have a theory of quantum gravity, but that is only partly correct. Gravity can, and has been, quantized using the normal methods of quantization already in the 1960s by Richard Feynman and Bryce DeWitt. However, the theory one obtains this way (“perturbative quantum gravity”), breaks down in exactly the strong curvature regime that physicists want to use it (“perturbatively non-renormalizable”). Therefore, this approach is today considered merely a low-energy approximation (“effective theory”) to the yet-to-be-found full theory of quantum gravity (“UV-completion”).Past the 1960s, almost all research efforts in quantum gravity focused on developing that full theory. The best known approaches are string theory, loop quantum gravity, asymptotic safety, and causal dynamical triangulation. The above mentioned case with heavy objects in quantum superpositions, however, does not induce strong curvature and hence falls into the realm of the boring and supposedly well-understood theory from the 1960s. Ironically, for this reason there are almost no theoretical predictions for such an experiment from either of the major approaches to the full theory of quantum gravity.Indeed, the quantization procedure for Yang-Mills theories is a logical nightmare.Most people in the field presently think that perturbative quantum gravity must be the correct low-energy limit of any theory of quantum gravity. A minority, however, holds that this isn’t so, and members of this club usually quote one or both of the following reasons.The first objection is philosophical. It does not conceptually make much sense to derive a supposedly more fundamental theory (quantum gravity) from a less fundamental one (non-quantum gravity) because by definition the derived theory is the less fundamental one. Indeed, the quantization procedure for Yang-Mills theories is a logical nightmare. You start with a non-quantum theory, make it more complicated to obtain another theory, though that is not strictly speaking a derivation, and if you then take the classical limit you get a theory that doesn’t have any good interpretation whatsoever. So why did you start from it to begin with it?Well, the obvious answer is: We do it because it works, and we do it this way because of historical accident not because it makes a lot of sense. Nothing wrong with that for a pragmatist like me, but also not a compelling reason to insist that the same method should apply to gravity.The second often-named argument against the perturbative quantization is that you do not get atomic physics by quantizing water, either. So if you think that gravity is not a fundamental interaction but comes from the collective behavior of a large number of microscopic constituents (think “atoms of space-time”), then quantizing general relativity is simply the wrong thing to do.Those who take this point of view, that gravity is really a bulk-theory for some unknown microscopic constituents, follow an approach called “emergent gravity.” It is supported by the (independent) observations of Ted Jacobson, Thanu Padmanabhan, and Erik Verlinde, that the laws of gravity can be rewritten so that they appear like thermodynamical laws. My opinion about this flip-flops between “most amazing insight ever” and “curious aside of little relevance,” sometimes several times a day.Be that as it may, if you think that emergent gravity is the right approach to quantum gravity, then the question where gravity-as-we-know-and-like-it breaks down becomes complicated. It should still break down at high curvature, but there may be further situations where you could see departures from general relativity.Verlinde, for example, interprets dark matter and dark energy as relics of quantum gravity. If you believe this, we do already have evidence for quantum gravity! Others have suggested that if space-time is made of microscopic constituents, then it may have bulk-properties like viscosity, or result in effects normally associated with crystals like birefringence, or the dispersion of light. In summary, the expectation that quantum effects of gravity should become relevant for strong space-time curvature is based on an uncontroversial extrapolation and pretty much everyone in the field agrees on it.* In certain approaches to quantum gravity, deviations from general relativity could also become relevant at long distances, low acceleration, or low energies. An often neglected possibility is to probe the effects of quantum gravity with quantum superpositions of heavy objects.I hope to see experimental evidence for quantum gravity in my lifetime.Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.* Except me, sometimes.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: We can unify the great theories of physics.This post was originally published on BackRe(Action), Hossenfelder’s blog, and is reprinted with permission. "
  },
  {
    "imageUrl": "http://static.nautil.us/15774_a3fc55e5fcbb7efeebd787168151bb2d.jpg",
    "title": "How Our Universe Could Emerge as a Hologram",
    "description": "Posted by Natalie Wolchover on February 26, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.The fabric of space and time is widely believed by physicists to be emergent, stitched out of quantum threads according…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.The fabric of space and time is widely believed by physicists to be emergent, stitched out of quantum threads according to an unknown pattern. And for 22 years, they’ve had a toy model of how emergent space-time can work: a theoretical “universe in a bottle,” as its discoverer, Juan Maldacena, has described it.The space-time filling the region inside the bottle—a continuum that bends and undulates, producing the force called gravity—exactly maps to a network of quantum particles living on the bottle’s rigid, gravity-free surface. The interior “universe” projects from the lower-dimensional boundary system like a hologram. Maldacena’s discovery of this hologram has given physicists a working example of a quantum theory of gravity.But that doesn’t necessarily mean the toy universe shows how space-time and gravity emerge in our universe. The bottle’s interior is a dynamic, Escheresque place called anti–de Sitter (AdS) space that is negatively curved like a saddle. Different directions on the saddle curve in opposite ways, with one direction curving up and the other curving down. The curves tend toward vertical as you move away from the center, ultimately giving AdS space its outer boundary—a surface where quantum particles can interact to create the holographic universe inside. However, in reality, we inhabit a positively curved “de Sitter (dS) space,” which resembles the surface of a sphere that’s expanding without bounds.Ever since 1997, when Maldacena discovered the AdS/CFT correspondence — a duality between AdS space and a “conformal field theory” describing quantum interactions on that space’s boundary—physicists have sought an analogous description of space-time regions like ours that aren’t bottled up. The only “boundary” of our universe is the infinite future. But the conceptual difficulty of projecting a hologram from quantum particles living in the infinite future has long stymied efforts to describe real space-time holographically.“I believe that de Sitter holography also works as a quantum error-correcting code, and I would very much like to understand how.”In the last year, though, three physicists have made progress toward a hologram of de Sitter space. Like the AdS/CFT correspondence, theirs is also a toy model, but some of the principles of its construction may extend to more realistic space-time holograms. There is “tantalizing evidence,” said Xi Dong of the University of California, Santa Barbara, who led the research, that the new model is a piece of “a unified framework for quantum gravity in de Sitter [space].”Dong and co-authors Eva Silverstein of Stanford University and Gonzalo Torroba of the Bariloche Atomic Center in Argentina constructed a hologram of dS space by taking two AdS universes, cutting them, warping them and gluing their boundaries together.The cutting is needed to deal with a problematic infinity: the fact that the boundary of AdS space is infinitely far away from its center. (Picture a ray of light traveling an infinite distance up the saddle’s curve to reach the edge.) Dong and co-authors rendered AdS space finite by chopping off the space-time region at a large radius. This created what’s known as a “Randall-Sundrum throat,” after the physicists Lisa Randall and Raman Sundrum, who devised the trick. This space is still approximated by a CFT that lives on its boundary, but the boundary is now a finite distance away.Next, Dong and co-authors added ingredients from string theory to two of these theoretical Randall-Sundrum throats to energize them and give them positive curvature. This procedure, called “uplifting,” turned the two saddle-shaped AdS spaces into bowl-shaped dS spaces. The physicists could then do the obvious thing: “glue” the two bowls together along their rims. The CFTs describing both hemispheres become coupled with each other, forming a single quantum system that is holographically dual to the entire spherical de Sitter space.“The resulting space-time has no boundary, but by construction it is dual to two CFTs,” Dong said. Because the equator of the de Sitter space, where the two CFTs live, is itself a de Sitter space, the construction is called the “dS/dS correspondence.”Silverstein proposed this basic idea with three co-authors back in 2004, but new theoretical tools have enabled her, Dong and Torroba to study the dS/dS hologram in greater detail and show that it passes important consistency checks. In a paper published last summer, they calculated that the entanglement entropy—a measure of how much information is stored in the coupled CFTs living on the equator—matches the known entropy formula for the corresponding spherical region of de Sitter space.They and other researchers are further exploring the de Sitter hologram using tools from computer science. As I described in a recent Quanta article, physicists have discovered in the last few years that the AdS/CFT correspondence works exactly like a “quantum error-correcting code”—a scheme for securely encoding information in a jittery quantum system, be it a quantum computer or a CFT. Quantum error correction may be how the emergent fabric of space-time achieves its robustness, despite being woven out of fragile quantum particles.Dong, who was part of the team that discovered the connection between AdS/CFT and quantum error correction, said, “I believe that de Sitter holography also works as a quantum error-correcting code, and I would very much like to understand how.” There’s little hope of experimental evidence verifying that this new perspective on de Sitter space-time is correct, but according to Dong, “you instinctively know you are on the right track if the pieces start to fit together.”Patrick Hayden, a theoretical physicist and computer scientist at Stanford who studies the AdS/CFT correspondence and its relationship to quantum error correction, said he and other experts are mulling over Dong, Silverstein and Torroba’s dS/dS model. He said it’s too soon to tell whether insights about how space-time is woven and how quantum gravity works in AdS space will carry over to a de Sitter model. “But there’s a path—something to be done,” Hayden said. “You can formulate concrete mathematical questions. I think a lot is going to happen in the next few years.”Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScience and other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award, the 2016 Evert Clark/Seth Payne Award, and the American Institute of Physics’ 2017 Science Communication Award for Articles.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15679_c9b9f44e513842d70a8cf2bcfb548d80.jpg",
    "title": " Why Doing Good Makes It Easier to Be Bad",
    "description": "Posted by Abbas Panjwani on February 05, 2019  Oscar Wilde, the famed Irish essayist and playwright, had a gift, among other things, for counterintuitive aphorisms. In “The Soul of Man Under Socialism,” an 1891 article, he wrote,…",
    "category": "Culture",
    "content": "Oscar Wilde, the famed Irish essayist and playwright, had a gift, among other things, for counterintuitive aphorisms. In “The Soul of Man Under Socialism,” an 1891 article, he wrote, “Charity creates a multitude of sins.”So perhaps Wilde wouldn’t have been surprised to hear of a series of recent scandals in the U.K.: The all-male charity, the President’s Club, which raised money for causes including children’s hospitals through high-valued auctions, was forced to close after the Financial Times uncovered sexual assault and misogyny at its annual dinner; executives of Oxfam, a poverty eradication charity, visited prostitutes while delivering aid in earthquake-stricken Haiti, and were allowed to slink off to other charities, rather than being castigated for their actions; and ex-Save the Children executives Brendan Cox and Justin Forsyth stepped down from their roles at other charities, after allegations of sexual harassment and bullying toward junior female colleagues resurfaced.You might wonder how people who seem so good by occupation could be so bad in private. The theory of moral licensing could help explain why: When humans are good, it says, we give ourselves license to be bad.In a recent paper, economists at the University of Chicago reported that working for a socially responsible company motivated employees to act immorally. In one experiment, people were hired to transcribe images of short German texts and paid 10 percent upfront, with the remaining payment being delivered if they completed the transcriptions, or if they declared the documents too illegible to transcribe. When they were told that, for every job completed or marked illegible, 5 percent of their wages would be donated to Unicef’s educational programs, the instances of cheating rose by 25 percent, compared to where no charitable donation was offered. Cheating manifested in both workers not completing jobs (taking the 10 percent upfront fee and running) and also workers saying that documents were too illegible to transcribe (and so receiving the full fee).“The share of cheaters [was] highest when we frame corporate social responsibility as a prosocial act on behalf of workers,” the researchers, John A. List and Fatemeh Momeni, found. When the workers felt a greater sense that their own actions would lead to charitable donations, like Robin Hood, they in turn felt enough license to steal, essentially, from their employer to give to charity. “The ‘doing good’ nature of [corporate social responsibility] induces workers to misbehave on another dimension that hurts the firm,” List and Fatemeh concluded.When humans are good, we give ourselves license to be bad.A parallel might be drawn between the transcribers cheating in order to give more money to charity, and the organizers of the male-only dinner hiring hostesses for titillation, in order to increase the appeal of the event and drive up donations. But there are problems using this theory to explain instances of sexual assault. Moral licensing only applies when the bad behavior can be self-rationalized as good—or, at least, ambiguous.In a 2011 study, researchers at the University of Oklahoma asked students to complete mental math tests on a computer, simple arithmetic problems only involving numbers one through 20. They were told that they would be shown a math problem, and needed to press the spacebar to bring up the response box. If they failed to do that quickly enough, the answer would automatically appear, ostensibly due to a bug in the computer program, still in its piloting phase.Students were given either 10 seconds or 1 second to press the spacebar, on the working assumption that, while students who failed to press the spacebar in 10 seconds were deliberately cheating, those who failed to press the spacebar within 1 second, could “rationalize their failure to do so as incidental rather than immoral.” After the test, the students were then asked how many times they had failed to press the spacebar quickly enough, thereby seeing the answer. The prevalence of lying was significantly higher amongst the 1-second group.But generalizing that moral licensing only occurs when bad behavior can be “rationalized” or “prosocial” overlooks the nuance of moral license theory, argues Daniel Effron, of the London Business School, who specializes in organizational ethics. “There are two versions of moral licensing theory,” he says. “One is the ‘moral credentials mechanism,’ which is more to do with rationalization. Basically it states, ‘I’ve done some good stuff. I’ve shown that I’m a good enough person. Now I can act ambiguously, because, as a good person, I know that my behavior is more likely to be good than bad.’ The other is the ‘moral credits’ mechanism, which works like a bank account. You do good stuff, you put a deposit in your bank account. You do bad stuff, you take a withdrawal. In that case, the bad deeds don’t have to be rationalizable.”The latter version explains—though, obviously, does not excuse—the bad actions of Forsyth, Cox, and company. They had built up enough “moral credit” to justify taking some withdrawals, at least in their minds. This isn’t to say that the theory can determine or predict whether a good action will lead to bad behavior, Effron says.He also stresses that the “charity sector isn’t any more vulnerable” to instances of moral licensing than any other sector. Humans are very good, he says, at finding reasons to be bad and making “mountains of morality out of molehills of virtue.” Studies have shown that trivial acts, including buying environmentally friendly cosmetics, can give consumers a moral license to behave badly. But, he adds, “You could make the argument that in the charity sector, you don’t have to work as hard to find your moral license for being bad.”Abbas Panjwani is a journalist at Full Fact, the UK’s leading fact-checking charity. He has previously written for the Sunday Times. Follow him on Twitter @abbas_panjwani.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in March 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15732_ed2ddd0dcd323046d0f9a51e5cc51c60.jpg",
    "title": "New Evidence for the Strange Geometry of Thought",
    "description": "Posted by Adithya Rajagopalan on February 08, 2019  In 2014, the Swedish philosopher and cognitive scientist Peter Gärdenfors went to Krakow, Poland, for a conference on the mind. He was to lecture at Jagiellonian University, courtesy of…",
    "category": "Biology",
    "content": "In 2014, the Swedish philosopher and cognitive scientist Peter Gärdenfors went to Krakow, Poland, for a conference on the mind. He was to lecture at Jagiellonian University, courtesy of the Copernicus Center for Interdisciplinary Studies, on his theory of conceptual, or “cognitive,” spaces. Gärdenfors had been working on his idea of cognitive spaces, which explain how our brains represent concepts and objects, for decades. In his book Conceptual Spaces, from 2000, he wrote, “It has long been a common prejudice in cognitive science that the brain is either a Turing machine working with symbols or a connectionist system using neural networks.” In Krakow, Gärdenfors pushed against that prejudice. In his talk, “The Geometry of Thinking,” he suggested that humans are able to do things that today’s powerful computers can’t do—like learn language quickly and generalize from particulars with ease (to see, in other words, without much training, that lions and tigers are four-legged felines)—because we, unlike our computers, represent information in geometrical space.In a 2018 Science paper, co-authored with Jacob Bellmund, Christian Doeller, and Edvard Moser—neuroscientists from the Max Planck Institute in Leipzig and the Kavli Institute in Trondheim—Gärdenfors, of the University of Lund, buttressed his idea with recent advances in brain science. He argued that the brain represents concepts in the same way that it represents space and your location, by using the same neural circuitry for the brain’s “inner GPS.”“Cognitive spaces are a way of thinking about how our brain might organize our knowledge of the world,” Bellmund said. It’s an approach that concerns not only geographical data, but also relationships between objects and experience. “We were intrigued by evidence from many different groups that suggested that the principles of spatial coding in the hippocampus seem to be relevant beyond the realms of just spatial navigation,” Bellmund said. The hippocampus’ place and grid cells, in other words, map not only physical space but conceptual space. It appears that our representation of objects and concepts is very tightly linked with our representation of space.Gärdenfors’ theory highlights a fruitful path, not only for cognitive scientists, but for neurologists and machine-learning researchers.Work spanning decades has found that regions in the brain—the hippocampus and entorhinal cortex—act like a GPS. Their cells form a grid-like representation of the brain’s surroundings and keep track of its location on it. Specifically, neurons in the entorhinal cortex activate at evenly distributed locations in space: If you drew lines between each location in the environment where these cells activate, you would end up sketching a triangular grid, or a hexagonal lattice. The activity of these aptly named “grid” cells contains information that another kind of cell uses to locate your body in a particular place. The explanation of how these “place” cells work was stunning enough to award scientists John O’Keefe, May-Britt Moser, and Edvard Moser, the 2014 Nobel Prize in Physiology or Medicine. These cells activate only when you are in one particular location in space, or the grid, represented by your grid cells. Meanwhile, head-direction cells define which direction your head is pointing. Yet other cells indicate when you’re at the border of your environment—a wall or cliff. Rodent models have elucidated the nature of the brain’s spatial grids, but, with functional magnetic resonance imaging, they have also been validated in humans.Recent fMRI studies show that cognitive spaces reside in the hippocampal network—supporting the idea that these spaces lie at the heart of much subconscious processing. For example, subjects of a 2016 study—headed by neuroscientists at Oxford—were shown a video of a bird’s neck and legs morph in size. Previously they had learned to associate a particular bird shape with a Christmas symbol, such as Santa or a Gingerbread man. The researchers discovered the subjects made the connections with a “mental picture” that could not be described spatially, on a two-dimensional map. Yet grid-cell responses in the fMRI data resembled what one would see if subjects were imagining themselves walking in a physical environment. This kind of mental processing might also apply to how we think about our family and friends. We might picture them “on the basis of their height, humor, or income, coding them as tall or short, humorous or humorless, or more or less wealthy,” Doeller said. And, depending on whichever of these dimensions matters in the moment, the brain would store one friend mentally closer to, or farther from, another friend.But the usefulness of a cognitive space isn’t just restricted to already familiar object comparisons. “One of the ways these cognitive spaces can benefit our behavior is when we encounter something we have never seen before,” Bellmund said. “Based on the features of the new object we can position it in our cognitive space. We can then use our old knowledge to infer how to behave in this novel situation.” Representing knowledge in this structured way allows us to make sense of how we should behave in new circumstances.Data also suggests that this region may represent information with different levels of abstraction. If you imagine moving through the hippocampus, from the top of the head toward the chin, you will find many different groups of place cells that completely map the entire environment but with different degrees of magnification. Put another way, moving through the hippocampus is like zooming in and out on your phone’s map app. The area in space represented by a single place cell gets larger. Such size differences could be the basis for how humans are able to move between lower and higher levels of abstraction—from “dog” to “pet” to “sentient being,” for example. In this cognitive space, more zoomed-out place cells would represent a relatively broad category consisting of many types, while zoomed-in place cells would be more narrow.Yet the mind is not just capable of conceptual abstraction but also flexibility—it can represent a wide range of concepts. To be able to do this, the regions of the brain involved need to be able to switch between concepts without any informational cross-contamination: It wouldn’t be ideal if our concept for bird, for example, were affected by our concept for car. Rodent studies have shown that when animals move from one environment to another—from a blue-walled cage to a black-walled experiment room, for example—place-cell firing is unrelated between the environments. Researchers looked at where cells were active in one environment and compared it to where they were active in the other. If a cell fired in the corner of the blue cage as well as the black room, there might be some cross-contamination between environments. The researchers didn’t see any such correlation in the place-cell activity. It appears that the hippocampus is able to represent two environments without confounding the two. This property of place cells could be useful for constructing cognitive spaces, where avoiding cross-contamination would be essential. “By connecting all these previous discoveries,” Bellmund said, “we came to the assumption that the brain stores a mental map, regardless of whether we are thinking about a real space or the space between dimensions of our thoughts.”Scientists still need to experimentally verify the link between the hippocampus and higher-order cognitive functions in humans. fMRI studies like the ones from the group in Oxford are, as yet, only suggestive. “Although the coarse nature of the fMRI signal urges caution in making conclusions at the level of neuronal codes,” the researchers concluded, “we have reported an unusually precise hexagonal modulation of the fMRI signal during nonspatial cognition.” It is also unknown whether place cells can actually represent objects at particular locations in a cognitive space. Revealing this in experiments with human subjects is hard, since they require very fine-resolution brain imaging. But recent advances in higher-resolution fMRI could possibly provide a solution.Bellmund pointed out that rodent research could also reveal the existence of cognitive spaces. A 2017 paper, for example, found that place cells in rats can form a map of sound frequencies. Different cells in the hippocampus respond to different frequencies of sound—forming a cognitive space of sound. What’s more, studies in humans that have seen grid-like activity in the hippocampus have also seen this activity in other parts of the cortex. Therefore, it is highly likely that complicated, higher-order cognitive abilities arise from interactions between several parts of the brain.Gärdenfors’ theory highlights a fruitful path, not only for cognitive scientists, but for neurologists and machine-learning researchers. It is a kind of incomplete, generic sketch on a canvas that invites refinement and elaboration. Cognitive spaces are, as Gärdenfors and Bellmund put it, a “domain-general format for human thinking,” an “overarching framework” that can help unravel the causes of neurodegenerative diseases, like Alzheimer’s, and “to inform novel architectures in artificial intelligence.”Adithya Rajagopalan is a second-year graduate student in the department of neuroscience at Johns Hopkins University & Janelia Research Campus. Follow him on Twitter @adi_e_r.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15768_3eebaed369eb3ae36a90f310fc33638c.jpg",
    "title": "The Case for Being Skeptical of Moral Outrage",
    "description": "Posted by Scott Koenig on February 22, 2019  The episode last month at the Lincoln Memorial, involving the boys from Covington Catholic High School, and a Native American man, was like so many Internet-born controversies before it:…",
    "category": "Culture",
    "content": "The episode last month at the Lincoln Memorial, involving the boys from Covington Catholic High School, and a Native American man, was like so many Internet-born controversies before it: It spawned vituperative reactions, reactions to the reactions, and sweeping meta-analyses of the reactions to the reactions. Altogether it was exactly the type of politically charged commotion that nobody could seem to resist weighing in on. Yet at the heart of it was a misinterpreted, arguably meaningless event driven by an emotion that social media is making more and more familiar to all of us: moral outrage.Moral outrage is the powerful impulse we feel to condemn bad behavior, and it serves the important role of holding wrongdoers accountable and reinforcing social norms. Yet moral outrage, at least on Twitter and other similar platforms, appears no more effective at reinforcing social norms than it is at driving people to theatrically overreact to the behavior of strangers. After all, it’s hard to see how things like doxxing minors or throwing shaving blades down the toilet, in protest of an earnest Gillette ad on “toxic masculinity,” help uphold ethical standards.One recent study offers insight into moral outrage that helps to clarify why so much online discourse has devolved into pointless noise and fury. Researchers asked 1,065 participants, across four experiments, to read about hypothetical moral transgressions and offer judgments, such as how angry they were about the transgressions and how much empathy they felt for the victims. What the researchers found is that participants’ expressions of moral outrage were mostly independent from their empathy for the victims of the transgressions. What triggered moral outrage in their study was whether a particular behavior seemed intrinsically wrong—that is, wrong even if it didn’t harm anyone.  This research implies that when we angrily condemn someone’s actions on Twitter, we do so with little regard for the consequences of the ostensibly immoral action we’re condemning. And this tendency makes us easy prey for outrage bait. For example, as professor of law, science, and technology at Columbia Law School, Tim Wu, pointed out in his book The Attention Merchants, social media companies have financial incentives to propagate content that appears to reveal a moral violation, no matter how substantial, because moral violations grab attention.Of course, many seemingly immoral actions that pop up on the Internet do indeed have harmful consequences and merit attention. But if, as the research shows, our moral outrage is highly sensitive to actions but not consequences, we might want to treat feelings of moral outrage—whether others’ or our own—skeptically.One reason is that an action’s “intrinsic wrongness” is often in the eye of the beholder. Two people can look at the exact same behavior and, depending on factors like what social groups they belong to or their personal relationships to the people involved, draw radically different moral conclusions. In one study, for example, researchers assigned each of their participants to one of two groups based on their performance on a computer task that was unrelated to the true goal of the study. They then asked each participant to observe another person—who was either part of the participant’s group or an outsider—as that person made a moral decision and acted on it. It turned out that on average, participants who observed an ingroup member rated that person’s behavior as fairer than those who observed an outgroup member, even though the behavior was the same for both groups.The Covington Catholic story is an apt example of how subjective moral judgments can be: Some people praised the boys’ “exemplary and respectful behavior” while others likened the standoff between the boys and the Native American activists to the sit-ins of the civil rights movement.Considering whether a given behavior has actually done harm may help us avoid fruitless online shouting matches, not only because the very act of weighing consequences can force us to slow down and think a bit more, but also because considerations of harm can be used as an objective measure of moral wrongness. The moral philosophy of utilitarianism is based on the idea of producing “the greatest good for the greatest number,” and many of its proponents use the scientific method to determine what kinds of behaviors will do just that. Arguments about right and wrong are often based on conflicts between subjective values, but embracing some measure of utilitarianism could serve to anchor moral debates in a shared reality.Good and bad consequences can be difficult to quantify, of course, and people may disagree about which consequences deserve the most attention. But still, recognizing the biases of moral outrage and deliberately shifting some attention from actions themselves to their real-world effects, may, at the very least, keep us from blowing a fuse over behavior that hasn’t really caused anyone to suffer.Scott Koenig is a doctoral student in neuroscience at CUNY, where he studies psychopathy, emotion, and morality. Follow him on Twitter @scotttkoenig.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How social networks have changed human interaction. "
  },
  {
    "imageUrl": "http://static.nautil.us/15807_72085ca61c54cb3316dcf4b61e0b198d.jpg",
    "title": "Here’s How We’ll Know an AI Is Conscious",
    "description": "Posted by Joel Frohlich on March 05, 2019  The Australian philosopher David Chalmers famously asked whether “philosophical zombies” are conceivable—people who behave like you and me yet lack subjective experience. It’s an…",
    "category": "Ideas",
    "content": "The Australian philosopher David Chalmers famously asked whether “philosophical zombies” are conceivable—people who behave like you and me yet lack subjective experience. It’s an idea that has gotten many scholars interested in consciousness, including myself. The reasoning is that, if such zombies, or sophisticated unfeeling robots, are conceivable, then physical properties alone—about the brain or a brain-like mechanism—cannot explain the experience of consciousness. Instead, some additional mental properties must account for the what-it-is-like feeling of being conscious. Figuring out how these mental properties arise has become known as the “hard problem” of consciousness.But I have a slight problem with Chalmers’ zombies. Zombies are supposed to be capable of asking any question about the nature of experience. It’s worth wondering, though, how a person or machine devoid of experience could reflect on experience it doesn’t have. In an episode of the “Making Sense” (formerly known as “Waking Up”) podcast with neuroscientist and author Sam Harris, Chalmers addressed this puzzle. “I don’t think it’s particularly hard to at least conceive of a system doing this,” Chalmers told Harris. “I mean, I’m talking to you now, and you’re making a lot of comments about consciousness that seem to strongly suggest that you have it. Still, I can at least entertain the idea that you’re not conscious and that you’re a zombie who’s in fact just making all these noises without having any consciousness on the inside.”This is not a strictly academic matter—if Google’s DeepMind develops an AI that starts asking, say, why the color red feels like red and not something else, there are only a few possible explanations. Perhaps it heard the question from someone else. It’s possible, for example, that an AI might learn to ask questions about consciousness simply by reading papers about consciousness. It also could have been programmed to ask that question, like a character in a video game, or it could have burped the question out of random noise. Clearly, asking questions about consciousness does not prove anything per se. But could an AI zombie formulate such questions by itself, without hearing them from another source or belching them out from random outputs? To me, the answer is clearly no. If I’m right, then we should seriously consider that an AI might be conscious if it asks questions about subjective experience unprompted. Because we won’t know if it’s ethical to unplug such an AI without knowing if it’s conscious, we better start listening for such questions now.The 21st century is in dire need of a Turing test for consciousness.Our conscious experiences are composed of qualia, the subjective aspects of sensation—the redness of red, the sweetness of sweet. The qualia that compose conscious experiences are irreducible, incapable of being mapped onto anything else. If I were born blind, no one, no matter how articulate, would ever be able to give me a sense of the color blood and roses share. This would be true even if I were among a number of blind people who develop something called blindsight—the ability to avoid obstacles and accurately guess where objects appear on a computer monitor despite being blind.Blindsight seems to demonstrate that some behaviors can be purely mechanized, so to speak, occurring without any subjective awareness—echoing Chalmers’ notion of zombies. The brains of blindsighted people appear to exploit preconscious areas of the visual system, yielding sighted behavior without visual experience. This often occurs after a person suffers a stroke or other injury to the visual cortex, the part of the cerebral cortex that processes visual information. Because the person’s eyes are still healthy, they may feed information hidden from consciousness to certain brain regions, such as the superior colliculus.By the same token, there are at least a few documented cases of deaf hearing. One such case, detailed in a 2017 Philosophical Psychology report, is patient LS, a man deaf since birth, yet able to discriminate sounds based on their content. For people such as LS, this discernment occurs in silence. But if a deaf-hearing person were to ask the sort of questions people who can hear ask—“Doesn’t that sound have a weird sort of brassiness to it?”—then we’d have good reason to suspect this person isn’t deaf at all. (We couldn’t be absolutely sure because the question could be a prank.) Likewise, if an AI began asking, unprompted, the sorts of questions only a conscious being could ask, we’d reasonably form a similar suspicion that subjective experience has come online.The 21st century is in dire need of a Turing test for consciousness. AI is learning how to drive cars, diagnose lung cancer, and write its own computer programs. Intelligent conversation may be only a decade or two away, and future super-AI will not live in a vacuum. It will have access to the Internet and all the writings of Chalmers and other philosophers who have asked questions about qualia and consciousness. But if tech companies beta-test AI on a local intranet, isolated from such information, they could conduct a Turing-test style interview to detect whether questions about qualia make sense to the AI.What might we ask a potential mind born of silicon? How the AI responds to questions like “What if my red is your blue?” or “Could there be a color greener than green?” should tell us a lot about its mental experiences, or lack thereof. An AI with visual experience might entertain the possibilities suggested by these questions, perhaps replying, “Yes, and I sometimes wonder if there might also exist a color that mixes the redness of red with the coolness of blue.” On the other hand, an AI lacking any visual qualia might respond with, “That is impossible, red, green, and blue each exist as different wavelengths.” Even if the AI attempts to play along or deceive us, answers like, “Interesting, and what if my red is your hamburger?” would show that it missed the point.Of course, it’s possible that an artificial consciousness might possess qualia vastly different than our own. In this scenario, questions about specific qualia, such as color qualia, might not click with the AI. But more abstract questions about qualia themselves should filter out zombies. For this reason, the best question of all would likely be that of the hard problem itself: Why does consciousness even exist? Why do you experience qualia while processing input from the world around you? If this question makes any sense to the AI, then we’ve likely found artificial consciousness. But if the AI clearly doesn’t understand concepts such as “consciousness” and “qualia,” then evidence for an inner mental life is lacking.Building a consciousness detector is no small undertaking. Alongside such a Turing test, tomorrow’s researchers will likely apply today’s abstract theories of consciousness in an effort to infer the existence of consciousness from a computer’s wiring diagram. One such theory considers the amount of information integrated by a brain or other system, and is already being applied to infer the existence of consciousness in brain-injured patients and even schools of fish. Indeed, before the motivation to detect artificial consciousness garners substantial funding for such research, the need to detect consciousness in brain-injured patients has already erased the C-word from science’s taboo list.My own lab, led by Martin Monti at the University of California, Los Angeles, strives to improve the lives of brain-injured patients by developing better means of inferring consciousness from electrical or metabolic brain activity. Just as ethical tragedies arise when we pull the plug on patients who are aware yet unresponsive, similar tragedies will arise if we pull the plug on artificial consciousness. And just as my lab at UCLA relates theoretical measures of consciousness to the hospital bed behavior of brain-injured patients, future researchers must relate theoretical measures of artificial consciousness to an AI’s performance on something akin to a Turing test. When we close the textbook at the day’s end, we still need to consider the one question zombies can’t answer.Joel Frohlich is a postdoctoral researcher studying consciousness in the laboratory of Martin Monti at the University of California, Los Angeles. He received his PhD in neuroscience in the laboratory of Shafali Jeste at UCLA while studying biomarkers of neurodevelopmental disorders. He is the editor in chief of the science-communication website “Knowing Neurons.” \n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How to explain consciousness as a system of integrated information. "
  },
  {
    "imageUrl": "http://static.nautil.us/15831_79f93d49fc21735186d6e59d6880abd1.jpg",
    "title": "Galaxy Simulations Offer a New Solution to the Fermi Paradox",
    "description": "Posted by Rebecca Boyle on March 08, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.As far as anyone knows, we have always been alone. It’s just us on this pale blue dot, “home to everyone you love,…",
    "category": "Numbers",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.As far as anyone knows, we have always been alone. It’s just us on this pale blue dot, “home to everyone you love, everyone you know, everyone you ever heard of,” as Carl Sagan so memorably put it. No one has called or dropped by. And yet the universe is filled with stars, nearly all those stars have planets, and some of those planets are surely livable. So where is everybody?The Italian physicist Enrico Fermi was purportedly the first to pose this question, in 1950, and scientists have offered a bounty of solutions for his eponymous paradox since. One of the most famous came from Sagan himself, with William Newman, who postulated in a 1981 paper that we just need patience. Nobody has visited because they’re all too far away; it takes time to evolve a species intelligent enough to invent interstellar travel, and time for that species to spread across so many worlds. Nobody is here yet.Other researchers have argued that extraterrestrial life might rarely become space-faring (just as only one species on Earth ever has). Some argue that tech-savvy species, when they arise, quickly self-destruct. Still others suggest aliens may have visited in the past, or that they’re avoiding us on purpose, having grown intelligent enough to be suspicious of everyone else. Perhaps the most pessimistic answer is a foundational paper from 1975, in which the astrophysicist Michael Hart declared that the only plausible reason nobody has visited is that there really is nobody out there.Now comes a paper that rebuts Sagan and Newman, as well as Hart, and offers a new solution to the Fermi paradox that avoids speculation about alien psychology or anthropology.The research, which is under review by The Astrophysical Journal, suggests it wouldn’t take as long as Sagan and Newman thought for a space-faring civilization to planet-hop across the galaxy, because the movements of stars can help distribute life. “The sun has been around the center of the Milky Way 50 times,” said Jonathan Carroll-Nellenback, an astronomer at the University of Rochester, who led the study. “Stellar motions alone would get you the spread of life on time scales much shorter than the age of the galaxy.” Still, although galaxies can become fully settled fairly quickly, the fact of our loneliness is not necessarily paradoxical: According to simulations by Carroll-Nellenback and his colleagues, natural variability will mean that sometimes galaxies will be settled, but often not—solving Fermi’s quandary.It’s possible that the Milky Way is partially settled, or intermittently so…The question of how easy it would be to settle the galaxy has played a central role in attempts to resolve the Fermi paradox. Hart and others calculated that a single space-faring species could populate the galaxy within a few million years, and maybe even as quickly as 650,000 years. Their absence, given the relative ease with which they should spread, means they must not exist, according to Hart.Sagan and Newman argued it would take longer, in part because long-lived civilizations are likelier to grow more slowly. Faster-growing, rapacious societies might peter out before they could touch all the stars. So maybe there have been a lot of short-lived, fast-growing societies that wink out, or a few long-lived, slowly expanding societies that just haven’t arrived yet, as Jason Wright of Pennsylvania State University, a coauthor of the new study, summarized Sagan and Newman’s argument. But Wright doesn’t agree with either solution.“That conflates the expansion of the species as a whole with the sustainability of individual settlements,” he said. “Even if it is true for one species, it is not going to be this iron-clad law of xenosociology where if they are expanding, they are necessarily short-lived.” After all, he noted, life on Earth is robust, “and it expands really fast.”In their new paper, Carroll-Nellenback, Wright and their collaborators Adam Frank of Rochester and Caleb Scharf of Columbia University sought to examine the paradox without making untestable assumptions. They modeled the spread of a “settlement front” across the galaxy, and found that its speed would be strongly affected by the motions of stars, which previous work—including Sagan and Newman’s—treated as static objects. The settlement front could cross the entire galaxy based just on the motions of stars, regardless of the power of propulsion systems. “There is lots of time for exponential growth basically leading to every system being settled,” Carroll-Nellenback said.But the fact that no interstellar visitors are here now—what Hart called “Fact A”—does not mean they do not exist, the authors say. While some civilizations might expand and become interstellar, not all of them last forever. On top of that, not every star is a choice destination, and not every planet is habitable. There’s also what Frank calls “the Aurora effect,” after Kim Stanley Robinson’s novel Aurora, in which settlers arrive at a habitable planet on which they nonetheless cannot survive.When Carroll-Nellenback and his coauthors included these impediments to settlement in their model and ran many simulations with different star densities, seed civilizations, spacecraft velocities and other variations, they found a vast middle ground between a silent, empty galaxy and one teeming with life. It’s possible that the Milky Way is partially settled, or intermittently so; maybe explorers visited us in the past, but we don’t remember, and they died out. The solar system may well be amid other settled systems; it’s just been unvisited for millions of years.Anders Sandberg, a futurist at the University of Oxford’s Future of Humanity Institute who has studied the Fermi paradox, said he thinks spacecraft would spread civilizations more effectively than stellar motions. “But the mixing of stars could be important,” he wrote in an email, “since it is likely to spread both life, through local panspermia”—the spread of life’s chemical precursors—“and intelligence, if it really is hard to travel long distances.”Frank views his and his colleagues’ new paper as SETI-optimistic. He and Wright say that now we need to look harder for alien signals, which will be possible in the coming decades as more sophisticated telescopes open their eyes to the panoply of exoplanets and begin glimpsing their atmospheres.“We are entering an era when we are going to have actual data relevant to life on other planets,” Frank said. “This couldn’t be more relevant than in the moment we live.”Seth Shostak, an astronomer at the SETI Institute who has studied the Fermi paradox for decades, thinks it is likely to be explained by something more complex than distance and time—like perception.Maybe we are not alone and have not been. “The click beetles in my backyard don’t notice that they’re surrounded by intelligent beings—namely my neighbors and me,” Shostak said, “but we’re here, nonetheless.”Rebecca Boyle is an award-winning freelance journalist in Saint Louis, Missouri. She is a contributing writer for The Atlantic and a frequent contributor at FiveThirtyEight. Rebecca is also a member of the group science blog The Last Word on Nothing. Her work regularly appears in Scientific American, New Scientist, Aeon, Wired, Popular Science, and other publications, and has twice been anthologized in the Best American Science & Nature Writing.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: What Donald Trump teaches us about the Fermi Paradox "
  },
  {
    "imageUrl": "http://static.nautil.us/15750_6984fba75d83f56682b0329c93e651df.png",
    "title": " Why We Should Think Twice About Colonizing Space",
    "description": "Posted by Phil Torres on February 18, 2019  There are lots of reasons why colonizing space seems compelling. The popular astronomer Neil deGrasse Tyson argues that it would stimulate the economy and inspire the next generation of…",
    "category": "Ideas",
    "content": "There are lots of reasons why colonizing space seems compelling. The popular astronomer Neil deGrasse Tyson argues that it would stimulate the economy and inspire the next generation of scientists. Elon Musk, who founded SpaceX, argues that “there is a strong humanitarian argument for making life multiplanetary…to safeguard the existence of humanity in the event that something catastrophic were to happen.”  The former administrator of NASA, Michael Griffin, frames it as a matter of the “survival of the species.” And the late astrophysicist Stephen Hawking has conjectured that if humanity fails to colonize space within 100 years, we could face extinction.To be sure, humanity will eventually need to escape Earth to survive, since the sun will make the planet uninhabitable in about 1 billion years. But for many “space expansionists,” escaping Earth is about much more than dodging the bullet of extinction: it’s about realizing astronomical amounts of value by exploiting the universe’s vast resources to create something resembling utopia. For example, the astrobiologist Milan Cirkovic calculates that some 1046 people per century could come into existence if we were to colonize our Local Supercluster, Virgo. This leads Nick Bostrom to argue that failing to colonize space would be tragic because it would mean that these potential “worthwhile lives” would never exist, and this would be morally bad.But would these trillions of lives actually be worthwhile? Or would colonization of space lead to a dystopia?In a recent article in Futures, which was inspired by political scientist Daniel Deudney’s forthcoming book Dark Skies, I decided to take a closer look at this question. My conclusion is that in a colonized universe the probability of the annihilation of the human race could actually rise rather than fall.The argument is based on ideas from evolutionary biology and international relations theory, and it assumes that there aren’t any other technologically advanced lifeforms capable of colonizing the universe (as a recent study suggests is the case).Consider what is likely to happen as humanity hops from Earth to Mars, and from Mars to relatively nearby, potentially habitable exoplanets like Epsilon Eridani b, Gliese 674 b, and Gliese 581 d. Each of these planets has its own unique environments that will drive Darwinian evolution, resulting in the emergence of novel species over time, just as species that migrate to a new island will evolve different traits than their parent species. The same applies to the artificial environments of spacecraft like “O’Neill Cylinders,” which are large cylindrical structures that rotate to produce artificial gravity. Insofar as future beings satisfy the basic conditions of evolution by natural selection—such as differential reproduction, heritability, and variation of traits across the population—then evolutionary pressures will yield new forms of life.But the process of “cyborgization”—that is, of using technology to modify and enhance our bodies and brains—is much more likely to influence the evolutionary trajectories of future populations living on exoplanets or in spacecraft. The result could be beings with completely novel cognitive architectures (or mental abilities), emotional repertoires, physical capabilities, lifespans, and so on.In other words, natural selection and cyborgization as humanity spreads throughout the cosmos will result in species diversification. At the same time, expanding across space will also result in ideological diversification. Space-hopping populations will create their own cultures, languages, governments, political institutions, religions, technologies, rituals, norms, worldviews, and so on. As a result, different species will find it increasingly difficult over time to understand each other’s motivations, intentions, behaviors, decisions, and so on. It could even make communication between species with alien languages almost impossible. Furthermore, some species might begin to wonder whether the proverbial “Other” is conscious. This matters because if a species Y cannot consciously experience pain, then another species X might not feel morally obligated to care about Y. After all, we don’t worry about kicking stones down the street because we don’t believe that rocks can feel pain. Thus, as I write in the paper, phylogenetic and ideological diversification will engender a situation in which many species will be “not merely aliens to each other but, more significantly, alienated from each other.”But this yields some problems. First, extreme differences like those just listed will undercut trust between species. If you don’t trust that your neighbor isn’t going to steal from, harm, or kill you, then you’re going to be suspicious of your neighbor. And if you’re suspicious of your neighbor, you might want an effective defense strategy to stop an attack—just in case one were to happen. But your neighbor might reason the same way: she’s not entirely sure that you won’t kill her, so she establishes a defense as well. The problem is that, since you don’t fully trust her, you wonder whether her defense is actually part of an attack plan. So you start carrying a knife around with you, which she interprets as a threat to her, thus leading her to buy a gun, and so on. Within the field of international relations, this is called the “security dilemma,” and it results in a spiral of militarization that can significantly increase the probability of conflict, even in cases where all actors have genuinely peaceful intentions.So, how can actors extricate themselves from the security dilemma if they can’t fully trust each other? On the level of individuals, one solution has involved what Thomas Hobbes’ calls the “Leviathan.” The key idea is that people get together and say, “Look, since we can’t fully trust each other, let’s establish an independent governing system—a referee of sorts—that has a monopoly on the legitimate use of force. By replacing anarchy with hierarchy, we can also replace the constant threat of harm with law and order.” Hobbes didn’t believe that this happened historically, only that this predicament is what justifies the existence of the state. According to Steven Pinker, the Leviathan is a major reason that violence has declined in recent centuries.The point is that if individuals—you and I—can overcome the constant threat of harm posed by our neighbors by establishing a governing system, then maybe future species could get together and create some sort of cosmic governing system that could similarly guarantee peace by replacing anarchy with hierarchy. Unfortunately, this looks unpromising within the “cosmopolitical” realm. One reason is that for states to maintain law and order among their citizens, their various appendages—e.g., law enforcement, courts—need to be properly coordinated. If you call the police about a robbery and they don’t show up for three weeks, then what’s the point of living in that society? You’d be just as well off on your own! The question is, then, whether the appendages of a cosmic governing system could be sufficiently well-coordinated to respond to conflicts and make top-down decisions about how to respond to particular situations. To put it differently: If conflict were to break out in some region of the universe, could the relevant governing authorities respond soon enough for it to matter, for it to make a difference?Probably not, because of the immense vastness of space. For example, consider again Epsilon Eridani b, Gliese 674 b, and Gliese 581 d. These are, respectively, 10.5, 14.8, and 20.4 light-years from Earth. This means that a signal sent as of this writing, in 2018, wouldn’t reach Gliese 581 d until 2038. A spaceship traveling at one-quarter the cosmic speed limit wouldn’t arrive until 2098, and a message to simply affirm that it had arrived safely wouldn’t return to Earth until 2118. And Gliese 581 is relatively close as far as exoplanets go. Just consider that he Andromeda Galaxy is some 2.5 million light-years from Earth and the Triangulum Galaxy about 3 million light-years away. What’s more, there are some 54 galaxies in our Local Group, which is about 10 million light-years wide, within a universe that stretches some 93 billion light-years across.These facts make it look hopeless for a governing system to effectively coordinate law enforcement activities, judicial decisions, and so on, across cosmic distances. The universe is simply too big for a government to establish law and order in a top-down fashion.But there is another strategy for achieving peace: Future civilizations could use a policy of deterrence to prevent other civilizations from launching first strikes. A policy of this sort, which must be credible to work, says: “I won’t attack you first, but if you attack me first, I have the capabilities to destroy you in retaliation.” This was the predicament of the US and Soviet Union during the Cold War, known as “mutually-assured destruction” (MAD).But could this work in the cosmopolitical realm of space? It seems unlikely. First, consider how many future species there could be: upwards of many billions. While some of these species would be too far away to pose a threat to each other—although see the qualification below—there will nonetheless exist a huge number within one’s galactic backyard. The point is that the sheer number would make it incredibly hard to determine who initiated a first strike, if one is attacked. And without a method for identifying instigators with high reliability, one’s policy of deterrence won’t be credible. And if one’s policy of deterrence isn’t credible, then one has no such policy!Second, ponder the sorts of weapons that could become available to future spacefaring civilizations. Redirected asteroids (a.k.a., “planetoid bombs”), “rods from God,” sun guns, laser weapons, and no doubt an array of exceptionally powerful super-weapons that we can’t currently imagine. It has even been speculated that the universe might exist in a “metastable” state and that a high-powered particle accelerator could tip the universe into a more stable state. This would create a bubble of total annihilation that spreads in all directions at the speed of light—which opens up the possibility that a suicidal cult, or whatever, weaponizes a particle accelerator to destroy the universe.The question, then, is whether defensive technologies could effectively neutralize such risks. There’s a lot to say here, but for the present purposes just note that, historically speaking, defensive measures have very often lagged behind offensive measures, thus resulting in periods of heightened vulnerability. This is an important point because when it comes to existentially dangerous super-weapons, one only needs to be vulnerable for a short period to risk annihilation.So far as I can tell, this seriously undercuts the credibility of policies of deterrence. Again, if species A cannot convince species B that if B strikes it, A will launch an effective and devastating counter strike, then B may take a chance at attacking A. In fact, B does not need to be malicious to do this: it only needs to worry that A might, at some point in the near- or long-term future, attack B, thus making it rational for B to launch a preemptive strike (to eliminate the potential danger). Thinking about this predicament in the radically multi-polar conditions of space, it seems fairly obvious that conflict will be extremely difficult to avoid.The lesson of this argument is not to uncritically assume that venturing into the heavens will necessarily make us safer or more existentially secure. This is a point that organizations hoping to colonize Mars, such as SpaceX, NASA, and Mars One should seriously contemplate. How can humanity migrate to another planet without bringing our problems with us? And how can different species that spread throughout the cosmos maintain peace when sufficient mutual trust is unattainable and advanced weaponry could destroy entire civilizations? Human beings have made many catastrophically bad decisions in the past. Some of these outcomes could have been avoided if only the decision-makers had deliberated a bit more about what could go wrong—i.e., had done a “premortem” analysis. We are in that privileged position right now with respect to space colonization. Let’s not dive head-first into waters that turn out to be shallow.Phil Torres is the director of the Project for Human Flourishing and the author of Morality, Foresight, and Human Flourishing: An Introduction to Existential Risks.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Should we pessimistic about the deep future?This classic Facts So Romantic post was originally published in July 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15622_fc1d0d2f2375b7a4b27ed621c335b19b.jpg",
    "title": "Evolution’s Gravity: A Paean to Natural Selection",
    "description": "Posted by Michael McCullough on January 08, 2019  Physicists speak of four fundamental forces that govern the interactions among the bits of matter that make up our universe. The strongest of these four forces, aptly known as the Strong…",
    "category": "Ideas",
    "content": "Physicists speak of four fundamental forces that govern the interactions among the bits of matter that make up our universe. The strongest of these four forces, aptly known as the Strong Force, is so powerful that it can keep an atom’s positively charged protons from ripping the atom’s nucleus apart as their mutually repellent positive charges push them in opposite directions. The second fundamental force, electromagnetism, is 137 times weaker than the strong force, but its ability to cause bits of matter with opposing electrical charges to attract each other, and to cause bits of matter with like charges to avoid each other, is what gives unique three-dimensional structure to atoms, molecules, and even the proteins that form the building blocks of our body’s cells. At only one-millionth the strength of the strong force, the third fundamental force—the so-called weak force—changes quarks from one bizarre “flavor” to another and gives rise to nuclear fusion reactions.The weak force deserves a better name: It’s actually the fourth force—gravity—that’s the weakling of the bunch. At only 6/1,000,000,000,000,000,000,000,000,000,000,000,000,000 the strength of the strong force, the influence of gravity on the interactions of protons, quarks, and other subatomic particles amounts to, well, about as close to zero as you can get. When I use the refrigerator magnet that holds up my kid’s school photo to lift the ring of keys on the kitchen table, the magnet easily overcomes the gravitational pull of the entire planet. At Subatomic Beach, gravity is the scrawny guy who’s always getting sand kicked in his face.Gravity is the Charles Atlas of the cosmos. Gravity is a star-maker.But the only reason gravity looks like such a weakling in comparison to the other fundamental forces is because we haven’t yet zoomed out to the scales of mass and distance that reveal gravity’s actual power to guide the interactions among bits of matter. For the change of perspective that can reveal gravity’s real power, we have to use a telescope, not a particle accelerator. When we’re studying the interactions of very small things that are separated by small distances, gravity is the only fundamental force that doesn’t matter. But when we’re studying the interactions of large things that are separated by great distances, it’s the only one that does.Every time the mass of an object increases one-hundredfold, the influence of gravity upon its particles increases tenfold. Because very massive objects like planets and stars have no net electrical charge (the charges of all of their constituent bits more or less cancel each other), it’s the weakling gravity—acting across huge distances, always attracting, never repelling—that causes their interactions. And when an object gets really massive—roughly the size of 100 Jupiters—the gravitational forces acting on the atoms that make up that jumbo object can hold the object’s particles in a spherical shape even when weak force interactions among those particles have turned the center of the object into a nuclear fusion reactor. Gravity is the Charles Atlas of the cosmos. Gravity is a star-maker.Natural selection, one of the fundamental processes of evolution, has something in common with gravity: A public relations problem. At one level of analysis, natural selection, like gravity, looks like a chump. When you’re looking up close at the tiny bits of stuff that go into making humans—the sequences of DNA that constitute the human genome—and how they came to be arranged in the manner that they are, natural selection doesn’t seem to have done very much. Other evolutionary processes, such as mutation, migration, and drift, seem to have exerted far more powerful influences on our genomes. For that matter, distinctly non-evolutionary events—one-off famines, freezes, floods, and fires—can exert a far more powerful influence on the fate of a species at any given point in time than natural selection can.However, when you zoom out and look at evolution from a high-altitude vantage point, natural selection is the only evolutionary force that matters at all. This is because natural selection is the only evolutionary force that can produce design. Natural selection, like gravity, acts uniformly and consistently, through deep time, to sift genes according to one hard-and-fast criterion: It increases the prevalence of genes that are good at increasing their own rates of propagation and it reduces the prevalence of genes that are less good at increasing their rates of propagation.As Richard Dawkins has described so brilliantly in so many different ways, genes take actions in the world that alter their rates of replication by cloaking themselves in really cool features and gadgets—mitochondria, ribosomes, specialized cells, arms, legs, eyes, ears, neurons, brains, beliefs, desires. Those features that increase the genes’ replication rates get conserved and elaborated upon. Those that reduce the genes’ rates of replication are shuffled off. As the result of aeons and aeons of a gene-sifting process that operates according to a single criterion—does this gene create phenotypic effects that speed up its propagation in the population, or does it slow its propagation?—organisms accumulate design.None of the other evolutionary forces can produce this kind of complex functional design. The result of all of natural selection’s criterion-based gene-sifting is that organisms end up looking like geniuses for thriving in the environments to which they are adapted. Bacteria, birds, bees, bats, bears, boas—and even Bill and Betty—every one is a genius.Natural selection, like gravity, is a star-maker.Michael McCullough is a psychologist at the University of Miami in Coral Gables Florida, where he directs the Evolution and Human Behavior Laboratory. He studies the cognitive foundations of human social behavior. This essay is excerpted from his forthcoming book, Why We Give a Damn: The Enigma of Generosity in a World of Strangers (Basic Books). Follow him on Twitter @ME_McCullough.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15642_3f4243a6f681f2920ac3e95a63517274.jpg",
    "title": "Jellyfish Genome Hints That Complexity Isn’t Genetically Complex",
    "description": "Posted by Jonathan Lambert on January 15, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.An overarching theme in the story of evolution, at least over the past half billion years or so, is rising complexity.…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.An overarching theme in the story of evolution, at least over the past half billion years or so, is rising complexity. There are other themes, of course, but life has undoubtedly become more complicated since its origin. Early cells globbed together to form multicellular coalitions. Those developed more complex bodies and lifestyles as the millennia passed, finding ever more varied ways to make a living. You might expect that as bodies became more complex, genomes did as well.But a recent study appearing in Nature Ecology & Evolution shows that not to be the case—at least for jellyfish, humble organisms that evolved at a crucial juncture in animal history. They did not need more genes—or even notably different ones—to power their giant leap in complexity. This new study adds to a growing body of work that casts doubt on finding straightforward genomic signatures of the evolution of complexity.Jellyfish sit alongside sea anemones, coral and hydra on the Cnidarian branch of the animal tree of life. The fork leading to cnidarians represents the final turnoff before animals become bilaterally symmetric, which makes them an interesting group to study because of the greater complexity that came with that later innovation. Jellyfish begin their lives much like their cousins, as sessile polyps anchored to the seafloor, scrounging for food in the passing currents. Unlike their cousins, they eventually break free and transform into a free-swimming form known as a medusa, “what we normally think of when we think of jellyfish,” said David Gold, a biologist at the University of California, Davis, who led the study.Gold explained that the medusa stage represents a quantum leap in complexity. Medusas actively hunt plankton and navigate the water column with neural sensory structures that detect light and orientation. To go from being a stationary polyp to a floating medusa is almost akin to humans evolving the ability to swim through the air and capture birds with springy, netlike appendages.The danger when gauging animal complexity, however, is that it’s hard to be objective about it. Mansi Srivastava, a biologist at Harvard University who studies animal complexity and was not involved in this study, cautions that how we view complexity can have more to do with us than with what we’re trying to define. Because we’re so different from sponges and jellyfish, we can miss complex metabolic pathways or other features of “simpler” animal life, leading us to conclude simplicity where there is nuance.Still, Srivastava and Gold agree that if you tie complexity to life history, jellyfish are more complex than their cnidarian kin. But how they made this jump was unclear. “We just had no idea of what sort of genetic changes were needed to go from this more simple lifestyle to this more complex lifestyle,” said Gold. To find out, the researchers decided to sequence the genome of Aurelia, the moon jellyfish, and then compare it to those of cnidarians without medusas.If a radical shift in life history requires a big boost in gene content, the Aurelia genome should be riddled with novel genes unique to jellyfish. Instead, Gold found that, broadly speaking, “there really isn’t a whole lot of difference between Aurelia and their relatives with simpler lifestyles.” There were some new genes, but no more than you might expect from any distinct group.This finding was not terribly surprising to Gold because the genomes of other, much more disparate species also look fairly similar if you squint at them. Gold already had a more nuanced hypothesis: When it comes to building a body, it’s not just what genes exist that matters but also when they’re used. If you’re looking for genes that build complexity in jellyfish, then it makes sense to look for them when that complexity is manifesting, during medusa development. Gold thought that genes unique to jellyfish would be active during the transformation from polyp to medusa.But to his surprise, that’s not what he found. New genes unique to jellyfish were no more likely to be expressed in the medusa stage, or any stage of development, than other, older genes were. “At the broad genetic level, it doesn’t seem like you need major changes in the genome to make these big changes in your life history,” Gold said.An alternative explanation accounts for the lack of differences by flipping around the origin story. Perhaps complexity, in the form of a medusa stage, existed in early cnidarians but was lost in all related groups living today except jellyfish. Either scenario would produce a similar genomic signature, although Gold thinks the latter less likely. Comparing more cnidarian sequences at different stages of divergence will eventually reveal the answer.“It’s not that surprising that the jellyfish didn’t just invent a whole bunch of new genes to make a medusa stage,” said Srivastava, “but we don’t know until we look.” She was intrigued by the finding that novel genes weren’t overrepresented in the medusa stage because it suggests that “very different body plans can arise by connecting the same genes in different ways.”Gold’s results broadly align with those from another jellyfish genome, Clytia. That research, too, found no large role for novel genes. To add to the mystery, there were even hints that in Clytia, more ancient and conserved pathways played a larger role in medusa development.In any case, for now, the genetic changes that orchestrate this metamorphosis in jellyfish remain unknown. The transformation may depend on regions of the genome that don’t encode proteins, but instead regulate when genes are turned on and off. Perhaps it’s easier for life to innovate by rearranging its existing gene networks instead of evolving scores of new genes. Or perhaps the broad first pass at the genomes simply missed a handful of coding genes that play an outsized role in the process.The Aurelia genome joins a growing number of studies that complicate our view of complexity. When scientists began comparing the genes and genomes of different branches of the tree of life, they expected to find vast differences, but instead discovered remarkable similarity. For example, humans and cats share about 90 percent of our genes; we share nearly two-thirds of our genome with fruit flies, despite being separated for approximately 800 million years.Even the earliest animal lineages harbor unexpected complexities. When Srivastava and colleagues sequenced the first sponge genome in 2010, they were stunned to find genes that built the brains and muscles of other animals already present in this brainless, muscleless sponge. “The genes are the same, but clearly they aren’t working together to do the same things,” she said.As more genomes get sequenced, researchers will be able to move beyond broad brush descriptions. “I think genome sequencing is a really great tool for generating hypotheses,” said Srivastava. “And then the sequence itself provides us with a tool to more effectively test those hypotheses.” Armed with sequence data and functional tools like CRISPR, scientists can start tweaking individual genes and gene networks to disentangle the relationship between their connections and the varied forms they build.“I can give you a list of genes that seem to be associated with an increase in complexity,” Srivastava said, “but at some level that’s not very powerful information. It can’t explain why a human looks like a human and a sponge looks like a sponge.” According to her, “the next big advances will come from sort of the hard work of doing experimental biology,” and she’s excited to see where that hard work takes us.Johnathan Lambert is an evolutionary biologist turned science writer, currently working as Quanta Magazine‘s writing intern. Before that, he was a AAAS Mass Media fellow at the Dallas Morning News, and was also an assistant producer on the science podcast for kids from American Public Media, Brains On!. He is currently a Ph.D student at Cornell University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15646_5aa69473df6673476b7fbcbdf4455955.jpg",
    "title": "When People Are as Predictable as Water",
    "description": "Posted by Brian  Gallagher on January 16, 2019  Can we apply a physics-like reductionism to people? That’s a question we asked Simon DeDeo, a professor of social and decision sciences at Carnegie Mellon University, who also heads the…",
    "category": "Culture",
    "content": "Can we apply a physics-like reductionism to people? That’s a question we asked Simon DeDeo, a professor of social and decision sciences at Carnegie Mellon University, who also heads the Laboratory for Social Minds at the Santa Fe Institute. DeDeo was well suited to the question. With a background in astrophysics, studying galaxy formation, he’s applied a similar, mathematical approach to both contemporary and historical social phenomena (see his Nautilus feature on shifting attitudes toward violent crime, “When Theft Was Worse Than Murder”).“One of the bugbears of the social sciences—and the study of groups and the origins and development of civilization—is this notion of human nature,” DeDeo told Nautilus editor in chief Michael Segal. “Since the very beginning of what you might call a ‘science of society,’ people have always gone back to this idea that there are some invariants of human society.” These boil down to a list of biological constants that are able to generate a diversity in human societies that somewhat mirrors galactic variety. “If you’re a physicist, you’re really proud of how little you need to assume to get where you’re going—and I think in the modern era now, in the study of society, of human behavior, we’re beginning to develop a taste for those kinds of explanations that get further with less,” DeDeo said.They modeled the runners not as individuals, but as part of a continuous flowing material.It shouldn’t be a surprise, then, to hear that people, moving as a crowd, shift and jostle in ways that render them as predictable as water spilling down a channel. In a paper published in Science this month, two French scientists, Nicolas Bain and Denis Bartolo, modeled marathon runners as they walked up to the starting line of the Chicago Marathon. However, they modeled the runners not as individuals, but as part of a continuous flowing material. “Guided by the spectral properties of velocity waves, we build on conservation laws and symmetry principles to construct a predictive theory of pedestrian flows without resorting to any behavioral assumption,” the authors wrote.As if taking a cue from DeDeo—getting further with less—Bain and Bartolo eliminated any trace of human characteristics in their model and explained human behavior with a physicist’s precision. From a practical perspective, they concluded, understanding the behavior of crowds in terms of hydrodynamics could help civic planners design more efficient and safe crowd controls.By likening social phenomena to physical processes, DeDeo explains in a clip of our interview below, it’s possible to model even more complex human behavior, like variation among cultures.  Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15546_6b4017c4c626882acb363d25dbb6f3c0.jpg",
    "title": " The Psychological Challenges of Just Getting to Mars",
    "description": "Posted by Brian  Gallagher on December 11, 2018  Life outside Earth has its own Hobbesian description: isolated, confined, and extreme—or I.C.E. “Space is the quintessential ICE environment,” according to a 2018 paper, published…",
    "category": "Culture",
    "content": "Life outside Earth has its own Hobbesian description: isolated, confined, and extreme—or I.C.E. “Space is the quintessential ICE environment,” according to a 2018 paper, published in American Psychologist. Space includes inhospitable planets like Mars, whose arresting vistas, canyons, and mountains beckon. But only humans sealed inside cumbersome suits, trained to weather such nerve-racking circumstances, can explore them. Just getting to Mars, says Lauren Blackwell Landon, the paper’s lead author and a behavioral performance researcher at NASA, presents a major challenge. “The astronauts will be months away from home, confined to a vehicle no larger than a mid-sized RV”—the still-under-development Orion spacecraft—“for two to three years,” she says. Unlike on the International Space Station, “there will be an up to 45-minute lag on communications to and from Earth.”Orion is NASA’s answer to the call of deep-space exploration. “It will be the safest, most advanced spacecraft ever built,” a NASA document states, “and it will be flexible and capable enough to take us to a variety of destinations,” including the moons of Mars and, by the mid or late 2030s, the red planet itself. Landon and her co-authors, Kelley J. Slack and Jamie D. Barrett, worry about how a crew of four, Orion’s max capacity, will fare on the journey. They will be “operating in extreme isolation and confinement,” the authors write. “Special considerations,” like screening for certain personality traits, for example, “must be made to enhance teamwork and team well-being…”This may be true for astronauts aboard Orion. But perhaps not aboard SpaceX’s Big Falcon Rocket, or B.F.R., which began construction in March. “The BFR is the big dream,” Alan Boyle, a veteran space journalist and a science editor at Geekwire, said. “It’s what will fulfill Musk’s goal of getting to Mars.” With 40 cabins, it can house upwards of a 200-person crew. “You could conceivably have five or six people per cabin, if you wanted to cram people in,” Musk said last year, in a presentation of the the B.F.R. “But, mostly, we would expect to see two to three people per cabin, so normally about a hundred people per flight to Mars.” Those passengers will also, according to current plans, have “large common areas” at their disposal, with at least one dedicated, Musk added, to “entertainment.” In other words, though space may be the quintessential I.C.E. environment, Musk appears to be aiming to make trips there—aspirationally scheduled to commence in 2024—as far away from I.C.E.-y as possible.“...although the ‘physics’ of Antarctica might be ‘wrong,’ the psychological ‘mind-set’ was right.”It might be a good thing that the Orion spacecraft won’t be taking any astronauts to Mars in the near future. It’s a journey NASA may not know how to train for. Landon and her colleagues write:First, few astronauts have participated in long-duration space missions, and there is a limited number of analog missions per year…Second, researchers and astronauts from different cultures may use disparate models. Third, astronauts have a heavy nominal workload, leaving little time for answering surveys or participating in teamwork research related to mission experiences. Fourth, there is a lack of standard measures, both in spaceflight studies and spaceflight analog studies, further restricting total sample sizes and the comparison of findings across isolated, confined, extreme environments. Due to lack of data from spaceflight and spaceflight analog environments, meta-analysis is simply not a viable option for examining many of the different factors that will be critical to teams on a Mars mission.One recently completed analog study, sponsored by NASA, is HI-SEAS—Hawai’i Space Exploration Analog and Simulation. Without real-time communication to the outside world, six crew members lived and worked for over a year 8,000 feet above sea level, in a dome around the size of a 3-bedroom apartment, to help figure out how well an isolated and confined team could perform as a Mars research outpost. After a few months in, Sheyna Gifford, the crew’s physician, wrote for Nautilus about the surprising lack of “asthenia”—a word that the Russian space program used to describe the way the banality of routine can sap a cosmonaut’s strength.“Whether in real or simulated space, the daily and weekly regimentation of everything we do for the sake of safety and efficiency—like swapping air filters, filing reports with ground control, running someone else’s experiments, and so on—as well as our lack of power to break or even affect the pattern, can become oppressive,” she wrote. “Yet, through it all, asthenia has yet to appear. Here’s why, I think: On [simulated] Mars, each of us knows how un-alone we are and how essential each person on our mission is. The doctor, the biologist, the physicist, the soil scientist, the engineer, and the architect count on each other for nearly everything we need.”Psychological reports like these, Landon and her colleagues say, are valuable despite spaceflight itself not being part of the simulation. “Fortunately, even with a disconnect between the physical environments of analogs and spaceflight, psychological fidelity can be achieved,” they write. “When talking to the astronaut corps about his recent trip searching for meteorites on the surface of Antarctica, Don Pettit, an astronaut with experience on two ISS missions, stated that although the ‘physics’ of Antarctica might be ‘wrong,’ the psychological ‘mind-set’ was right.”Along with needing several more years’ worth of data on what promotes and hinders teamwork on long-duration missions in I.C.E. environments, NASA will also need to be more empirical in its team-selection process, the authors write. “NASA currently does not use a scientifically based approach to composing teams, but this knowledge gap is scheduled to be filled by the 2020s.” Going to Mars on Orion is such a high-stress, high-pressure scenario that aspects of personality and background that don’t normally merit close attention start to matter to team functioning. “Given that nuanced and deep-level characteristics of values, culture, and humor are important for successful long-term teamwork, more research is needed into long-duration, international teams and data-driven methods of team composition that may be applied across cultures,” Landon and her colleagues write. “A team living together for multiple years will be required to not only perform task assignments effectively but also fulfill social roles within the team,” like being the one who can be counted on to ease tensions by cracking jokes.If SpaceX’s B.F.R. avoids some of the challenges of an Orion-style mission to Mars, it also creates new ones—like how to have around 200 people, arriving on two B.F.R.s accompanied by two cargo-filled ones, survive on an alien planet while building the rudiments of a settlement. If things go according to plan, those first explorers will also find two other B.F.R.s filled with cargo—to be launched in 2022—waiting for them. “We should—particularly with six ships there—have plenty of landed mass to construct the [rocket] propellant depot, which will consist of a large array of solar panels, and then everything necessary to mine and refine water, draw the CO2 out of the atmosphere, and then create and store deep cryo CH4 and O2,” Musk wrote in a New Space paper. The more granular details of how six ships transform into a Mars base are undecided. But there’s little doubt that it’ll take teamwork, among other things, to start “terraforming Mars,” as Musk says, “and making it a really nice place to be.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in May 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15595_f3c013d50e1737ca632a8f17e5815afc.jpg",
    "title": " Why Black Hole Interiors Grow (Almost) Forever",
    "description": "Posted by Natalie Wolchover on January 02, 2019  Reprinted with permission from Quanta Magazine’s Abstractions blog.Leonard Susskind, a pioneer of string theory, the holographic principle, and other big physics ideas spanning the…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Leonard Susskind, a pioneer of string theory, the holographic principle, and other big physics ideas spanning the past half-century, has proposed a solution to an important puzzle about black holes. The problem is that even though these mysterious, invisible spheres appear to stay a constant size as viewed from the outside, their interiors keep growing in volume essentially forever. How is this possible?In a series of recent papers and talks, the 78-year-old Stanford University professor and his collaborators conjecture that black holes grow in volume because they are steadily increasing in complexity—an idea that, while unproven, is fueling new thinking about the quantum nature of gravity inside black holes.Black holes are spherical regions of such extreme gravity that not even light can escape. First discovered a century ago as shocking solutions to the equations of Albert Einstein’s general theory of relativity, they’ve since been detected throughout the universe. (They typically form from the inward gravitational collapse of dead stars.) Einstein’s theory equates the force of gravity with curves in space-time, the four-dimensional fabric of the universe, but gravity becomes so strong in black holes that the space-time fabric bends toward its breaking point—the infinitely dense “singularity” at the black hole’s center.Quantum mechanics says the universe preserves all information about the past.According to general relativity, the inward gravitational collapse never stops. Even though, from the outside, the black hole appears to stay a constant size, expanding slightly only when new things fall into it, its interior volume grows bigger and bigger all the time as space stretches toward the center point. For a simplified picture of this eternal growth, imagine a black hole as a funnel extending downward from a two-dimensional sheet representing the fabric of space-time. The funnel gets deeper and deeper, so that infalling things never quite reach the mysterious singularity at the bottom. In reality, a black hole is a funnel that stretches inward from all three spatial directions. A spherical boundary surrounds it called the “event horizon,” marking the point of no return.Since at least the 1970s, physicists have recognized that black holes must really be quantum systems of some kind—just like everything else in the universe. What Einstein’s theory describes as warped space-time in the interior is presumably really a collective state of vast numbers of gravity particles called “gravitons,” described by the true quantum theory of gravity. In that case, all the known properties of a black hole should trace to properties of this quantum system.Indeed, in 1972, the Israeli physicist Jacob Bekenstein figured out that the area of the spherical event horizon of a black hole corresponds to its “entropy.” This is the number of different possible microscopic arrangements of all the particles inside the black hole, or, as modern theorists would describe it, the black hole’s storage capacity for information.Bekenstein’s insight led Stephen Hawking to realize two years later that black holes have temperatures, and that they therefore radiate heat. This radiation causes black holes to slowly evaporate away, giving rise to the much-discussed “black hole information paradox,” which asks what happens to information that falls into black holes. Quantum mechanics says the universe preserves all information about the past. But how does information about infalling stuff, which seems to slide forever toward the central singularity, also evaporate out?The relationship between a black hole’s surface area and its information content has kept quantum gravity researchers busy for decades. But one might also ask: What does the growing volume of its interior correspond to, in quantum terms? “For whatever reason, nobody, including myself for a number of years, really thought very much about what that means,” said Susskind. “What is the thing which is growing? That should have been one of the leading puzzles of black hole physics.”In recent years, with the rise of quantum computing, physicists have been gaining new insights about physical systems like black holes by studying their information-processing abilities—as if they were quantum computers. This angle led Susskind and his collaborators to identify a candidate for the evolving quantum property of black holes that underlies their growing volume. What’s changing, the theorists say, is the “complexity” of the black hole—roughly a measure of the number of computations that would be needed to recover the black hole’s initial quantum state, at the moment it formed. After its formation, as particles inside the black hole interact with one another, the information about their initial state becomes ever more scrambled. Consequently, their complexity continuously grows.Using toy models that represent black holes as holograms, Susskind and his collaborators have shown that the complexity and volume of black holes both grow at the same rate, supporting the idea that the one might underlie the other. And, whereas Bekenstein calculated that black holes store the maximum possible amount of information given their surface area, Susskind’s findings suggest that they also grow in complexity at the fastest possible rate allowed by physical laws.John Preskill, a theoretical physicist at the California Institute of Technology who also studies black holes using quantum information theory, finds Susskind’s idea very interesting. “That’s really cool that this notion of computational complexity, which is very much something that a computer scientist might think of and is not part of the usual physicist’s bag of tricks,” Preskill said, “could correspond to something which is very natural for someone who knows general relativity to think about,” namely the growth of black hole interiors.Researchers are still puzzling over the implications of Susskind’s thesis. Aron Wall, a theorist at Stanford (soon moving to the University of Cambridge), said, “The proposal, while exciting, is still rather speculative and may not be correct.” One challenge is defining complexity in the context of black holes, Wall said, in order to clarify how the complexity of quantum interactions might give rise to spatial volume.A potential lesson, according to Douglas Stanford, a black hole specialist at the Institute for  Advanced Study in Princeton, New Jersey, “is that black holes have a type of internal clock that keeps time for a very long time. For an ordinary quantum system,” he said, “this is the complexity of the state. For a black hole, it is the size of the region behind the horizon.”If complexity does underlie spatial volume in black holes, Susskind envisions consequences for our understanding of cosmology in general. “It’s not only black hole interiors that grow with time. The space of cosmology grows with time,” he said. “I think it’s a very, very interesting question whether the cosmological growth of space is connected to the growth of some kind of complexity. And whether the cosmic clock, the evolution of the universe, is connected with the evolution of complexity. There, I don’t know the answer.”Natalie Wolchover is a senior writer and editor at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScience and other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award, the 2016 Evert Clark/Seth Payne Award, and the American Institute of Physics’ 2017 Science Communication Award for Articles.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15593_4e5916dda041e42d18d9cf266d56b62b.jpg",
    "title": " The Case Against Geniuses",
    "description": "Posted by Brian  Gallagher on December 31, 2018  Once you’re called a “genius,” what’s left? Super genius? No, getting called a “genius” is the final accolade, the last laudatory label for anyone. At least that’s how several…",
    "category": "Culture",
    "content": "Once you’re called a “genius,” what’s left? Super genius? No, getting called a “genius” is the final accolade, the last laudatory label for anyone. At least that’s how several members of Mensa, an organization of those who’ve scored in the 98th percentile on an IQ test, see it. “I don’t look at myself as a genius,” LaRae Bakerink, a business consultant and a Mensa member, said. “I think that’s because I see things other people have done, things they have created, discovered, or invented, and I look at those people in awe, because that’s not a capability I have.”Yet the notion of genius as a capability a person can possess has come under attack recently in several ways. Megan Garber, writing in The Atlantic, criticized the excuses of bad behavior that so-called geniuses—specifically men—enjoy. “Genius, a means to godliness and its best evidence, cannot be argued with,” she writes. “Genius cannot be reasoned with. Genius is the answer and the question. It will be heard. It will be respected.” Not if Garber can help it. “The genius-bias is a strong one,” she writes, but not one we must submit to.Genius, as a noun, has always been a sort of illusion.Whether genius is partly genetic is hard to say; intelligence has a hereditary component, but its link to genius is hazy, as Hans Eysenck pointed out in his book, Intelligence: A New Look. “There is no case of a genius having a genius father—the best we can do is Mozart, whose father was a reasonably good musician, and Bach, who had several musically gifted relatives in his family, none of genius rank,” he wrote. “For the great majority, father and mother were ordinary folk, without any special gifts or achievements…” Moreover, these parents very rarely provided the sort of promising environments one might expect future geniuses to require. Genius seems to strike without warning.Earlier this year, Yuval Sharon, artistic director of The Industry in Los Angeles, whose experimental operas have won widespread acclaim, shared his anxiety about winning a 2017 MacArthur Fellowship, notoriously called the “Genius Grant.” In an essay titled “Genius as Circumstance” in the Los Angeles Review of Books, Sharon writes, “Moments, ideas, a single poem in a collection—a work of genius, no matter how individually wrought—is never the product of a single individual. We should stop thinking of genius as an attribute and instead start to think of it as a condition, a circumstance.” As a theater director, Sharon writes, “my work consists entirely of creating the conditions for genius to flow.” He defines genius as “the oxygen that those in a shared space breathe in and are transformed by; it allows them to reach their full potential. In this way, ‘genius’ returns to its original Latin meaning of an ‘attendant spirit.’”Jazz artist Vijay Iyer, another MacArthur fellow, is also uncomfortable with the conventional meaning of genius. In a Nautilus interview with Kevin Berger, Iyer says the label “genius” narrows our understanding of art and artists, and by extension, science and scientists. “The ‘G word’ is often used to shut down conversation or inquiry into a particular artist, into his or her community and connection to others,” Iyer says. “No music happens in a vacuum. Anybody who has the privilege of making music for others got there through the help of others. Even real singular talents, and I’ve known many, are nurtured and brought into a community. So I always try to understand the term relationally, understand artists in the historical, social, and political context in which they were living and working.”Both Sharon and Iyer echo the work of Hungarian-American psychologist Mihaly Csikszentmihalyi. “The location of genius is not in any particular individual’s mind, but in a virtual space, or system, where an individual interacts with a cultural domain and with a social field,” he wrote in his 2015 book, The Systems Model of Creativity. He suggests that genius, as a noun, has always been a sort of illusion. “In popular usage, ‘genius’ is sometimes used as a noun that stands by itself, yet in reality it appears always with a modifier: musical genius, mathematical genius, scientific genius, and so forth,” he wrote. “Genius cannot show itself except when garbed in a concrete symbolic form.”David Krakauer, a complexity theorist and president of the Santa Fe Institute, seems to agree. In an interview with Steve Paulson for Nautilus, he said, “I have my own favorite explanation for what genius is. If intelligence is making hard problems easy, genius is making problems go away!” For Krakauer, genius is more of an adjective that applies to solutions rather than scientists. “Genius just changes the rules of the game,” he said, referring to Albert Einstein’s theory of general relativity, and how it superseded Isaac Newton’s view, which couldn’t account for “very large masses and very large velocities.” General relativity didn’t just make our understanding of physics “better, or easier, or more efficient,” as a merely intelligent solution might, Krakauer said; it instead changed “the nature of the representation of the problem [of gravity] so completely that you get the kind of vertigo of unfamiliarity,” which is his litmus test of genius: “When you change the rules, you make a lot of people uncomfortable, and it looks a little crazy.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The meaning of genius.This classic Facts So Romantic post was originally published in May 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15619_9563122a75eef032c2a1681289515325.jpg",
    "title": " The Case for More Science and Philosophy Books for Children",
    "description": "Posted by Massimo Pigliucci on January 07, 2019  During my career as a scientist and a philosopher I have written and edited, thus far, 14 books. Of these, seven are for the general public. Of those, only one (my very first one, as it…",
    "category": "Ideas",
    "content": "During my career as a scientist and a philosopher I have written and edited, thus far, 14 books. Of these, seven are for the general public. Of those, only one (my very first one, as it turns out) was for children. The same picture emerges if one looks at the lifetime production of major science (and philosophy) popularizers, from Richard Dawkins to Stephen Jay Gould in biology, Brian Greene to Janna Levin in physics, Nigel Warburton to Rebecca Newberger Goldstein in philosophy. You might think aiming at a youthful demographic would be more appealing. Those early years, when curiosity runs high, are intellectually formative—it’s when we, my fellow educators, can hook young minds onto what philosophers call the scientific “image” of the world. Voltaire mischievously attributed to the founder of the Society of Jesus, Ignatius of Loyola, the saying, “Give me the child for the first seven years and I will give you the man.” Voltaire was probably worried about the Church brainwashing the next generation (“Écrasez l’Infâme!”—“crush the infamous”—as he used to sign his letters). But Loyola had a point, if he ever uttered those words. The ancient Greco-Romans talked about “the age of reason,” the period around the age of seven when children begin to use their rational faculties, which they saw as crucial to the moral formation of an individual, an idea that modern developmental psychology supports.For this reason, I take it as obvious that we should write science and philosophy books for children and teenagers. The issue is not to turn every kid into a future scientist. Despite all the hype about getting into STEM fields in college, our society doesn’t need millions of new PhDs in physics and biology every generation. But it does need citizens who are scientifically literate. This doesn’t just mean knowing the second law of thermodynamics—an isolated system’s entropy will increase over time—among other basic facts (though they certainly don’t hurt)! It means, more importantly, that they have developed a healthy respect for the scientific enterprise and are able to be properly critical of it when necessary. This isn’t to say that science books for children aren’t out there. An Amazon search for “evolution,” for instance, presents the following raw data: 29,840 titles under “biology” (which, to be fair, includes textbooks), but only 184 under “children & biology.” The category doesn’t even show up in conjunction with “teen & young adults,” likely indicating an even lower number of offerings. “Physics” returns 208,917 entries, only 604 of which are in the sub-category “science for kids.” These numbers certainly don’t represent a systematic sociological study of the issue, but one is sorely needed (a Google Scholar search didn’t turn up much).It is very difficult to change the mindset of adult human beings, fraught as we are with all sorts of cognitive biases that entrench our pre-existing beliefs, inoculating them from any challenging argument or inconvenient truth. The result is an adult population, for instance in the United States, where 42 percent of responders to a survey from the National Science Foundation say astrology is “sort of” or “very” scientific. The numbers were the worst for the youngest section of the surveyed population (18-24 years old)—58 percent of them thought astrology had some merit, since they didn’t choose to describe it as “not at all scientific.” Moreover, Gallup found that three out of every four Americans believe in the paranormal. Formal education, especially college, can’t be the only answer to the problem. While it is true that the percentage of people who believe that astrology is “sort of” scientific goes down from 37 percent to 20 percent if we compare people without a high school diploma with those holding a Bachelor’s degree, 20 percent is still high.Universities are inhabited by a bunch of intellectual snobs.So why is it that many science and philosophy popularizers—including yours truly—don’t make more of an effort to imitate the alleged counsel of Ignatius of Loyola? Again, I don’t have a technical study to draw from, but I can tell you about my personal experience: Writing well for children is really difficult. Take my first and so far only experience, back in 1986. It was a book of evolutionary biology for kids 8-12 years old, titled Il Romanzo della Vita (The Romance of Life). The editor, at the Italian publisher Mondadori, looked at my submission and said: “Massimo, this is very good—if you were writing for adults with a good general education. But you are writing for kids. Go back and do it again.” It took a lot of thinking to get the second version ready, and it was hard work, precisely because I don’t think like a child (at least not anymore), and it is difficult for me to even remember what it was like to be one. Maybe this partly explains why there’s a relative dearth of children’s books in science and philosophy.A second reason is, frankly, the lack of incentives, indeed the presence of strong disincentives, for academic writers. Universities are inhabited by a bunch of intellectual snobs. I know a number of people who refuse to teach introductory courses because they think these are below their dignity, and strongly prefer to teach graduate students over undergraduates. Writing for the public? Forget it. The prevalent myth is still that if you do that, you must not be a good enough scholar—evidence to the contrary be damned. Writing for children? Even worse. That probably means you don’t even know how to write for adults!This is, of course, complete nonsense. But prestige is what gets you promotions, tenure, research grants, and, of course, an ego boost—much needed in a profession where the majority of your stuff gets rejected at least once, often with accompanying (usually anonymous) vitriolic commentary by a reviewer. You are not going to give up that hard-earned prestige, assuming you have any, just because you think writing for kids may be the single most effective contribution to humanity you may be able to make.To be fair, this sort of upside-down priority reflects the attitude of society at large. Politicians and the public generally pride themselves in championing a concern for children, but elementary and middle school teachers are badly paid, badly trained, and considered the lowest in rank in a profession already mistrusted by much of the public. (I’m sure you know the joke about the fact that the people who can, do, and those who can’t, teach.). If we, as a society, were serious about our children, then children’s education—especially for those beginning “the age of reason”—would be our highest priority. The teachers of those children would be lionized like movie stars and athletes, and paid handsomely for their crucial work. But that’s a world we’ll likely never live in.Unfortunately, I don’t have a general solution to the problem, but I’m working on putting my money (well, really, my time and effort) where my mouth is. I’m co-authoring a graphic novel-style book on the practical philosophy of Stoicism with a friend who is an excellent graphic artist. I can tell you right now it’s exciting, but also very slow going. It helps that my co-author has a small child. He is witnessing the sort of situations we are thinking of exposing our characters to, like bullying, or failing a test, or not being the popular kid on the block. Maybe each of our imaginary kids will interact with one of the major Stoics, who become sort of their personal mentors and guides to the difficult life of a middle schooler. Fate permitting, as the Stoics say, it will work.What can you do to help? Buy good science and philosophy books for your child, or for a child you know. That’s how my grandfather nurtured my budding interest in astronomy first and biology later. We would take a regular walk to his favorite bookstore in downtown Rome, where I would be allowed to range freely and to come out with at least one new book that piqued my curiosity. Oh, and be kind to any teacher you know, regardless of the grade they teach. But especially the first eight.Massimo Pigliucci is the K.D. Irani Professor of Philosophy at the City College of New York. He works on evolutionary theory and the nature of pseudoscience. His new book (not for children, unfortunately) is How to Be a Stoic: Using Ancient Philosophy to Live a Modern Life. Massimo blogs at platofootnote.org.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How the Director of the Institute of Advanced Study, at Princeton, got excited about science.This classic Facts So Romantic post was originally published in July 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15564_de2d3b18505f768bdf94162f550a2c38.jpg",
    "title": "What a Newfound Kingdom Means for the Tree of Life",
    "description": "Posted by Jonathan Lambert on December 18, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.The tree of life just got another major branch. Researchers recently found a certain rare and mysterious microbe…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.The tree of life just got another major branch. Researchers recently found a certain rare and mysterious microbe called a hemimastigote in a clump of Nova Scotian soil. Their subsequent analysis of its DNA revealed that it was neither animal, plant, fungus, nor any recognized type of protozoan—that it in fact fell far outside any of the known large categories for classifying complex forms of life (eukaryotes). Instead, this flagella-waving oddball stands as the first member of its own “supra-kingdom” group, which probably peeled away from the other big branches of life at least a billion years ago.“It’s the sort of result you hope to see once in a career,” said Alastair Simpson, a microbiologist at Dalhousie University who led the study.Impressive as this finding about hemimastigotes is on its own, what matters more is that it’s just the latest (and most profound) of a quietly and steadily growing number of major taxonomic additions. Researchers keep uncovering not just new species or classes but entirely new kingdoms of life—raising questions about how they have stayed hidden for so long and how close we are to finding them all.Yana Eglit is a Dalhousie graduate student dedicated to discovering novel lineages of the single-cell eukaryotes called protists. While hiking in Nova Scotia on a cold spring day in 2016, she fell back from her friends to scrape a few grams of dirt into a plastic tube. (Such impromptu soil sampling, she said, is “a professional hazard.”) Back in the lab, Eglit soaked her sample in water, and over the next month she periodically peeked at it through a microscope for signs of unusual life.Lab members were instead stunned to find that hemimastigotes fit nowhere on the tree.Late one evening, something odd in the sample caught her eye. An elongated cell radiating whiplike flagella was “awkwardly swimming, as though it didn’t realize it had all these flagella that could help it move,” Eglit said. Under a more powerful scope, she saw it fit the description of a hemimastigote, a rare kind of protist that was notoriously hard to cultivate. The next morning, the lab was abuzz with excitement over the opportunity to describe and sequence the specimen. “We dropped everything,” she recalled.Hemimastigotes represent one of a handful of Rumsfeldian “known unknown” protist lineages—moderately well-described groups whose positions on the tree of life are not precisely known because they are difficult to culture in a lab and sequence. Protistologists have used peculiarities of hemimastigotes’ structure to infer their close relatives, but their guesses were “‘shotgunned’ all over the phylogeny,” Simpson said. Without molecular data, lineages like hemimastigotes remain orphans of unknown ancestry.But a new method called single-cell transcriptomics has revolutionized such studies. It enables researchers to sequence large numbers of genes from just one cell. Gordon Lax, another graduate student in the Simpson lab and an expert on this method, explained that for hard-to-study organisms like hemimastigotes, single-cell transcriptomics can produce genetic data of a quality previously reserved for more abundant cells, making deeper genomic comparisons finally possible.The team sequenced more than 300 genes, and Laura Eme, now a postdoctoral researcher at Uppsala University, modeled how those genes evolved to infer a classification for hemimastigotes. “We were fully expecting them to fall within one of the existing supergroups,” she explained. Lab members were instead stunned to find that hemimastigotes fit nowhere on the tree. They represented their own distinct lineage apart from the other half-dozen super groups.To understand how evolutionarily distinct the hemimastigote lineage is, imagine the eukaryotic tree splayed out before you on the ground as a narrowing set of paths, which begin with places for all living groups of eukaryotes near your toes and converge far in the distance at our common ancestor. Starting at our mammalian tip, walk down the path and back into history, past the fork where our lineage diverged from reptiles and birds, past the turnoffs for fishes, for starfish, and for insects, and then farther still, beyond the split that separates us from fungi. If you turn around and look back, all the diverse organisms you passed fall within just one of the six eukaryote supergroups. Hemimastigotes are still up ahead, in a supergroup of their own, on a path that nothing else occupies.Fabien Burki, a biologist at Uppsala University in Sweden who wasn’t involved in this study, was happy to see this result, but not entirely surprised. “It’s a bit like searching for life on other planets,” he said. “When we finally find it, I don’t think we will be very surprised, but it will be a huge discovery.”Burki, Simpson, Eglit, and many others also think we have much more of the tree of life to uncover, largely because of how quickly it’s changing. “The tree of life is being reshaped by new data. It is really quite different than even what it was 15 or 20 years ago,” Burki said. “We’re seeing a tree with many more branches than we thought.”Finding a lineage as distinct as hemimastigotes is still relatively rare. But if you go down a level or two on the hierarchy, to the mere kingdom level—the one that encompasses, say, all animals—you find that new major lineages are popping up about once a year. “That rate isn’t slowing down,” said Simpson. “If anything, it might be speeding up.”The availability of more capable sequencing technology such as single-cell transcriptomics is part of what’s driving this trend in eukaryotes, especially for known unknown groups. It empowers researchers to glean usable DNA from single specimens. But Eme cautions that these methods still require the keen eye of skilled protistologists, like Eglit, “so that we can actually target what we want to look at.”Another kind of sequencing, called metagenomics, could accelerate discovery even further. Researchers can now venture into the field, grab a sample of dirt from the trail or a biofilm from a deep-sea vent, and sequence everything in the sample. The catch is that it’s usually just a snippet of one gene. For bacteria and archaea—organisms in the two other domains of life distinct from eukaryotes—that’s usually enough to work with, and metagenomics has been behind recent huge discoveries such as the Asgard archaea, an enormous phylum of archaea totally unknown to science until about three years ago.But for eukaryotes, which tend to have larger and more complicated genomes, metagenomics is a troublesomely broad way to sample. It reveals many types of organisms that live in an environment, “but unless you have a larger known reference sequence, it’s very difficult to put these different things into an evolutionary framework,” Burki said. That’s why, according to Simpson, most of the recent, really deep eukaryotic lineages have been discovered the “old fashioned” way, through identifying a weird protist in the lab and targeting it for sequencing.“But the two methods are complementary and inform one another,” Simpson said. For example, it’s now clear that hemimastigotes popped up in previously published metagenomic databases. Yet “we just had no way of recognizing them until we had longer hemimastigote sequences to compare them to,” he said. Metagenomics can point to potential hot spots of unknown diversity, and deeper sequencing can make metagenomic data more meaningful.The future is bright for researchers cataloging diversity, in both ordinary and extraordinary environments. While metagenomic tools allow us to explore extreme environments—like the sediment near hydrothermal vents where the Asgard archaea were found—researchers can also find new lineages in their backyards. “This whole new supra-kingdom lineage was discovered by a graduate student out on a hike who happened to collect some dirt,” Burki said. “Imagine if we could scan every environment on Earth.”As scientists continue to fill out the tree, the algorithms used to add branches will only get more efficient, according to Eme. This will help researchers resolve deeper, more ancient splits in the history of life. “Our understanding of how life unfolded is still very much incomplete,” said Burki. Questions like why eukaryotes emerged or how photosynthesis evolved remain unanswered because “we don’t have a tree that is stable enough to pinpoint where these key events happened,” he said.Beyond answering such fundamental questions, the simple joy of discovery motivates researchers like Burki and Eglit. “The microbial world is a wide-open frontier,” said Eglit. “It’s thrilling to explore what’s out there.”Jonathan Lambert is an evolutionary biologist turned science writer, currently working as Quanta Magazine‘s writing intern. Before that, he was a AAAS Mass Media fellow at the Dallas Morning News, and was also an assistant producer on the science podcast for kids from American Public Media, Brains On!. He is currently a Ph.D student at Cornell University.\n\tThe newest and most popular articles delivered right to your inbox!\nLead image: A micrograph of Hemimastix kukwesjijk, the newly described hemimastigote named after a “hairy, rapacious ogre” from the traditions of the Mi’kmaq First Nation of Nova Scotia, where the specimen was collected. Credit: Courtesy of Yana Eglit "
  },
  {
    "imageUrl": "http://static.nautil.us/15682_522fc3256cfe95832ea18d7458713a5b.jpg",
    "title": "Studies Shoot Down Tech’s Harmful Effects on Kids—So Now What?",
    "description": "Posted by Jordan Shapiro on January 28, 2019  It looks like grownups can disregard the fear-mongering about the ill effects of digital media on kids. A 2017 study in Child Development found “little or no support for harmful links…",
    "category": "Ideas",
    "content": "It looks like grownups can disregard the fear-mongering about the ill effects of digital media on kids. A 2017 study in Child Development found “little or no support for harmful links between digital screen use and young people’s psychological well-being.” Parents can police their kids’ smartphone use if they like, but they should know such restrictions aren’t evidence-based. As authors Andrew Przybylski and Netta Weinstein explain, “a critical cost-benefit analysis is needed to determine whether setting firm limits constitutes a judicious use of caregiver and professional resources.”The American Academy of Pediatrics currently recommends that parents limit screen time to less than two hours for children two to five years old. But when researchers compared those who implemented these limits with those who didn’t, they found no significant difference in the level of children’s well-being. Apparently, the guidelines hadn’t been empirically evaluated before; they were based on findings which show that media use can supplant other activities, potentially leading to sedentary behaviors and/or loss of sleep.There has never been conclusive evidence that screens are a direct cause of harm. But couple correlation with the fear of change, and that’s reason enough, apparently, for grownups to treat digital devices as suspect. It turns out, the prejudice wasn’t warranted.Human beings always mediate their experiences through tools.Przybylski and Weinstein’s study—based on data collected from a representative sample of 19,957 parents who reported on their children’s levels of attachment, resilience (or bouncing back quickly from adversity), curiosity, and positive affect—found that young people who engaged in less than the AAP’s recommended limits showed “slightly higher levels of resilience but lower levels of positive affect compared to those who did not.” These differences became insignificant once the authors considered contextual factors like ethnicity, household income, and caregiver’s education level. Surprisingly, they also found “extremely small positive effects” that correlated with digital engagement at a level much higher (“up to 7/hr for both television and computer-based media”) than what’s commonly considered healthy.That’s not all: A new study in Nature Human Behaviour, which looked at data from more than 350,000 adolescents, also found that digital tech use mattered little to kids’ well-being. The authors, Amy Orben and Przybylski, argue that prior research, which examined the impact of social media on teens and tweens, was based on weak correlations and insufficiently comprehensive methods, and therefore drew false conclusions. When they analyzed the same data, they found that “the association of well-being with regularly eating potatoes was nearly as negative as the association with technology use, and wearing glasses was more negatively associated with well-being.” To put a number on it: Digital tech use explains about 0.4 percent of the variation in well-being, the researchers write. “Taking the broader context of the data into account suggests that these effects are too small to warrant policy change.”These results don’t suggest we should stop thinking about how to best raise kids to thrive in a connected world. Instead, it suggests we find a different, more useful way of discussing screen media. People usually talk about kids and technology from a medical perspective. We ask: Is it healthy or unhealthy? Even psychologists tend to use a diagnostic approach: Does social media exposure lead to anxiety or depression? Narcissism? Loneliness? Feelings of isolation?These sorts of questions tend to leave out the crucial fact that human beings always mediate their experiences through tools. In fact, toolmaking may have played a pivotal role in humanity’s cognitive development. I suspect a more optimistic story about the place of digital devices in our lives would better serve our children: Consider that we use trendy technological metaphors to describe human nature—you’re hardwired for social connection; the adolescent brain is programmed to take risks. The tools of the times, in other words, always provide the symbolic foundation for representing and understanding the human self. We urgently need to iterate the old theories of child rearing, making sure the essential values and principles—compassion, kindness, respect, integrity—remain relevant, even as the economic and technological trends change.We might borrow from preeminent thinkers like G. Stanley Hall and Jean Piaget, who both integrated epistemology, sociology, and moral philosophy into their understandings of child development. Their messages about the social, cultural, and civic benefits of child’s play, which now hide beneath a century’s worth of research, are worth resurfacing. What’s more, we can transfer what scientists now know about playgrounds to the digital sandbox. Play enhances attention inhibition, cognitive flexibility, executive function, self-regulation, and more. And while there’s still not adequate research to draw evidence-based conclusions about whether all the same benefits come from digital play, it seems unlikely that the presence of new toys would suddenly render play harmful—especially when you consider that the sandbox, the sliding board, and the teddy bear were all new toys during the industrial era.  Similarly, we can borrow from Object-Relations theorists like D.W. Winnicott. He gave us a structural model of the adolescent process of reality acceptance. He described how maturation involves learning to reconcile our internal experiences with external feedback that often feels dissonant or divergent. He explained how playful experiences in “transitional spaces” enable teens and tweens to cope with the recognition that grownup authority figures are fallible—and yet these same adults continue to be our primary role-models, icons of adulthood that contour and trigger feelings of guilt, shame, regret, pride, aspiration, and more. Could it be that Instagram, Snapchat, Twitter and TikTok play the same role in social development that sock-hops, soda shops, and 7-Eleven parking lots once did? Perhaps. But we won’t know until we try to apply Winnicott’s findings to the social media landscape.Ultimately, what matters most is that we provide our children with a sense of agency and autonomy by teaching them that tools don’t use us, we use them. Parents should stop worrying that there’s something fundamentally wrong with screen time. Sure, kids can develop unhealthy relationships with their devices, and when they do, intervention is appropriate. But when the grownups imagine digital technology as an evil temptation, an addictive inebriant, or an impenetrable coercive taskmaster, we give the tools more sovereignty than they deserve.Instead, let’s raise a generation of kids who know that they have the power to use digital technologies to manifest the best of human kindness and creativity in magnificent and unimaginable ways. Jordan Shapiro, Ph.D., is a senior fellow for the Joan Ganz Cooney Center at Sesame Workshop and Nonresident Fellow in the Center for Universal Education at the Brookings Institution. He teaches at Temple University, and he wrote a column for Forbes’ on global education and digital play from 2012 to 2017. His book, The New Childhood, was released by Little, Brown Spark in December 2018.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: What the flood of online information is doing to us. "
  },
  {
    "imageUrl": "http://static.nautil.us/15582_a7ba5241462c95541aabd6790a8072ec.jpg",
    "title": " Why Social Science Needs Evolutionary Theory",
    "description": "Posted by Cristine H. Legare on December 24, 2018  My high school biology teacher, Mr. Whittington, put a framed picture of a primate ancestor in the front of his classroom—a place of reverence. In a deeply religious and conservative…",
    "category": "Biology",
    "content": "My high school biology teacher, Mr. Whittington, put a framed picture of a primate ancestor in the front of his classroom—a place of reverence. In a deeply religious and conservative community in rural America, this was a radical act. Evolution, among the most well-supported scientific theories in human history, was then, and still is, deliberately censored from biological science education. But Whittington taught evolution unapologetically, as “the single best idea anybody ever had,” as the philosopher Dan Dennett described it. Whittington saw me looking at the primate in wonder one day and said, “Cristine, look at its hands. Now look at your hands. This is what common descent looks like.”Evolution has shaped the human body, but it also shaped the human brain, so evolutionary principles are indispensable for understanding our psychology. Yet many students, teachers, and even social scientists struggle to see how our evolutionary history significantly shapes our cognition and behavior today. “Learning” and “culture” do not explain behavior so completely that turning to ideas from evolution is unnecessary. The lack of willingness to view human cognition and behavior as within the purview of evolutionary processes has prevented evolution from being fully integrated into the social science curriculum.A deeper scientific understanding leads to the view that learning doesn’t compete with evolution as an explanation for human psychology. Learning requires evolved psychological adaptations—general learning mechanisms or mechanisms which may be specific to a particular adaptive problem. Specialized learning mechanisms help us avoid eating toxic food, yet no one is born knowing which particular foods to avoid. Humans have also evolved an aversion to mating with their genetic kin but are not born knowing who their kin are. Solving these adaptive challenges requires evolved psychological learning mechanisms.Our technological complexity is the outcome of our species’ capacity for cumulative culture.Human cognition and behavior is the product of the interaction of genetic and cultural evolution. Gene-culture co-evolution has allowed us to adapt to highly diverse ecologies and to produce cultural adaptations and innovations. It has also produced extraordinary cultural diversity. In fact, cultural variability is one of our species’ most distinctive features. Humans display a wider repertoire of behaviors that vary more within and across groups than any other animal. Social learning enables cultural transmission, so the psychological mechanisms supporting it should be universal. These psychological mechanisms must also be highly responsive to diverse developmental contexts and cultural ecologies.Take the conformity bias. It is a universal proclivity of all human psychology—even very young children imitate the behavior of others and conform to group norms. Yet beliefs about conformity vary substantially between populations. Adults in some populations are more likely to associate conformity with children’s intelligence, whereas others view creative non-conformity as linked with intelligence. Psychological adaptations for social learning, such as conformity bias, develop in complex and diverse cultural ecologies that work in tandem to shape the human mind and generate cultural variation.Truly satisfying explanations of human behavior requires identifying the components of human cognition that evolution designed to be sensitive to social or ecological conditions and information. For example, populations in which food resources show high variance (large game hunting is very much hit or miss) tend to evoke cooperative adaptations for group-wide sharing compared to those in which food variance is lower and more dependent on individual effort, like gathered foods. Recent discoveries in the field of cultural evolution have demonstrated that our technological complexity is the outcome of our species’ capacity for cumulative culture. It has set our genus Homo on an evolutionary pathway remarkably distinct from the one traversed by any other species. In a paper last year, I proposed that this was a result of psychological adaptations being universal but sufficiently flexible for innovations to build on each other, supporting the acquisition of highly variable behavioral repertoires.Applying evolutionary theory to social science has the potential to transform education and, through it, society. For example, evolutionary perspectives can help social scientists understand, and eventually address, common social problems. Schoolyard bullying provides one example. Without an evolutionary understanding of the phenomenon, interventions are likely to be ineffective, since they misdiagnose the causes of bullying. Bullying is not merely negative interpersonal behavior; it’s goal-oriented and serves the social function of gaining status and prestige for the bully, which must be understood to combat it. For example, bullying often occurs in front of an audience, suggesting that social attention drives, and may reinforce, the behavior. A 2015 paper suggests most interventions don’t work because they remove the rewards of bullying—increased social status—without offering any alternatives. The researchers recommend that the esteem bullies seek “should be borne in mind when engineering interventions” designed to either decrease a bully’s social status or channel the bully’s social motivations to better ends. A deep understanding of the evolved functions of bullying, in short, provides a fulcrum for potential remedies.If “nothing in biology makes sense except in the light of evolution,” as the evolutionary biologist Theodosius Dobzhansky argued in 1973, then nothing in human psychology, behavior, and culture does either. Social scientific research should reflect this fact.Cristine Legare is an associate professor of psychology and the director of the Evolution, Variation, and Ontogeny of Learning Laboratory at The University of Texas at Austin. Her research examines how the human cognitive system enables us to learn, create, and transmit culture. She conducts comparisons across age, culture, and species to address fundamental questions about cognitive and cultural evolution. Follow her on Twitter @CristineLegare.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The evolutionary purpose of having a teenage brain.This classic Facts So Romantic post was originally published in June 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15351_94f338fe2f7f9a84751deeefae6bcba2.jpg",
    "title": " Why Forests Give You Awe",
    "description": "Posted by Mike Shanahan on October 19, 2018  Can you remember the time when you first felt awe, that feeling of being in the presence of something immense and mind-blowing? The natural world—with its domineering mountains, colossal…",
    "category": "Biology",
    "content": "Can you remember the time when you first felt awe, that feeling of being in the presence of something immense and mind-blowing? The natural world—with its domineering mountains, colossal trees, and tall waterfalls—is one of its main sources. I felt awe first when I was a young boy at the feet of the biggest tree in the world. I felt it next as a young man, when I walked in a tropical rainforest for the first time, in Sri Lanka. Here’s how I described it in my 2016 book Gods, Wasps and Stranglers: The Secret History and Redemptive Future of Fig Trees:It hugged all it contained in a humid, humming gloom. The trees towered over us, viscerally alive yet so alien to our animal ways. Their breath sweetened the air we inhaled. It is hard to explain, but I could feel the concentration of life around me, as if its great density there had somehow reached into me physically. What struck me was the neutrality of that force. There was no malice or love there, just existence.In 2003, psychologists Dacher Keltner and Jonathan Haidt wrote that “nature-produced awe involves a diminished self, the giving way of previous conceptual distinctions (e.g., between master and servant) and the sensed presence of a higher power. Natural objects that are vast in relation to the self…are more likely to produce awe.”I asked Keltner what it might be about forests—as opposed to, say, single very large trees—that inspires feelings of awe. “I think it’s the perception of collectivity in forests,” he said, “where the eye doesn’t focus on one object but on interconnections amongst many.” That chimes with my experience. When you walk in a tropical forest, the sheer abundance and variety of life can have a powerful and somewhat disorienting effect.In 1836, in his essay Nature, Ralph Waldo Emerson hinted at this: “Standing on the bare ground, my head bathed by the blithe air and uplifted into infinite space, all mean egotism vanishes. I become a transparent eyeball; I am nothing; I see all; the currents of the Universal Being circulate through me; I am part or parcel of God.”It’s a positive experience. Keltner’s research has revealed that awe is good for our minds, bodies, and relationships. Awe may even make us kinder. It appears to encourage us to look beyond ourselves and to cooperate with others. Tapping into this power may be as simple as taking a walk in a forest; the less familiar it is, the better—Keltner and Haidt say nature is more likely to produce awe if it transcends one’s previous knowledge.Here’s how Alfred Russel Wallace, co-discoverer of evolution with Charles Darwin, described his first experience of the forests of Southeast Asia. “When, for the first time, the traveller wanders in these primeval forests, he can scarcely fail to experience sensations of awe. There is a vastness, a solemnity, a gloom, a sense of solitude and of human insignificance which for a time overwhelm him.”With so much life vibrating in one place, it is hard not to feel some connection to the rhythm.Mike Shanahan is a freelance writer with a doctorate in rainforest ecology. He has lived in a national park in Borneo, bred endangered penguins, investigated illegal bear farms, produced award-winning journalism and spent several weeks of his life at the annual United Nations climate change negotiations. He has written for numerous publications including The Economist, Nature, The Ecologist and Ensia. He maintains a blog at Under the Banyan. His book Gods, Wasps and Stranglers (Chelsea Green, 2016) will be out in paperback April 2018.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The Buddhists are right—the self is partly illusory.This classic Facts So Romantic post was originally published in March 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15496_62781274c42619851e9eddb19e2c8441_314x177.jpg",
    "title": " 5 Places Where People Slow Down Aging",
    "description": "Posted by Kaitlyn  Wells on November 27, 2018  Around the world, people are living longer, healthier lives than ever before. One area this is most visible is in the number of centenarians, or people living to the age of 100. In 1840,…",
    "category": "Culture",
    "content": "Around the world, people are living longer, healthier lives than ever before. One area this is most visible is in the number of centenarians, or people living to the age of 100. In 1840, there were 90 centenarians in the United States—one for every 189,000 people—according to United States Census Bureau records. Today, there are more than 53,000—or one for every 5,800 people. Though we know people are living longer, we don’t necessarily how they do it.After discovering that there are longevity hot spots where people tend to live especially long, writer Dan Buettner spent over a decade locating and documenting these areas, dubbed “blue zones.” “I increasingly was interested in mysteries that dealt with the human condition,” says Buettner, a National Geographic fellow.Through that research, he found several factors that might prolong health and life for people in blue zones. “Longevity is a consequence of constant, longterm little things,” Buettner says. “There’s no silver bullet.” Buettner shared the findings in his books The Blue Zones (2009) and Thrive (2010); here are some of the high points:Ikaria, Greece: Enjoy a nap after teaResidents of this Greek island are three times as likely to reach age 90 than people in the U.S. Living to nonagenarian status may be more likely when you get some bonus shut-eye.Among Ikarian elders who regularly nap in the middle of the day (90 percent of them, according to a 2011 study in Cardiology Research and Practice), none of them exhibited symptoms of depression, while many of the non-nappers did. A broader study examining the sleeping behaviors of all Greeks found that those who cat-napped for at least 30 minutes had a 37 percent lower risk of dying from heart disease than those who didn’t. Being under Sandman’s spell isn’t the only thing helping them live longer: They also regularly enjoy herbal tea. Traditional Greek teas include wild mint, which fights gingivitis and gastrointestinal disorders; rosemary for gout; and artemisia for blood circulation. Many local teas also contain mild diuretics that can treat hypertension. This may explain why Ikaria has half the rate of cardiovascular disease compared to the rest of the region.Okinawa, Japan: Maintain positive relationshipsLiving to an average of 83 years old, Japan is the nation with the highest life expectancy in the world, reports the World Health Organization. Okinawans are especially long-lived, and are three times as likely to reach 100 as Americans. Buettner attributes that feat to close ties with social networks. “Having a grounding sense of purpose is something we see in all five Blue Zones,” he says.Okinawans remain faithful to traditional Japanese culture, including an emphasis on maintaining lifelong friendships called moais, which provide emotional and social support. The moais are cultivated throughout adolescence, and the members become confidants for the rest of their lives. Just the presence of moais can be a benefit, as it increases social interaction and support. Take, for example, a study in Social Science & Medicine that examined the mental health of older adults in Japan. Those who weren’t married reported a higher positive well-being and less distress in their lives when they had social support. Simply put, the closer your friendships, the happier you’ll be. “It’s not that Americans are stuffing their face all the time, it’s that we’re eating a little too much every day—about 200 calories more than we should.”Loma Linda, California: Eat your veggiesSixty miles east of Los Angeles lies Loma Linda, where the proportion of people aged 85 or over is more than double that of the rest of California. Roughly four in 10 Loma Lindans are Seventh-day Adventists, whose adherents live longer than any other religious group in America, according to BlueZones.com. The Adventist faith endorses healthy living by discouraging smoking and alcohol consumption and encouraging exercise. A study, funded in part by the National Institutes of Health’s National Cancer Institute, examined the eating habits of 73,000 Adventist from across the U.S. and Canada. The vegetarians in the group were 12 percent less likely to die of cardiovascular disease, diabetes, and renal disorders combined compared to the non-vegetarians. Another study found that female Adventist vegetarians live four years longer, and males live seven years longer, compared to other Californias. The study concluded that regular consumption of  nuts, fruit, cereal fiber, green salad, and polyunsaturated fatty acids (such as wild salmon, safflower oil, and peanut butter) was associated with reduced mortality rates. Nicoya, Costa Rica: Drink some juiceIn Nicoya a man at age 60 has twice the chance of reaching 90 as a man living in France, Japan, or the U.S. It may have something to do with their diet. Nicoyans typically eat their biggest meal in the morning and their smallest at night, which Buettner says helps them avoid overeating. “The Blue Zones eat a lot of food, but the key is their foods are calorically less dense,” Buettner adds. “It’s not that Americans are stuffing their face all the time, it’s that we’re eating a little too much every day—about 200 calories more than we should.” Large quantities of tropical fruit, which are low in calories, are staples at meals in Nicoya. Oranges, sweet lemons, and a variety of banana are popular choices. Nicoyans also eat a couple of more distinctive fruits: maroñon, a red-orange fruit with more vitamin C than oranges, and the anona, a pear-like fruit rich in antioxidants. Some studies have shown an association between a reduced risk of cancer and diets that are high in foods that contain vitamin C, but lab tests studying use of vitamin C by itself have produced mixed results.Sardinia, Italy: Take a walkIn addition to a large number of centenarians, Sardinia is also home to a remarkably high population of sheep considering its size. The Mediterranean island’s shepherd and farming community is loaded with centenarian men who, in their younger years, regularly walked up to five miles a day across rugged terrain to tend to their shepherding duties, according to Buettner’s site, BlueZones.com. Daily trips to the grocery store are also carried out on foot, and climbing the stairs in each multi-story home provides more exercise. Kaitlyn Wells is a freelance writer based in New York City.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How one cellular biologist changed our understanding of aging.This classic Facts So Romantic post was originally published in February 2014. "
  },
  {
    "imageUrl": "http://static.nautil.us/15513_70c96c4ef725a5fd488815d60a1ec206.jpg",
    "title": "How Einstein Reconciled Religion to Science",
    "description": "Posted by Brian  Gallagher on November 30, 2018  I recently heard an echo of Albert Einstein’s religious views in the words of Elon Musk. Asked, at the close of a conversation with Axios, whether he believed in God, the CEO of both…",
    "category": "Ideas",
    "content": "I recently heard an echo of Albert Einstein’s religious views in the words of Elon Musk. Asked, at the close of a conversation with Axios, whether he believed in God, the CEO of both SpaceX and Tesla paused, looked away from his interlocutors for a brief second, and then said, in that mild South African accent, “I believe there’s some explanation for this universe, which you might call God.”Einstein did call it God. The German-Jewish physicist is famous for many things—his special and general theories of relativity, his burst of gray-white hair—including his esoteric remark, often intoned in discussions of the strange, probabilistic nature of quantum mechanics, that “God does not play dice.” A final or ultimate equation, describing the laws of nature and the origin of the cosmos, Einstein believed, could not involve chance intrinsically. Insofar as it did—it being the Copenhagen interpretation of quantum mechanics—it would be incomplete. (The consensus now among physicists is that he was wrong; God is indeterminate. “All evidence points to him being an inveterate gambler,” Stephen Hawking once said, “who throws the dice on every possible occasion.”)But what was with Einstein’s God-language in the first place? The question may be considered anew, in light of the auction at Christie’s, on Tuesday in New York, of a 1954 letter Einstein wrote that is expected to sell for up to $1.5 million. For the occasion the Princeton Club is hosting a panel discussion on the conflict, or lack thereof, between science and religion, featuring theoretical physicist Brian Greene, philosopher Rebecca Newberger Goldstein, cognitive psychologist Tania Lombrozo, and Rabbi Geoff Mitelman, founding director of Sinai and Synapses, an organization dedicated to fostering respectful dialogue about religion and science. The event, today, is open to the public, and I am excited to attend. (Full disclosure: I am a current Sinai and Synapses fellow.) I believe Einstein can still offer some insight on how to think about religion and science.“I believe in Spinoza’s God, who reveals himself in the lawful harmony of the world, not in a God who concerns himself with the fate and the doings of mankind.”What Einstein said, in a note to the philosopher Eric Gutkind, whose book Choose Life: The Biblical Call to Revolt Einstein was reviewing, was nearly as scathing as any contemporary critique of religion you might hear from Richard Dawkins, Sam Harris, or Christopher Hitchens. “The word God is for me,” Einstein wrote, “nothing more than the expression and product of human weakness, the Bible a collection of honorable, but still purely primitive, legends.* No interpretation, no matter how subtle, can change this for me.”It is no wonder why, for decades, Einstein’s views on religion became muddled in the popular imagination: The inconsistency is clear. Here, God means one thing; over there, another. Just going off his letter to Gutkind, Einstein appears to be an atheist. But read Einstein in other places and you find him directly declaring that he is not one. “I am not an Atheist,” he said in an interview published in 1930. “I do not know if I can define myself as a Pantheist. The problem involved is too vast for our limited minds.” Einstein was asked whether he was a pantheist. The rest of his response is worth quoting in full:May I not reply with a parable? The human mind, no matter how highly trained, cannot grasp the universe. We are in the position of a little child, entering a huge library whose walls are covered to the ceiling with books in many different tongues. The child knows that someone must have written those books. It does not know who or how. It does not understand the languages in which they are written. The child notes a definite plan in the arrangement of the books, a mysterious order, which it does not comprehend, but only dimly suspects. That, it seems to me, is the attitude of the human mind, even the greatest and most cultured, toward God. We see a universe marvellously arranged, obeying certain laws, but we understand the laws only dimly. Our limited minds cannot grasp the mysterious force that sways the constellations. I am fascinated by Spinoza’s Pantheism. I admire even more his contributions to modern thought. Spinoza is the greatest of modern philosophers, because he is the first philosopher who deals with the soul and the body as one, not as two separate things.Benedict Spinoza, the 17th century Jewish-Dutch philosopher, was also in his day confused for an atheist for writing things like this, from his treatise Ethics: “All things, I say, are in God, and everything which takes place takes place by the laws alone of the infinite nature of God, and follows (as I shall presently show) from the necessity of His essence.”In 1929, Einstein received a telegram inquiring about his belief in God from a New York rabbi named Herbert S. Goldstein, who had heard a Boston cardinal say that the physicist’s theory of relativity implies “the ghastly apparition of atheism.” Einstein settled Goldstein down. “I believe in Spinoza’s God, who reveals himself in the lawful harmony of the world,” he told him, “not in a God who concerns himself with the fate and the doings of mankind.”What that amounted to for Einstein, according to a 2006 paper, was a “cosmic religious feeling” that required no “anthropomorphic conception of God.” He explained this view in the New York Times Magazine: “The religious geniuses of all ages have been distinguished by this kind of religious feeling, which knows no dogma and no God conceived in man’s image; so that there can be no church whose central teachings are based on it. Hence it is precisely among the heretics of every age that we find men who were filled with this highest kind of religious feeling and were in many cases regarded by their contemporaries as atheists, sometimes also as saints. Looked at in this light, men like Democritus, Francis of Assisi, and Spinoza are closely akin to one another.”So, as Einstein would have it, there is no necessary conflict between science and religion—or between science and “religious feelings.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\n*Editor’s note: This piece originally included in Einstein’s letter a translator’s error. It has been removed. "
  },
  {
    "imageUrl": "http://static.nautil.us/15452_46c3b2e84687fd51101929492e53fced.jpg",
    "title": "World’s Oldest Fossils Now Appear to Be Squished Rocks",
    "description": "Posted by Jonathan Lambert on November 14, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.In August 2016, a research team claimed to have unearthed evidence of life in a remote outcrop of 3.7-billion-year-old…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.In August 2016, a research team claimed to have unearthed evidence of life in a remote outcrop of 3.7-billion-year-old rocks in Greenland. This bold claim not only pushed back the origin of life by at least 220 million years, it also added to a growing body of evidence that challenged the standard story of Earth’s violent beginning, as Quanta Magazine reported this year in “Fossil Discoveries Challenge Ideas About Earth’s Start.” Joining a series of ancient fossil finds—as well as geological evidence from Earth and the moon—the Greenland discovery added weight to the idea that Earth was warm and watery from the outset, and that in such conditions, life emerged quickly.But a follow-up study published in Nature last month makes the case that those Greenlandian signs of life may just be a case of squished rock and mistaken identity. In it, the authors argue that the geologic features that were taken as clear marks of life can be readily explained by the normal, lifeless workings of tectonic forces. The debate highlights the challenges in reading signs of life into relics.The controversy concerns stromatolites, which are ancient ruins of microbial activity preserved in rock. Stromatolites form in warm, shallow waters when photosynthetic microbes glob together in a gooey, flat mat, which can harden over time. New mats then grow atop the old. Over thousands of years, these mats grow into mounds that reach upward in pursuit of sunlight, thin stacks of pancakes that harden into stone.In an ideal world, the outline of an ancient stromatolite would pop out in vivid contrast to a background of rock. It would be finely layered and shaped like a pyramid, dome or perhaps even a perfect cone—a form difficult to explain without invoking biology.Alas, geology is rarely ideal. Over the course of billions of years, quotidian geologic processes can create structures embedded in rock that look a lot like stromatolites. What’s more, a once-pristine stromatolite can be squeezed, warped, crimped, and crushed so that any signal of past life becomes virtually indistinguishable from geologic noise. “The further you go back in time, the more difficult it is to glean any evidence of life from rocks,” said Elizabeth Trower, a geologist at the University of Colorado, Boulder.So are the structures identified in 2016 evidence of ancient life or not? Abigail Allwood, lead author of the newest study and an astrobiologist at NASA’s Jet Propulsion Laboratory, is in a good position to investigate. She previously described the earliest known stromatolites—3.5-billion-year-old specimens from Western Australia that are close to ideal. Their clearly defined layers are embedded within sediment indicative of prime microbe habitat. While their shapes vary, they all point upward, making it difficult to argue that brute geologic forces produced such delicate diversity. Scientists largely agree that these specimens represent our earliest record of microbial life.Initially, Allwood was excited by the 2016 finding, which was published in Nature by Allen Nutman, a geologist at the University of Wollongong in Australia, and his colleagues. She even penned a commentary praising the study.But then she noticed something odd.The proposed stromatolites were almost too perfect, with each cone sliced neatly through its apex. If they truly were stromatolites, then they were perfectly aligned in a row. “It seemed a bit miraculous that they were all perfectly exposed,” Allwood said.In search of a view from another angle, Allwood and her colleagues helicoptered to the site in Greenland. Almost immediately after finding the site, they spotted a geological red flag. Stromatolite cones should always point upward, toward the sun. Yet within a meter of Nutman’s findings, some of the stromatolites were upside down. “That alone is almost enough to refute a biogenic origin,” Allwood said.To get a fuller picture, Allwood cut a slab out of the rock less than half a meter from one of Nutman’s sites. True stromatolites should be approximately triangular from any angle. And from one angle, the putative stromatolites did resemble triangles. But when viewed from another side, they were flat, like ridges. It was like a long set of waves rolling across the sea: pointy from the side, but flat from the front. In Allwood’s mind, these ridges looked like they were “pushed and squeezed and bent and tortured,” she said, through normal tectonic processes, not the delicate action of generations of microbial mats.In addition, Allwood found that the microcrystalline structures within the stromatolites orient in the same direction as the larger fold of the ridge, evidence that’s consistent with geological, not biological, forces.The ridges don’t definitively rule out a biotic origin, according to Trower, who was not involved in either study. But invoking Occam’s razor, “it’s probably not a stromatolite,” she said.Nutman disagrees with many points made in the 2018 study—most notably, he disagrees with where Allwood looked. In a written statement to Quanta, he said Allwood focused on a clearly deformed area his team actively avoided, resulting in “a classic comparing-apples-and-oranges scenario.” Allwood counters that she extracted her sample less than half a meter from one of Nutman’s sites. Generally, Allwood argues that the 2016 study failed to take in the broader geology, leading to suspect inferences.For example, to support the claim that these stromatolites formed in an ancient shallow sea, Nutman pointed to a nearby “breccia,” a formation in which shards of rock were haphazardly encased in the sediment. He took this snapshot of geological disarray as evidence of an ancient storm that disturbed the seafloor, which would imply that the stromatolites formed in an ice-free shallow sea.Allwood agreed at the time with this interpretation, but when she examined the site herself, she noticed the breccia connected with larger formations that suggested it resulted from normal tectonic pushing and pulling.Allwood and colleagues also found fault with the chemical analysis used to back up the 2016 study’s claims. Joel Hurowitz, a geologist at Stony Brook University who led the chemical analysis in the 2018 study, did not find the fine internal layers expected in a stromatolite.The debate over the Greenland stromatolites captures the inherent difficulty in peering so far into the past, said Kurt Konhauser, a geologist at the University of Alberta. “Imagine that you’re on the beach today, and you see bacteria forming a microbial mat. Now that then gets buried over 3 billion years, up to 10 kilometers deep. It gets folded, pushed up, fluids run through it, with minerals dissolving and re-precipitating. Is it reasonable to assume that what you’d see 3 billion years from now would look like how it looks today?” he said. “That’s the problem we have.”Trower is slightly more optimistic, agreeing that this work is difficult, but that getting a more detailed, three-dimensional picture of the putative stromatolites and the surrounding geology “would be really helpful in getting more insight into what these structures originally were and how they got there.”Both Trower and Konhauser are glad to see a negative result end up in Nature. “We’ve had so many affirmative statements about early life put forward,” Konhauser said, “but then you have to test it.” Sometimes the findings stand up to the test, and sometimes they don’t. “That’s the way you move forward.”Jonathan Lambert is an evolutionary biologist turned science writer, currently working as Quanta Magazine‘s writing intern. Before that, he was a AAAS Mass Media fellow at the Dallas Morning News, and was also an assistant producer on the science podcast for kids from American Public Media, Brains On!. He is currently a Ph.D student at Cornell University.Lead image: The putative stromatolites appear as ridges in an outcropping taken from Greenland. While most of the ridges point up—consistent with forms that grow upward from the sea floor—one points down (highlighted with yellow arrow), indicating a non-biological origin. "
  },
  {
    "imageUrl": "http://static.nautil.us/15494_cc6a6632b380f3f6a1c54b1222cd96c2.jpg",
    "title": "Butterfly Wonk Robert Pyle Pens His First Novel 44 Years in the Making",
    "description": "Posted by Mary Ellen Hannibal on November 21, 2018  The acclaimed author, naturalist, and ecologist Robert Michael Pyle has been investigating the butterfly for about 60 years. In that time his prolific output has included several butterfly…",
    "category": "Ideas",
    "content": "The acclaimed author, naturalist, and ecologist Robert Michael Pyle has been investigating the butterfly for about 60 years. In that time his prolific output has included several butterfly field guides, chronicles of his adventures in the great outdoors (Where Bigfoot Walks, Thunder Tree, and Mariposa Road among them), and the magnum opus Nabokov’s Butterflies, a collection of the novelist’s butterfly writings. Pyle has also made seminal contributions to butterfly research, including his establishment of the Western Monarch’s migration pathway across the U.S. and into Mexico. Forty-seven years ago, he foresaw the acceleration of insect extinctions and founded the Xerces Society, devoted to invertebrate conservation.This year marked a first for 71-year old Pyle, with the publication of his long-awaited first novel, Magdalena Mountain, nearly half a century in the making. For those of us devoted to his nonfiction, Pyle’s imaginative treatment of the nonhuman world—he includes a butterfly and a mountain among his cast of characters—is both a surprise and perhaps a natural result of his artistic development. Like Nabokov, Pyle has consistently straddled the worlds of science and the imagination.Pyle caught up with Nautilus recently to chat about his work and the way butterflies shaped his early life.Is Magdalena Mountain your first foray into fiction?Fiction isn’t entirely new to me—I’ve published some short stories and poetry. I worked on this novel for 44 years. It went through ten drafts. I started it at Yale, while working on my dissertation, which was about the distribution of Washington State butterflies with respect to conserved land. It was a “gap analysis” before that term came into being. It was deeply enjoyable but very intense intellectually, and left-brain. I was inclined to write something creative at the same time.What inspired you to make a butterfly and a mountain into fictional characters?The idea came from a book called Wings in the Meadow, published by Jo Brewer in 1967. Brewer was a wonderful teacher, and her young adult novel was about a single Monarch butterfly and his migration—she called him Daneaus. I thought, I’ll write about a butterfly too, but about the Erebia Magdalena, which I had fallen in love with.Tell us about your love affair with butterflies.I started to notice butterflies in the spring I turned 11. That year, my father re-married and I acquired a step-mother and a step-brother and didn’t get along with either. But my step-mother’s family had a cabin in Crested Butte, Colorado. It was not remotely fancy then (it is a ski resort now). It was a crusty old coal town with cheap, crappy cabins. It happened to be a butterfly Valhalla. My father and I would escape the family’s endless smoky bridge games by going fishing. Luckily he never really taught me to tie flies or I might’ve gotten into it. As soon as I discreetly could, I would put my pole down and pick up my butterfly net. I would go up into these meadows full of skippers and fritillaries. I developed a passion for the satyrs, the wood nymphs. I liked their soft, striated browns and their eye spots. The Erebia Magdalena was the ultimate, the greatest among them. Something about it being black really captured me.And what drew you to center a mountain in your story?Thomas Hardy’s The Return of the Native was an inspiration. The first six pages of the book take place before a human steps on the page. Landscape is character. It is the place where things happen. One day in 1959, I looked down over a ridge and I saw about 100 people with butterfly nets. Huh? I uttered a childish version of “what the fuck?” I’d never seen anyone else with nets. I had been reading about butterflies and recognized Charles Remington and Paul Ehrlich. The living gods of lepidoptera! They were in their 30s, just kids—out with grad students near the Rocky Mountain Biological Lab. I was pathologically shy but somehow emboldened. I asked these guys if they knew where I could find the Magdalena—they told me there was a big rockslide in back of Ehrlich’s cabin where I would find them. And they were very generous with me, let me go butterfly hunting with them, sometimes even driving to pick me up. (Editor’s note: Remington convinced Pyle to apply to Yale, where he became Pyle’s mentor. Ehrlich went on to write The Population Bomb, with his wife Anne. Remington and Ehrlich both appear in Magdalena Mountain.)“Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?”The novel contains several storylines. There’s the journey of a woman who loses her memory and believes she is Mary Magdalene incarnate. A budding scientist struggles with a difficult past and graduate school. And a group of men seeking a return to pantheism convene a community called the Grove in the mountains. Did these narratives develop at different times, in different drafts, over the 44 years you were writing the novel?Yes, I did. I wanted to do this experiment. The butterfly was always at the center of the story and would drive the story, but also one of a cast. While the book is not a roman à clef, a butterfly makes things happen in the book, as it has made things happen in my life. The Magdalenian part is pseudo-religious, developed around the name of the actual black butterfly, but the story is grounded in reality. Researching Mary Magdalene—the most-written about woman in the Bible—was fascinating for me. How the mystery around the name “Magdalena” resolves is all true, as is the entomological research story. I had a lot of fun with it all.Vladimir Nabokov famously hunted butterflies in Colorado, references to which are woven throughout the book.For nine years I worked on compiling and annotating Nabokov’s butterfly writings, with his biographer Brian Boyd, and his son, Dmitri Nabokov. During the process I discovered Nabokov’s wonderful question, “Does there not exist a high ridge where the mountainside of ‘scientific knowledge’ joins the opposite slope of ‘artistic imagination’?” It inspired the title of my book Walking the High Ridge: Life as Field Trip. But there was more. In Look at the Harlequins, Nabokov has Vadim and Bel stay at Lupine Lodge, which was surely modeled on Columbine Lodge where Nabokov stayed in 1947. Bel scribbles a poem: “Longs’ Peacock Lake: / the Hut and its Old Marmot; / Boulderfield and its Black Butterfly; / And the intelligent trail.” The black butterfly! My Erebia! And of course it was near Telluride that Humbert realized the moral consequences of his crimes in Lolita.How do you use Nabokov’s idea of a “high ridge” between art and science as an educator?When I teach environmental writing I use Nabokov’s quote to build a model. I start by trying to establish the difference between the mirror-like reflection of reality sought by science and journalism and the refractive aim of those writing essays, fiction and poetry. C.P. Snow’s Two Cultures metaphorically fall on either side of a mountain, with subjective, lyrical imagination on one side, and objective analysis on the other. This becomes a model of the bicameral mind. I’m not a brain scientist, but the region of the brain lying between the hemispheres is the corpus callosum and it integrates the two. Now we’re stepping into metaphor. Essayists are the true ice skaters who need to get across. But why should any of us operate with only one set of tools? That’s like having one hand tied behind your back.It would seem that fiction allows a writer to use time differently. Nonfiction is mostly constrained by the linear necessity of then and now, perhaps looking into the future. In the novel, an ancient figure like Mary Magdalene can seamlessly play a present role.Yes, that’s right. And the long time-frame of a mountain’s life, as well as the cyclical life over time of butterfly ecology, can be integrated into the present time experienced by human characters. Fiction allowed me to intermingle the life of Erebia and its kind with the human cast, thus further smearing the invidious line between humans and the rest of nature. People are as much a vehicle for moving the butterfly plot as the reverse. I like their inextricability in this story. Fiction allows for a certain kind of wholeness.Mary Ellen Hannibal is most recently the author of Citizen Scientist: Searching for Heroes and Hope in an Age of Extinction, and a Stanford media fellow.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15354_e1740c5223422374b7fb1888478aabb9.jpg",
    "title": "Can Analogies Reveal the Laws of Physics?",
    "description": "Posted by Natalie Wolchover on October 17, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.Hoping to gain insight into domains of nature that lie beyond experimental reach—the interiors of black holes, the…",
    "category": "Ideas",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Hoping to gain insight into domains of nature that lie beyond experimental reach—the interiors of black holes, the subtleties of the quantum realm, the Big Bang—physicists are experimenting on “analogue” systems made of fluids and other easily manipulable materials that can be modeled by similar equations. Results from these analogue experiments often end up in top scientific journals, with a sense that they say something about the systems of interest. But do they? And how do we know?As Stephan Hartmann, philosopher of physics at Ludwig Maximilian University in Munich put it, “Under which conditions can evidence that we obtain here in a certain experiment confirm or support claims about a different system, which is far away?”The issue keeps coming up.In 2014, researchers reported in Nature that they had discovered a particle-like state in a fluid of supercold rubidium atoms that is analogous to a magnetic monopole—a long-sought, hypothetical elementary particle that would act like one end of a magnet. One physicist quoted in Nature News deemed the discovery “one more reason why the magnetic monopole as a fundamental particle should exist.” But is it really?The task now is for scientists to figure out how to interpret this new breed of experiment.In a case that I wrote about in Quanta, a physicist in Israel created a fluid that traps sound much as a black hole traps light; he then detected an effect in this “sonic black hole” that is analogous to Hawking radiation, a hypothetical black hole phenomenon predicted by Stephen Hawking in 1974 with profound consequences for how the universe works. Does the tabletop experiment provide indirect evidence for Hawking’s prediction?Hartmann argues that experimenting on sonic black holes may indeed shed light on real ones because there might be a “common cause” underlying their similar mathematics. In the same way, yellow fingers and heart disease are both caused by smoking, and detecting one can be evidence of the other. On the flip side, many black hole experts put no stock in the analogy and consider it potentially misleading, since it isn’t known whether Hawking’s math, upon which the analogy is based, actually does describe black holes.Either way, the situation “raises wonderful philosophical questions,” Hartmann said, “because we learn about new types of evidence.” Increasingly, he said, physics theories like string theory and the multiverse describe realms of nature that are inaccessible to experimenters. Direct tests of such theories appear impossible. “So we have to think about alternative ways of testing theories. And whatever we think about these analogue experiments at the moment, I think these works go exactly in the right direction.”People are trying to find indirect evidence for scientific theories, he said. “We need more of this, and we will surely see more of this in the future.”The task now is for scientists to figure out how to interpret this new breed of experiment.  “There’s an intuition that there’s something different about doing an analogue simulation, to doing a computer simulation, to doing a direct experiment, to doing a calculation,” said Karim Thebault, a philosopher of physics at the University of Bristol who has written about the sonic black hole experiments. “But what that difference is isn’t always explained clearly enough.”Natalie Wolchover is a senior writer at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScienceand other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award and the 2016 Evert Clark/Seth Payne Award for young science journalists. @NattyOver\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15356_bda989b1330ba4f420924a5be0ebcb61.jpg",
    "title": "Are Black Holes Actually Dark Energy Stars?",
    "description": "Posted by Jesse Stone on October 15, 2018  What does the supermassive black hole at the center of the Milky Way look like? Early next year, we might find out. The Event Horizon Telescope—really a virtual telescope with an effective…",
    "category": "Matter",
    "content": "What does the supermassive black hole at the center of the Milky Way look like? Early next year, we might find out. The Event Horizon Telescope—really a virtual telescope with an effective diameter of the Earth—has been pointing at Sagittarius A* for the last several years. Most researchers in the astrophysics community expect that its images, taken from telescopes all over the Earth, will show the telltale signs of a black hole: a bright swirl of light, produced by a disc of gases trapped in the black hole’s orbit, surrounding a black shadow at the center—the event horizon. This encloses the region of space where the black-hole singularity’s gravitational pull is too strong for light to escape.But George Chapline, a physicist at the Lawrence Livermore National Laboratory, doesn’t expect to see a black hole. He doesn’t believe they’re real. In 2005, he told Nature that “it’s a near certainty that black holes don’t exist” and—building on previous work he’d done with physics Nobel laureate Robert Laughlin—introduced an alternative model that he dubbed “dark energy stars.” Dark energy is a term physicists use to describe a peculiar kind of energy that appears to permeate the entire universe. It expands the fabric of spacetime itself, even as gravity attempts to bring objects closer together. Chapline believes that the immense energies in a collapsing star cause its protons and neutrons to decay into a gas of photons and other elementary particles, along with what he refers to as “droplets of vacuum energy.” These form a “condensed” phase of spacetime—much like a gas under enough pressure transitions to liquid—that has a much higher density of dark energy than the spacetime surrounding the star. This provides the pressure necessary to hold gravity at bay and prevent a singularity from forming. Without a singularity in spacetime, there is no black hole.The idea has found no support in the astrophysical community—over the last decade, Chapline’s papers on this topic have garnered only single-digit citations. His most popular paper in particle physics, by contrast, has been cited over 600 times. But Chapline suspects his days of wandering in the scientific wilderness may soon be over. He believes that the Event Horizon Telescope will offer evidence that dark energy stars are real.This strange toroidal geometry isn’t a bug of dark energy stars, but a feature.The idea goes back to a 2000 paper, with Evan Hohlfeld and David Santiago, in which Chapline and Laughlin modeled spacetime as a Bose-Einstein condensate—a state of matter that arises when taking an extremely low-density gas to extremely low temperatures, near absolute zero. Chapline and Laughlin’s model is quantum mechanical in nature: General relativity emerges as a consequence of the way that the spacetime condensate behaves on large scales. Spacetime in this model also undergoes phase transformations when it gains or loses energy. Other scientists find this to be a promising path, too. A 2009 paper by a group of Japanese physicists stated that “[Bose-Einstein Condensates] are one of the most promising quantum fluids for” analogizing curved spacetime.Chapline and Laughlin argue that they can describe the collapsed stars that most scientists take to be black holes as regions where spacetime has undergone a phase transition. They find that the laws of general relativity are valid everywhere in the vicinity of the collapsed star, except at the event horizon, which marks the boundary between two different phases of spacetime.In the condensate model the event horizon surrounding a collapsed star is no longer a point of no return but instead a traversable, physical surface. This feature, along with the lack of a singularity that is the signature feature of black holes, means that paradoxes associated with black holes, like the destruction of information, don’t arise. Laughlin has been reticent to conjecture too far beyond his and Chapline’s initial ideas. He believes Chapline is onto something with dark energy stars, “but where we part company is in the amount of speculating we are willing to do about what ‘phase’ of the vacuum might be inside” what most scientists call black holes, Laughlin said. He’s holding off until experimental data reveals more about the interior phase. “I will then write my second paper on the subject,” he said.In recent years Chapline has continued to refine his dark energy star model in collaboration with several other authors, including Pawel Mazur of the University of South Carolina and Piotr Marecki of Leipzig University. He’s concluded that dark energy stars aren’t spherical or oblate, like black holes. Instead, they have the shape of a torus, or donut. In a rotating compact object, like a dark energy star, Chapline believes quantum effects in the spacetime condensate generate a large vortex along the object’s axis of rotation. Because the region inside the vortex is empty—think of the depression that forms at the center of whirlpool—the center of the dark energy star is hollow, like an apple without its core. A similar effect is observed when quantum mechanics is used to model rotating drops of superfluid. There too, a central vortex can form at the center of a rotating drop and, surprisingly, change its shape from a sphere to a torus.  In the condensate model the event horizon surrounding a collapsed star is no longer a point of no return but instead a traversable, physical surface.For Chapline, this strange toroidal geometry isn’t a bug of dark energy stars, but a feature, as it helps explain the origin and shape of astrophysical jets—the highly energetic beams of ionized matter that are generated along the axis of rotation of a compact object like a black hole. Chapline believes he’s identified a mechanism in dark energy stars that explains observations of astrophysical jets better than mainstream ones, which posit that energy is extracted from the accretion disk outside of a black hole and focused into a narrow beam along the black hole’s axis of rotation. To Chapline, matter and energy falling toward a dark energy star would make its way to the inner throat (the “donut hole”), where electrons orbiting the throat would, as in a Biermann Battery, generate magnetic fields powerful enough to drive the jets.Chapline points to recent experimental work where scientists, at the OMEGA Laser Facility at the University of Rochester, created magnetized jets using lasers to form a ring-like excitation on a flat surface. Though the experiments were not conducted with dark energy stars in mind, Chapline believes it provides support for his theory since the ring-like excitation—Chapline calls it a “ring of fire”—is exactly what he would expect to happen along the throat of a dark energy star. He believes the ring could be the key to supporting the existence of dark energy stars. “This ought to eventually show up clearly” in the Event Horizon Telescope images, Chapline said, referring to the ring. Chapline also points out that dark energy stars will not be completely opaque to light, as matter and light can pass into, but also out of, a dark energy star. A dark energy star won’t have a completely black interior—instead it will show a distorted image of any stars behind it. Other physicists, though, are skeptical that these kinds of deviations from conventional black hole models would show up in the Event Horizon Telescope data. Raul Carballo-Rubio, a physicist at the International School for Advanced Studies, in Trieste, Italy, has developed his own alternative model to black holes known as semi-classical relativistic stars. Speaking more generally about alternative black hole models Caraballo-Rubio said, “The differences [with black holes] that would arise in these models are too minute to be detected” by the Event Horizon Telescope.Chapline plans to discuss his dark energy star predictions in December, at the Kavli Institute for Theoretical Physics in Santa Barbara. But even if his predictions are confirmed, he said he doesn’t expect the scientific community to become convinced overnight. “I expect that for the next few years the [Event Horizon Telescope] people will be confused by what they see.” Jesse Stone is a freelance writer based in Iowa City, Iowa. Reach him at jessebstone@gmail.com.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why the public is so fascinated by black holes. "
  },
  {
    "imageUrl": "http://static.nautil.us/15490_206c42ea72f6cc6dab4a283f8b063cfe.jpg",
    "title": "The Present Phase of Stagnation in the Foundations of Physics Is Not Normal",
    "description": "Posted by Sabine Hossenfelder on November 23, 2018  Nothing is moving in the foundations of physics. One experiment after the other is returning null results: No new particles, no new dimensions, no new symmetries. Sure, there are…",
    "category": "Matter",
    "content": "Nothing is moving in the foundations of physics. One experiment after the other is returning null results: No new particles, no new dimensions, no new symmetries. Sure, there are some anomalies in the data here and there, and maybe one of them will turn out to be real news. But experimentalists are just poking in the dark. They have no clue where new physics may be to find. And their colleagues in theory development are of no help.Some have called it a crisis. But I don’t think “crisis” describes the current situation well: Crisis is so optimistic. It raises the impression that theorists realized the error of their ways, that change is on the way, that they are waking up now and will abandon their flawed methodology. But I see no awakening. The self-reflection in the community is zero, zilch, nada, nichts, null. They just keep doing what they’ve been doing for 40 years, blathering about naturalness and multiverses and shifting their “predictions,” once again, to the next larger particle collider.I think stagnation describes it better. And let me be clear that the problem with this stagnation is not with the experiments. The problem is loads of wrong predictions from theoretical physicists.The problem is also not that we lack data. We have data in abundance. But all the data are well explained by the existing theories—the standard model of particle physics and the cosmological concordance model. Still, we know that’s not it. The current theories are incomplete.We know this both because dark matter is merely a placeholder for something we don’t understand, and because the mathematical formulation of particle physics is incompatible with the math we use for gravity. Physicists knew about these two problems already in 1930s. And until the 1970s, they made great progress. But since then, theory development in the foundations of physics has stalled. If experiments find anything new now, that will be despite, not because of, some ten-thousands of wrong predictions.We are today making more investments into the foundations of physics than ever before. And yet nothing is coming out of it.Ten-thousands of wrong predictions sounds dramatic, but it’s actually an underestimate. I am merely summing up predictions that have been made for physics beyond the standard model which the Large Hadron Collider (LHC) was supposed to find: All the extra dimensions in their multiple shapes and configurations, all the pretty symmetry groups, all the new particles with the fancy names. You can estimate the total number of such predictions by counting the papers, or, alternatively, the people working in the fields and their average productivity.They were all wrong. Even if the LHC finds something new in the data that is yet to come, we already know that the theorists’ guesses did not work out. Not. A. Single. One. How much more evidence do they need that their methods are not working?This long phase of lacking progress is unprecedented. Yes, it has taken something like two-thousand years from the first conjecture of atoms, by Democritus, to their actual detection. But that’s because for most of these two-thousand years, people had other things to do than contemplating the structure of elementary matter. Like, for example, how to build houses that don’t collapse on you. For this reason, quoting chronological time is meaningless. We should better look at the actual working time of physicists.I have some numbers for you on that too. Oh, yes, I love numbers. They’re so factual.According to membership data from the American Physical Society and the German Physical Society, the total number of physicists has increased by a factor of roughly 100 between the years 1900 and 2000.* Most of these physicists do not work in the foundations of physics. But for what publication activity is concerned the various subfields of physics grow at roughly comparable rates. And (leaving aside some bumps and dents around the second world war) the increase in the number of publications, as well as in the number of authors, is roughly exponential.It’s not institutional pressure that creates this resistance, it’s that scientists themselves don’t want to move their butts.Now let us assume for the sake of simplicity that physicists today work as many hours per week as they did 100 years ago—the details don’t matter all that much given that the growth is exponential. Then we can ask: How much working time starting today corresponds to, say, 40 years working time starting 100 years ago. Have a guess!Answer: About 14 months. Going by working hours only, physicists today should be able to do in 14 months what a century earlier took 40 years.Of course you can object that progress doesn’t scale that easily, for despite all the talk about collective intelligence, research is still done by individuals. This means processing time can’t be decreased arbitrarily by simply hiring more people. Individuals still need time to exchange and comprehend each other’s insights. On the other hand, we have also greatly increased the speed and ease of information transfer, and we now use computers to aid human thought. In any case, if you want to argue that hiring more people will not aid progress, then why hire them?So, no, I am not serious with this estimate, but it explains why the argument that the current stagnation is not unprecedented is ill-informed. We are today making more investments into the foundations of physics than ever before. And yet nothing is coming out of it. That’s a problem and it’s a problem we should talk about.I’ve recently been told that the use of machine learning to analyze LHC data signals a rethinking in the community. But that isn’t so. To begin with, particle physicists have used machine learning tools to analyze data for at least three decades. They use it more now because it’s become easier, and because everyone does it, and because Nature News writes about it. And they would have done it either way, even if the LHC would have found new particles. So, no, machine learning in particle physics is not a sign of rethinking.Another comment-not-a-question I constantly have to endure is that I supposedly only complain but don’t have any better advice for what physicists should do. First, it’s a stupid criticism that tells you more about the person criticizing than the person being criticized. Consider I was criticizing not a group of physicists, but a group of architects. If I inform the public that those architects spent 40 years building houses that all fell to pieces, why is it my task to come up with a better way to build houses?They constantly tell each other that what they are doing is good science. Why should they stop?Second, it’s not true. I have spelled out many times, very clearly, what theoretical physicists should do differently. It’s just that they don’t like my answer. They should stop trying to solve problems that don’t exist. That a theory isn’t pretty is not a problem. Focus on mathematically well-defined problems, that’s what I am saying. And, for heaven’s sake, stop rewarding scientists for working on what is popular with their colleagues.I don’t take this advice out of nowhere. If you look at the history of physics, it was working on the hard mathematical problems that led to breakthroughs. If you look at the sociology of science, bad incentives create substantial inefficiencies. If you look at the psychology of science, no one likes change.Developing new methodologies is harder than inventing new particles in the dozens, which is why they don’t like to hear my conclusions. Any change will reduce the paper output, and they don’t want this. It’s not institutional pressure that creates this resistance, it’s that scientists themselves don’t want to move their butts.How long can they go on with this, you ask? How long can they keep on spinning theory-tales? I am afraid there is nothing that can stop them. They review each other’s papers. They review each other’s grant proposals. And they constantly tell each other that what they are doing is good science. Why should they stop? For them, all is going well. They hold conferences, they publish papers, they discuss their great new ideas. From the inside, it looks like business as usual, just that nothing comes out of it.This is not a problem that will go away by itself.Sabine Hossenfelder is a Research Fellow at the Frankfurt Institute for Advanced Studies where she works on physics beyond the standard model, phenomenological quantum gravity, and modifications of general relativity. If you want to know more about what is going wrong with the foundations of physics, read her book Lost in Math: How Beauty Leads Physics Astray.\n\tThe newest and most popular articles delivered right to your inbox!\nThis post was originally published on BackRe(Action), Hossenfelder’s blog, and is reprinted with permission.*That’s faster than the overall population growth, meaning the fraction of physicists, indeed of scientists of general, has increased. "
  },
  {
    "imageUrl": "http://static.nautil.us/15519_3b8d83483189887a2f1a39d690463a8f.jpg",
    "title": " Dear iPhone—It Was Just Physical, and Now It’s Over",
    "description": "Posted by Katie Reid on December 05, 2018  As a kid, I’d sometimes try to imagine what life would be like without a particular sense or part of my body, like with questions from the Would You Rather? game. Would you rather be…",
    "category": "Culture",
    "content": "As a kid, I’d sometimes try to imagine what life would be like without a particular sense or part of my body, like with questions from the Would You Rather? game. Would you rather be deaf or blind? Would you rather have no legs or no arms? I’d try to erase the sound of my mom’s piano playing, the sight of the ground growing smaller as I soared on the tree swing in my backyard, or the feeling of playing basketball so hard my lungs might explode, but I just couldn’t. How could life go on without these sensations that were so tied to my idea of what it meant to be alive?I guess I’ve been feeling extra contemplative and nostalgic these days because I recently went through a pretty significant break-up…with my smartphone. My relationship with my phone was unhealthy in a lot of ways. I don’t remember exactly when I started needing to hold it during dinner or having to check Twitter before I got out of bed in the morning, but at some point I’d decided I couldn’t be without it. I’d started to notice just how often I was on my phone—and how unpleasant much of that time had become—when my daughter came along, and, just like that, time became infinitely more precious. So, I said goodbye. Now, as I reflect on the almost seven years my smartphone and I spent together, I’m starting to realize: What I had with my phone was largely physical.Cognitive scientists have long debated whether objects in our environment can become part of us. Philosophers Andy Clark and David Chalmers argued in their 1998 paper “The Extended Mind” that when tools help us with cognitive tasks, they become part of us—augmenting and extending our minds. Today the idea that phones specifically are extensions of ourselves is receiving a lot of recent attention. In February, in Aeon, philosopher Karina Vold explored the legal implications of applying the extended mind theory to our smartphones. If the extended mind view is correct, she writes, then smartphones would merit recognition “as a part of the essential toolkit of the mind.” In a fascinating New Yorker profile this year of Clark, Larissa MacFarquhar wrote that Clark thinks “we are all cyborgs, in the most natural way. Without the stimulus of the world, an infant could not learn to hear or see, and a brain develops and rewires itself in response to its environment throughout its life. Any human who uses language to think with has already incorporated an external device into his most intimate self, and the connections only proliferate from there.” For Clark, MacFarquhar continues, “The more devices and objects there are available to foster better ways of thinking, the happier he is.”I agree with the theory, if not Clark’s sunny outlook on its implications. (More on that later.) However, when it comes to the most widely useful of modern-day tools—the smartphone—both of these recent articles overlook a key component of the extended self: embodiment. Our devices aren’t just extensions of our minds, they’re extensions of our bodies too.Clark ventures into embodiment in his 2008 book, Supersizing the Mind, in which he spends half of the first chapter discussing how bodies and senses adapt to external technology. From a monkey learning to master a robotic arm to the familiar process of “body babbling,” in which infants learn, through practice, how neural commands control certain bodily movements, Clark shows that the ability to incorporate new objects into our bodies is part of how we’re designed:Because bodily growth and change continue, it is simply good design not to permanently lock in knowledge of any particular configuration but instead to deploy plastic neural resources and an ongoing regime of monitoring and recalibration.I experienced that recalibration when I got my first smartphone in 2011. In those first few weeks, it had a particular kind of novelty about it—merely owning it was obvious and unnatural. That solid, black iPhone 4 felt a bit unwieldy in my palm at the beginning, and pulling it out to Google something at dinner with my friends or to take a picture of something on the street was noticeable. Soon after, though, it went from being a bulky accessory to a predictable character trait, gradually and quietly gaining entry to a more intimate part of myself—a part fueled largely by instinct. When I was waiting in line at the grocery store or sitting alone at the bus stop, I reached for my phone without even thinking. That soothing scroll through Instagram or Twitter became much like a 21st century finger-tapping or knee-jiggling.It had become, in Clark’s words, “transparent equipment.” And the physiological effects of losing that equipment were acute: my heart began to race in the Verizon store when the employee told me he was deactivating my phone, and in the following hours and days, I would frequently find myself reaching for my iPhone, the way a girl reaches for a non-existent ponytail after a drastic haircut. Of course, I would gradually begin to notice not being able to use Google Maps or post to Instagram, but the physical sense of loss was instantaneous and intense. I literally felt a part of me was missing.My smartphone obviously helped me with a great number of cognitive tasks. It communicated with my friends. It managed my finances. It delivered work emails. It alerted me to emergencies in the area. It reminded me of appointments. It captured and stored memories. But this sudden and overwhelming awareness of its physical absence indicated that it had become just as important to my body as it had to my mind. If I’m honest, much of what I did on my phone could be characterized as mindless. I can’t count the number of times I pulled out my phone just for the feeling of unlocking the screen and swiping through applications, whether out of comfort—like a baby sucking her thumb—or boredom—like a teenager at school, tapping his fingers on a desk. In those cases, I sought not mental stimulation, but physical release.While Clark celebrates the idea of innovative devices ushering in new possibilities for the mind, I find myself wondering what we might be giving up along the way. If personal technology is improving the world of thought, what is it doing to the world of our moving, breathing bodies? Clark may see a smartphone extending my mind, but I could feel it dulling my senses.Without my phone, I’m more fully myself, both in mind and body. And now, more than ever, I know that looking at my phone is nothing compared to looking at my daughter while the room sways as I rock her to sleep, or how shades of indigo and orange pour in through the window and cast a dusky glow over her room, or the way her warm, milky breath escapes in tiny exhalations from her lips, or how the crickets outside sing their breathless, spring lullaby. See, once I looked up from my phone, I remembered that each experience could be a symphony for the senses, just like it had been when I was a child and, thank God, there was no such thing as smartphones.Katie Reid is director of digital media at The Boys’ Latin School of Maryland and an MFA candidate in integrated design at University of Baltimore. Her email is kreid@boyslatinmd.com.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: All technological innovations force a crisis.This classic Facts So Romantic post was originally published in May 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15400_a3014fc356e77277f42c498ad7f1c158.png",
    "title": "12 Mind-Bending Perceptual Illusions",
    "description": "Posted by Steve Stewart-Williams on October 26, 2018  Everyone loves a good optical illusion. Most people first come across them as kids, and are instantly transfixed. And most of us never quite outgrow them. Even cats seem to enjoy the occasional…",
    "category": "Culture",
    "content": "Everyone loves a good optical illusion. Most people first come across them as kids, and are instantly transfixed. And most of us never quite outgrow them. Even cats seem to enjoy the occasional optical illusion!The good news, then, for humans and nonhumans alike, is that our illusions seem to be getting better over time. In the age of social media, lots of people are making and sharing them, and the best ones are quickly going viral and setting the new standard. In effect, our illusions are evolving culturally to be more and more powerful.But although perceptual illusions are fun, they also have important philosophical implications. They show us in a clear and unambiguous way that we don’t directly perceive the world around us. Perceptual experience is a simulation—a mental model—that doesn’t always correspond to the reality it aims to depict.The following illusions are some of my favorites. Enjoy!1. The Power of Top-Down ProcessingTo get the ball rolling, here’s a good example of how expectations guide perception.We’re so used to words being arranged in a certain order that, when the words are jumbled up, we often don’t notice: We mistake our expectations for the world. That’s why proofreading your own essays is so hard.2. The Skye Blue Café Wall IllusionThis illusion, created by the artist Victoria Skye, was one of the top entries in the 2017 Best Illusion of the Year Contest. Believe it or not, the horizontal lines are all perfectly parallel. To prove this to yourself, just squint at the image or look at it from the side.Notice that, even after you’re completely convinced that the lines are parallel, the illusion continues to work. Perception is largely involuntary—and in many ways is walled off from our abstract knowledge of the world.3. ConfettiThis one’s a variation on the Munker Illusion, created by David Novick. The circles in the image are all the same color. The only thing that differs is the color of the lines around them.The illusion is a vivid demonstration of the fact that we don’t directly perceive the colors of objects in the world. Instead, the perceptual system takes an educated “guess,” based on the objects’ surroundings.4. The Rice Wave IllusionThis might look an animated GIF, but it’s not. The movement is all in your head.The shading and sequencing of the yellow blobs triggers the motion areas of the brain, creating a perception of movement in a stimulus that’s actually static. Interestingly, around 5 percent of people seem to be immune to this illusion.5. The Tilted Road IllusionThis looks like two photos of the same road, taken from different angles. But it’s actually just the same photo twice.Apparently, the visual system treats the image as if it were a single photo of two separate roads. The outlines of the two roads are parallel to one another in the two-dimensional image. For that to be the case in the image, the actual roads in the real world would have to be angling strongly away from each other. So, that’s what the visual system infers.6. Lightness IllusionThis one comes from master illusion-maker Akiyoshi Kitaoka (@AkiyoshiKitaoka).  In trying to make sense of the video, the visual system acts as if the gray square is being moved out of shadow into bright light, and then into dark shadow. For the square to look that shade in bright light, it would have to be quite dark—so the perpetual system infers that it is. Conversely, for the square to look that shade in dark shadow, it would have to be very light—so the perceptual system infers that, instead.Your brain is doing a lot of work behind the scenes—more than you might have imagined!7. The Dynamic EbbinghausAnother award-winning illusion. The orange circle doesn’t actually change size.  As with color and lightness, we don’t directly perceive the size of objects. The perceptual system makes an inference about their size, based on clues in our sense data—including the relative size of other, nearby objects.8. The Dynamic Müller-Lyer IllusionThis is one of the best illusions I’ve seen. The blue and red lines are all the same length; none is moving or changing size, and they’re all at the same level. Only the arrowheads are moving.  The illusion is a new variation on an old theme: the Müller-Lyer illusion. There are many theories about how this works, but no one’s 100 percent sure. There’s even debate about whether it works for all human beings everywhere, or instead is a culture-specific phenomenon.9. The Train IllusionYou can make the train change direction with the power of your mind … and you can get better at it with practice. Perception always involves going beyond the evidence of the senses. In this case, the evidence is relatively sparse, such that there are two plausible interpretations: The train is coming or the train is going. We can choose to see it either way.10. Rotating RingsThis one freaks a lot of people out. Every time you switch from looking at the red dot to the yellow, or vice versa, both wheels start spinning in the opposite direction. The illusion exploits differences in the way we interpret motion in the center of the visual field vs. the periphery. 11. The Spinning DancerOne of my all-time favorites. If you look at the dancer on the left and the one in the middle, the one in the middle spins clockwise. If you instead look at the dancer on the right and the one in the middle, the one in the middle starts spinning counter-clockwise. As with the train illusion, the secret is that the center image is ambiguous: It can be interpreted as a dancer spinning in either direction. The dancers on the left and right, in contrast, include additional details which force one or other interpretation. This forced interpretation then guides our perception of the middle, ambiguous figure.12. The Starry NightStare at the center of the top image for 30 seconds then check out Van Gogh’s Starry Night ... This is an example of a motion aftereffect. As you stare at the spiral, your visual system begins compensating for the motion so it can ignore this predictable stimulus. But then, when you look at the stationary painting, the system keeps compensating for the motion: motion that’s no longer there. This creates a false perception of motion in the opposite direction, which interacts with the details of the painting.Steve Stewart-Williams is author of The Ape That Understood the Universe: How the Mind and Culture Evolve (Cambridge University Press, 2018).Additional Reading My original Twitter thread that inspired the blog listicle post.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15309_4e6934f77e86b8c41a52f986de47181f.jpg",
    "title": "A Short History of the Missing Universe",
    "description": "Posted by Katia Moskvitch on October 01, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.The cosmos plays hide-and-seek. Sometimes, though, even when astronomers have a hunch for where their prey might…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.The cosmos plays hide-and-seek. Sometimes, though, even when astronomers have a hunch for where their prey might hide, it can take them decades of searching to confirm it. The case of the universe’s missing matter—a case that appears to now be closed, as I reported earlier this month—is one such instance. To me, it is a fascinating tale in which clever cosmological models drew a treasure map that took 20 years to explore.Scientists knew back in the 1980s that they could observe only a fraction of the atomic matter—or baryons—in the universe. (Today we know that all baryons taken together are thought to make up about 5 percent of the universe—the rest is dark energy and dark matter.) They knew that if they counted up all the stuff they could see in the universe—stars and galaxies, for the most part—the bulk of the baryons would be missing.But exactly how much missing matter there was, and where it might be hiding, were questions that started to sharpen in the 1990s. Around that time, astronomer David Tytler of the University of California, San Diego, came up with a way to measure the amount of deuterium in the light of distant quasars—the bright cores of galaxies with active black holes at their center—using the new spectrograph at the Keck telescope in Hawaii. Tytler’s data helped researchers understand just how many baryons were missing in today’s universe once all the visible stars and gas were accounted for: a whopping 90 percent.These results set off a firestorm of controversy, fanned in part by Tytler’s personality. “He [insisted] he was right in spite of, at the time, a lot of seemingly contradictory evidence, and basically said everyone else was a bunch of idiots who didn’t know what they were doing,” said Romeel Dave, an astronomer at the University of Edinburgh. “Turns out, of course, he was right.”Then in 1998, Jeremiah Ostriker and Renyue Cen, Princeton University astrophysicists, released a seminal cosmological model that tracked the history of the universe from its beginnings. The model suggested that the missing baryons were likely wafting about in the form of diffuse (and at the time undetectable) gas between galaxies.As it happens, Dave could have been the first to tell the world where the baryons were, beating Ostriker and Cen. Months before their paper came out, Dave had finished his own set of cosmological simulations, which were part of his Ph.D. work at the University of California, Santa Cruz. His thesis on the distribution of baryons suggested that they might be lurking in the warm plasma between galaxies. “I didn’t really appreciate the result for what it was,” said Dave. “Oh well, win some, lose some.”Dave continued to work on the problem in the years to follow. He envisioned the missing matter as hiding in ghostly threads of extremely hot and very diffuse gas that connect galaxy pairs. In astro-speak, this became the “warm-hot intergalactic medium,” or WHIM, a term that Dave coined.Many astronomers continued to suspect that there might be some very faint stars in the outskirts of galaxies that could account for a significant chunk of the missing matter. But after many decades of searching, the number of baryons in stars, even the faintest ones that could be seen, amounted to no more than 20 percent.More and more sophisticated instruments came online. In 2003, the Wilkinson Microwave Anisotropy Probe measured the universe’s baryon density as it stood some 380,000 years after the Big Bang. It turned out to be the same density as indicated by the cosmological models. A decade later, the Planck satellite confirmed the number.With the eventual failure to find hidden stars and galaxies that might be holding the missing matter, “attention turned toward gas in between the galaxies—the intergalactic medium distributed over billions of light years of low-density intergalactic space,” said Michael Shull, an astrophysicist at University of Colorado, Boulder. He and his team began searching for the WHIM by studying its effects on the light from distant quasars. Atoms of hydrogen, helium and heavier elements such as oxygen absorb the ultraviolet and X-ray radiation from these quasar lighthouses. The gas “steals a portion of light from the beam,” said Shull, leaving a deficit of light—an absorption line. Find the lines, and you’ll find the gas.The most prominent absorption lines of hydrogen and ionized oxygen are at very short wavelengths, in the ultraviolet and X-ray portions of the spectrum. Unfortunately for astronomers (but fortunately for the rest of life on Earth), our atmosphere blocks these rays. In part to solve the missing matter problem, astronomers launched X-ray satellites to map this light. With the absorption line method, Shull said, scientists eventually “accounted for most, if not all, of the predicted baryons that were cooked up in the hot Big Bang.”Other teams took different approaches, looking for the missing baryons indirectly. As my story from earlier this month shows, three teams, including Shull’s, are now saying that all the baryons are accounted for.But the WHIM is so faint, and the matter so diffuse, that it’s hard to definitely close the case. “Over the years, there have been many exchanges among researchers arguing for or against possible detections of the warm-hot intergalactic medium,” said Kenneth Sembach, director of the Space Telescope Science Institute in Baltimore. “I suspect there will be many more. The recent papers appear to be another piece in this complex and interesting cosmic puzzle. I’m sure there will be more pieces to come, and associated debates about how best to fit these pieces together.”Katia Moskvitch is a science and technology journalist, based in London, and the editor in chief of Professional Engineering magazine. She has written about physics, astronomy and other topics for Nature, Science, Scientific American, The Economist, Nautilus, New Scientist, the BBC and other publications. An aerospace engineer by education with a masters in journalism, she is the author of the book Call me ‘Pops’: Le Bon Dieu Dans La Rue.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15269_2fc02e925955d516a04e54a633f05608.jpg",
    "title": " Why Joel Osteen, “The Smiling Preacher,” Is So Darn Appealing",
    "description": "Posted by Adam Piore on September 17, 2018  It’s hard to quantify charisma, but by any measure Joel Osteen has some pretty impressive stats. Every week, the man some call “The Smiling Preacher,” draws an estimated 43,500 individuals…",
    "category": "Culture",
    "content": "It’s hard to quantify charisma, but by any measure Joel Osteen has some pretty impressive stats. Every week, the man some call “The Smiling Preacher,” draws an estimated 43,500 individuals to his Lakewood Church, which he moved into a former professional basketball stadium just off Houston’s Highway 59 in 2005. Osteen’s weekly sermons are beamed across seven networks in the United States and, by some estimates, reach 95 percent of the nation’s households and more than 150 countries.The 55-year-old pastor, with his boyish good looks, ubiquitous incisors, and his impeccably coiffed mane of wavy, brown locks, oversees a budget estimated at upward of $70 million. He has penned no less than seven best sellers (most derived from his sermons), has amassed a net worth estimated at $40 million, with book sales and related revenue reportedly exceeding $55 million, and lives in a 17,000-square foot, $10.5 million mansion. All of it is built upon the personality—the words, the wisdom, and in no small part the charisma—of the man the congregants of the nation’s largest charismatic church refer to, simply, as “Pastor Joel.”So, what is it that makes Osteen different from the rest of us? What is the source of his magical magnetism?Many—including Osteen himself—might attribute his gifts to the favor of a higher power. After all, charisma, wrote the early 20th century German sociologist Max Weber, who gave the word its most widely used modern definition, is a quality that sets an individual “apart from ordinary men,” and causes others to treat him as “endowed with supernatural, superhuman, or at least specifically exceptional powers or qualities.”But there’s a small but growing group of individuals who have another explanation. Using brain-scan technologies and modern statistical techniques, a band of committed academics in recent years have set out to decipher that mysterious quality from which legendary leadership is born. And some have reached what a previous generation of observers might have considered a dubious conclusion: That it’s possible not just to reverse-engineer charisma, but that it’s something, at least in part, we might learn to master.“Charismatic tactics can be taught, and the more charismatic leadership tactics used, the more individuals will be seen as leader-like by others,” says John Antonakis, a professor of Organizational Behavior, and Director of the Ph.D. program in management at the University of Lausanne. (Read the Nautilus feature about how we create charismatic leaders and the dangerous consequences of their power.) Right out of the gate, Osteen is using three of Antonakis’ identified tactics.By studying well-known charismatics and replicating their actions in the lab, Antonakis has identified a series of what he calls Charismatic Leadership Tactics (CLTs), which range from the use of metaphors and storytelling, to nonverbal methods of communication like open posture and animated gestures at key moments.At the request of Nautilus, Antonakis assigned a doctoral student, Benjamin Tur, to sit down and code the first 10 minutes of a 2012 sermon by Osteen, “The Power of I Am,” a speech that Oprah Winfrey says changed her life.The sermon opens with a photographic montage that includes an image of a smiling Osteen, standing with his photogenic family—son, daughter, and wife—autumn leaves cascading joyfully down around them. It moves to snapshots of his son throwing a football, his daughter kissing a puppy dog, and finally lands on Joel standing with his beautiful wife Victoria, her long blonde hair billowing gently in the wind. The screen cuts to a camera slowly moving over a huge multiracial stadium crowd of all shapes and sizes, panning in and resting with the handsome Osteen. That’s when the magic begins.Osteen is clad in an impeccable Cerulean blue suit, crisp white shirt and purple, paisley tie, and he is at that very moment, extending his arm and open hand outward toward the screen—toward me—toward all of us—beckoning viewers to join him.“God bless you! It’s a joy to come into your homes,” Osteen says, pointing his index finger E.T.-like at the viewing audience for just a second, flashing a humble smile, then leaning his right shoulder ever so slightly toward the camera, while blinking his long eyelashes rapidly, as if awakening to a bright, glorious morning. “We love you. If you are ever in our area, please stop by and be a part of one of our services! I promise you we’ll make you feel right at home. Thanks so much for tuning in.”Osteen shambles over to a wooden podium, places a hand gently on its edge, and tells the audience he likes to start with “something funny.”“I heard about this 92-year-old man,” Osteen begins. “He wasn’t feeling up to par and he went to the doctor for a checkup. A few days later the doctor saw him walking in the park. He had this beautiful young lady by his side and he seemed as happy as can be. The doctor said, ‘Wow you sure are feeling a lot better aren’t you!’ He said, ‘Yes, doctor, I’m just taking your orders. You said, ‘Get a hot mama and be cheerful.’ The doctor said, ‘I didn’t say that, I said ‘You got a heart murmur be careful!’”With the tone set, Osteen is off, exhorting his followers to hold their Bibles aloft, repeat a prayer, and then launching into an inspirational message.Right out of the gate, Osteen is using three of Antonakis’ identified tactics: an animated voice, facial expressions, and gestures. All three figure in Osteen’s opening, even before he has launched into his actual sermon. Taken together, the gestures cue the audience that they have arrived on friendly territory, and encourage them to let down their guards. Osteen begins his sermon. “I want to talk to you today about the power of I am,” he says. “What follows these two simple words will determine what kind of life you live. I am blessed. I am strong, I am healthy. Or, I am slow, I am unattractive, I am a terrible mother. The I ams that are coming out of your mouth will bring either success or failure.”To connect through a verbal message, Antonakis says, a leader must do three things. He must “frame” a vision or paint a picture by using metaphor or stories. He must express sentiments of the collective. Finally, he must deliver it all in an in animated and passionate way. In the minutes that follow, Osteen will continue to do all three.Of the 12 different CLTs that Antonakis and Tur look for, nine are verbal. They are: metaphor and comparison, story, rhetorical question, contrasts, lists and repetitions, moral convictions, expressing the sentiments of the collective, setting high and ambitious goals, and creating confidence that goals can be achieved. Osteen uses on average one charismatic verbal tactic every two sentences. By comparison, Martin Luther King’s “I Have a Dream” speech has well over three times as many verbal signaling techniques per sentence—his language is infused with powerful imagery and metaphor. “Osteen’s speech is rather average when it comes to use of verbal signaling techniques,” the academics say.But Osteen makes up for his relative poverty of verbal CLTs by the way he delivers his sermon. He takes full advantage of the medium of the television, which allows us to watch him up close.Antonakis and Tur say that Osteen shows an open body posture and uses representative gestures at key moments; for instance, when he says, “I am so old,” he mimics wrinkles at the corner of his eyes. There is also his voice. He displays variation both in term of pitch and speed, slowing down, using pauses or speeding up. “Like MLK, his voice sometimes vibrates in this preacher style,” Antonakis and Tur say. Finally, there is Osteen’s facial expression. “He is smiling constantly and accompanies that by raising his eyebrows, making his face more expressive.”In conclusion, say Antonakis and Tur, the handsome Osteen “embodies his speech and smiles constantly throughout the talk. This combination of nonverbal behavior makes the speech captivating for the audience.”Adam Piore is the author of The Body Builders: Inside the Science of the Engineered Human. Follow him on Twitter @adampiore.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: A biological anthropologist on her “four primary temperament dimensions.”This classic Facts So Romantic post was originally published in February 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15270_e14c38bd11cd714b970735ade2f233fa.png",
    "title": "The Case for Dancing Astrophysics",
    "description": "Posted by Paul M. Sutter on September 14, 2018  For millennia, cosmological and religious systems of thought were intertwined—and usually indistinguishable. European artwork of, say, the arrangements of planets and stars often went…",
    "category": "Culture",
    "content": "For millennia, cosmological and religious systems of thought were intertwined—and usually indistinguishable. European artwork of, say, the arrangements of planets and stars often went hand-in-hand with theological guides, and not a little bit of moralizing. But then Copernicus, Brahe, Kepler, and Galileo came along, and this connection fell apart. The result is a modern cosmological sense that is properly scientific, rooted in data and mathematics—but relatively inaccessible to non-experts.What do we do when confronted with deep, profound, and enigmatic mysteries that defy our common sense?  We do art. And what is the ideal art to communicate the concepts of cosmology? It isn’t placed on a canvas, but a dance stage. Cosmology is the story of the fundamental particles, forces, and energies that shape and govern our universe. And that story is one of rhythm and motion.In those rare moments, the audience got to feel what it’s like for two galaxies to collide.Look at images of colliding galaxies, each home to hundreds of billions of individual stars. Their merger occurs over the course of hundreds of millions of years. Eruptions of newborn stars stretch out, creating effervescent arcs tens of thousands of light years long. Look at the so-called dark ages, the period in the early history of the cosmos before the first stars were born. In those misty and murky times, flows of gas and matter, driven by the attractive bonds of gravity, built ever-large structures over the course of eons. Then, with a flash, in some small forgotten corner of the cosmos, when it was around 400 million years old, a critical threshold in density and temperature was reached, nuclear fusion ignited, and the first star was born, illuminating the cosmos with light and heat for the first time. Look at the great clusters of galaxies, where each galaxy swims through a hot, thin soup of plasma that fills out the cluster’s volume: a veritable beehive of frenzied activity.While the observations and theory that scientists like me use to understand these cosmic events are well-understood, it’s difficult—given the scales of time and space involved—to tell stories about them. But imagine an intimate theater setting: The house lights darken and the music begins to swell. The curtain parts, revealing two dancers on opposite sides of the stage, one lit by cool blues; the other by fiery reds. They approach and the intricate pas de deux begins, their motions expressing the competition between the forces of gravity and energetic release in the heart of a star. The inward crush of the star’s own weight is balanced by the raging nuclear fires in a twisting, convecting cauldron.In those brief moments we are transported to an other-worldly environment; in this case, the core of an alien sun. It’s one thing to state the technical term (“hydrostatic equilibrium”); it’s another to view the process as a struggle for supremacy between two indomitable forces leading to near-perfect balance, tinged with the bittersweet knowledge that the struggle won’t last forever. The star will die.The magic of dance, and the arts, is that they help us care, imagine, and, at the risk of sounding saccharine, dream. I’ve been lucky enough to work with dance companies and see this play out. My biggest project, “Song of the Stars,” which premiered, in 2016, at the Capitol Theater in Ohio, told the life stories of the stars using contemporary dance. The setup of the performance was simple: I would introduce each piece, weaving together a story and some essential physics, while the dancers took their places. As they danced, I got to watch the audience react.With each graceful movement or swell in the music, I would see faces in the audience light up, shoulders unconsciously sway, eyes begin to tear. In those rare moments, the audience got to feel what it’s like for two galaxies to collide, or for a star to end its life in a cataclysmic explosion.That’s something that a lecture or journal article can’t readily conjure.Paul M. Sutter is an astrophysicist at The Ohio State University and the Chief Scientist at COSI Science Center in Columbus, Ohio. He has authored over 50 academic papers on topics ranging from the earliest moments of the Big Bang, to the emptiest places in the universe, to novel methods for detecting the first stars.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The intersection of art and science. "
  },
  {
    "imageUrl": "http://static.nautil.us/15315_83ab0b13719ba7c929a84678b92ed5c0.jpg",
    "title": " The Case for Making Cities Out of Wood",
    "description": "Posted by Brian  Gallagher on October 03, 2018  Earlier this year, Dan Doctoroff, the C.E.O. of Sidewalk Labs, Google’s sibling company under Alphabet, answered a question about what his company “actually does” during a Reddit…",
    "category": "Culture",
    "content": "Earlier this year, Dan Doctoroff, the C.E.O. of Sidewalk Labs, Google’s sibling company under Alphabet, answered a question about what his company “actually does” during a Reddit “Ask Me Anything” session, replying, “The short answer is: We want to build the first truly 21st-century city.” Quayside, a Toronto neighborhood the company is developing in partnership with a Canadian tri-government agency, is the first step toward Doctoroff’s goal. It has been in the news because it could inspire a Black Mirror plot: It will be built from “the Internet up,” according to a project document, a merger of “physical and digital realms.” Fittingly, according to the New York Times, “No obvious way to opt out of Quayside’s surveillance systems exists, except by staying out of the area.”But Quayside’s newsworthy for another, more encouraging reason: The plan is to build the place, not out of concrete and steel, but wood—and wood is looking good. A recent advance in wood technology should interest the neighborhood’s developers: Teng Li, a University of Maryland mechanical engineer, created with his colleagues wood that’s as “strong as steel, but six times lighter,” he said. Liangbing Hu, Li’s co-author on the study, added, “This kind of wood could be used in cars, airplanes, buildings—any application where steel is used.” Making it is just a two-step process. The scientists first boiled natural wood in a mixture of sodium hydroxide and sodium sulfite, to remove some of the lignin and hemicellulose, substances contained in the walls of wood cells (the former retard parasite and bacterial attacks, the latter cover and bind fibers). Then they put the wood in a hot press, which leads, as they say in the paper, “to the total collapse of cell walls and the complete densification of the natural wood with highly aligned cellulose nanofibres.” The result, they conclude, is a “low-cost, high-performance, lightweight alternative” to “most structural metals and alloys.”“Our proposal is that we need timber to save us.”The idea of future wood cities has been hanging around for at least a few years. In 2014, for example, the Boston Globe wondered, “Will cities of the future be built of wood?” The rise of both human population and global temperature prompted much of the interest in the sustainability of city-scale timber construction. By 2050, we will number almost 10 billion; and since two-thirds of us will be city dwelling—up from just over half in 2014—cities will have to grow. It’d be ideal to do that without curtailing efforts to combat climate change; it wouldn’t be helpful, for instance, to keep extracting and transporting the raw materials needed to build steel and concrete structures, which are more expensive and have large carbon footprints. Wood, on the other hand, is “a plentiful resource that grows back relatively quickly, and even pulls carbon out of the atmosphere as it does,” the Globe noted. Yugon Kim, an architect passionate about timber construction, said, “We all are hard-wired to see the city as being steel, glass, and concrete. Our proposal is that we need timber to save us.”The proposal hasn’t been ignored. The Economist published a video titled, “Wooden skyscrapers could be the future for cities.” The main obstacle to this outcome—what a 2016 paper called “a full timber building renaissance”—is the public’s fear of city fires, which has been reflected over centuries in construction codes globally. Some researchers, like Alastair Bartlett, at the University of Edinburgh, see in these obstacles an “opportunity,” as he wrote in a 2017 study, “to revisit compartment fire behaviour and to quantify the impact of these new construction technologies on the compartment fire dynamics.”Bartlett, along with some colleagues, tested the sort of wood mid- and high-rise buildings—like Framework, in Portland—have recently been made out of, known in the industry as “cross-laminated” or “mass” or “tall” timber, which is how Quayside’s developers refer to it. Bartlett set up three rooms of equal size, about as big as a large walk-in closet. Two of the rooms had two exposed surfaces of timber (one with two walls exposed, the other a wall and the ceiling); the last room had two walls and the ceiling exposed (non-exposed surfaces were covered with plasterboard). Each room had four wooden “cribs,” or pallets, for the “fuel load,” placed on the ground, one of which was lit using fiber strips soaked in paraffin. Bartlett then documented how fast and how hot each room burned. The room with an exposed wall and ceiling managed to “auto-extinct”—the fire went out by itself. This depended “on the char layer [of the wood] remaining attached” rather than falling off while the cribs were burning, Bartlett concluded. He wants to do many more experiments because it’s still not clear how, in mass timber buildings, to get compartment fires to reliably burn out on their own, a “cornerstone of fire safety engineering design,” he writes. “The failure modes of common compartment construction systems and materials are relatively poorly documented from a scientific (rather than compliance testing) perspective.”The combustibility of this wood depends partly on its size, according to Daniel Safarik, editor of the Council on Tall Buildings and Urban Habitat, a Chicago-based non-profit. “Massive wood walls and structural beams and columns comprised of engineered panels have demonstrated fire performance equal to concrete and, in some cases, superior to steel,” he told The Architect’s Newspaper. “Mass timber has to burn through many layers before it is structurally compromised—basically it ‘chars’ long before it collapses.” Still, as a 2016 review concluded, “Large-scale and full-frame tests are also necessary…to determine the variety of possible fire induced failure modes that may occur in real mass timber buildings.” The same will no doubt be true of the new densified wood Li and his colleagues recently created.There is also, alongside the environmental and economic, an aesthetic-psychological case for wood cities. Clare Farrow, who is co-curating a current London exhibit called “Timber Rising–Vertical Visions for the Cities of Tomorrow,” wrote in Dezeen, “Studies are showing that the presence, scent and touch of wood can have remarkably positive effects, not only on people’s wellbeing in a general sense, but more specifically on stress levels, blood pressure, communication, learning and healing.” A 2015 review in Wood Science and Technology supports her claims and also suggests that “specific aspects of wood such as colour, quantity, and grain pattern should be examined” in future studies.I certainly have a fondness for wood. On work errands with my father, when I was a child, I remember being drawn to the plywood section of Home Depot. I’d run my hands against the surfaces of large slabs, sometimes still damp after being cut, and I’d smell the fresh sawdust piled up at the cutting station. I still have my 7th and 8th grade woodshop projects—gumball machine, foldable stool—and for some reason, to my wife’s chagrin, love making my apartment’s wood floor creak. Recently, I was bummed to learn that, last year, a plan to build a 10-story timber condo in New York City was scrapped (“the cooling-off of NYC’s condo market,” according to the website Curbed, “made the deal seem less attractive.”) But Quayside’s wood-centric vision of future cities has me feeling optimistic. “Timber construction is possible at all scales and at pioneering height,” a Quayside document states. We’ll have to see if that confidence catches on, in Newark and beyond.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @BSGallagher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The difference between a great ancient building and a modern one.This classic Facts So Romantic post was originally published in February 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15338_50075d2902d41e5cb9903a24689a7d78.jpg",
    "title": "Why Mathematicians Can’t Find the Hay in a Haystack",
    "description": "Posted by Kevin Hartnett on October 05, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.The first time I heard a mathematician use the phrase, I was sure he’d misspoken. We were on the phone, talking…",
    "category": "Numbers",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.The first time I heard a mathematician use the phrase, I was sure he’d misspoken. We were on the phone, talking about the search for shapes with certain properties, and he said, “It’s like looking for hay in a haystack.”“Don’t you mean a needle?” I almost interjected. Then he said it again.In mathematics, it turns out, conventional modes of thought sometimes get turned on their head. The mathematician I was speaking with, Dave Jensen of the University of Kentucky, really did mean “hay in a haystack.” By it, he was expressing a strange fact about mathematical research: Sometimes the most common things are the hardest to find.“In many areas of mathematics you’re looking for examples of something, and examples are really abundant, but somehow any time you try to write down an example, you get it wrong,” said Jensen.The hay-in-a-haystack phenomenon is at work in one of the first objects that kids encounter in mathematics: the number line. Points on the number line include the positive and negative integers (such as 2 and –29), rational numbers (ratios of integers like 3/2 and 1/137) and all irrational numbers—those numbers, like pi or √2, that can’t be expressed as a ratio.Irrational numbers occupy the vast, vast majority of space on a number line—so vast, in fact, that if you were to pick a number on the number line at random, there is literally a 100 percent chance that it will be irrational.*Yet despite their overwhelming presence, we almost never encounter irrational numbers in our daily lives. Instead we count with whole numbers and follow recipes with fractions. The numbers we know best are the extremely rare numbers, the special numbers—the needles in the haystack.The hay is hard to find precisely because it’s so unexceptional. Rational numbers have the distinctive property that it’s possible to write them down. This calls them to our attention. Irrational numbers have an infinite decimal expansion. You couldn’t write one down even with an endless amount of time. That these numbers lack the exceptional property of “write-down-able-ness” is what makes them nearly invisible to our way of seeing.“We’re looking with a magnet, and you’re not going to find hay with magnet; you’re only going to find needles,” said Dhruv Ranganathan, a mathematician who is in the midst of a move from the Massachusetts Institute of Technology to the University of Cambridge.The search for hay in a haystack characterizes many different areas of math, including the subject of my most recent Quanta article, “Tinkertoy Models Produce New Geometric Insights.” There I wrote about mathematicians who are investigating the relationship between geometric shapes and the equations used to describe them. In rare cases, objects can be expressed by simple equations. These are the needles, the shapes we know best: lines, parabolas, circles, spheres.The overwhelming preponderance of shapes resist such elegant formulation. They may be everywhere, but because you can’t write down the equations that describe them, it’s hard to establish that even a single one of them exists.In my article, I explained how techniques from a field called “tropical geometry” serve as an especially sly way of deducing the existence of these ubiquitous geometric objects—the ones that, like the irrational numbers, are everywhere, even if you can’t write them down.In mathematics it often happens that either something doesn’t exist, or it exists in abundance. The nature of those abundant objects might make them hard to detect, but if you’re a mathematician and you believe they’re there, and you believe they constitute almost all of everything, your task is straightforward: Find just one.It’s as if you were convinced the oceans were filled with water, said Ranganathan, but every time you took a sample, you came up with something else—a shell, a rock, a plant. Yet to start to believe your hypothesis was correct, you’d hardly need to empty the sea.“All you have to do is find any water,” he said. “One droplet of water will do.”Kevin Hartnett is a senior writer at Quanta Magazine covering mathematics and computer science. His work has been collected in the “Best Writing on Mathematics” series in 2013 and 2016. From 2013-2016 he wrote “Brainiac,” a weekly column for the Boston Globe‘s Ideas section.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15291_fa93e5500dfb05a3ff656d5a483bbd71.jpg",
    "title": "Why Did the Ancients Bury Their Dogs like Family Members?",
    "description": "Posted by Bridget Alex on September 24, 2018  As a kid, when my pet turtle died we had a funeral—of course—and buried him in the backyard. When the family dog passed, his remains were cremated and placed in an urn on the mantle.…",
    "category": "Culture",
    "content": "As a kid, when my pet turtle died we had a funeral—of course—and buried him in the backyard. When the family dog passed, his remains were cremated and placed in an urn on the mantle. In today’s society, mortuary rites for animals are so common, Yelp has reviews of pet cemeteries (5 stars for Animal Aftercare. 4.5 for Pet Haven).While online ratings are new, concern for the animal afterlife is not a modern fad. “People were doing this thousands and thousands of years ago… it’s a long, long standing practice,” says archaeologist Robert Losey.Archaeologists have unearthed ancient pet burials dating as far back as 14,000 years, from the dawn of animal domestication. Although interred animals are relatively rare (when considering the full archaeological record of all human societies), they occur in at least some cultures and time periods on every continent except Antarctica.It’s fairly easy for archaeologists to identify a burial: Filled-in pits have loose, jumbled sediment, while surrounding earth is more compact and layered. Also, complete skeletons in proper alignment indicate burials; otherwise the bones would be scattered and fragmented.The real challenge is to understand the motivations behind these burials. Ancient people may have interred animals for the same reasons we do today: because they were beloved pets, members of the family worthy of memorial. Alternatively, animals could have been sacrificed as part of a larger ritual or just buried to get rid of rotting carcasses.Here’s how archaeologists have made cases for putative pet cemeteries.Dogs Buried Human-styleMan’s best friend was also man’s first buried pet. During the Paleolithic, or Stone Age, hunter-gatherers domesticated wolves into dogs. Although the specific time and place (or places) this happened is disputed, by 14,000 years ago a canine, shown to be a domesticated dog by anatomical features and ancient DNA, was interred at the site of Bonn-Oberkassel, Germany. The approximately 6-month-old pup was buried with a middle aged man, twenty-something woman, and grave goods including a bone hairpin, elk sculpture and the penis bone of a bear. Across the Atlantic, over 200 dog burials more than 3,000 years old have been discovered in North America. The oldest are three dogs found in shallow pits at the Koster family farm in Illinois, dating to 8,500 years ago.Deliberate burial of special dogs was particularly common between 7,000-8,000 years ago around Lake Baikal in eastern Russia. During this period forager people began burying their dead in cemeteries, and gave the same treatment to some canines.“It’s my hypothesis that people really saw those particular dogs as being spiritually the same as themselves. That they were an animal with a soul, an animal with an afterlife,” says Losey, a professor at the University of Alberta, Canada who researches ancient human-dog relationships.The evidence to support his claim: First, dogs were buried amongst humans in designated areas that seem to have been cemeteries. According to Losey, people would have had to “travel with the dog’s body and take it to those locations. It’s not just putting it in a hole right next to where you’re living.” In some cases earlier human burials were disturbed to make way for more recently deceased canines.Next, the dogs likely died of natural causes. Some are old, and they don’t exhibit cut marks or other indicators of sacrifice. Finally, these animals were laid to rest with the same items buried with people, such as a spoon and a deer tooth necklace. “We see people wearing essentially the exact same necklace in human graves in that same area… They literally seem to be treating that dog just like a human,” Losey says.Resting by the Red SeaAnother convincing case comes from the port town of Berenike on the shores of the Red Sea, in southern Egypt. Between 75-150 A.D. people living there buried animals in a specific area on the outskirts of town, which, according to a 2016 study, was a cemetery of house pets.In ancient Egypt, animals were commonly sacrificed and mummified to accompany people to the grave. But the Berenike animals were treated differently. They were not mummified or placed in human tombs. Rather, they were buried in a cemetery, which has yielded nearly 100 complete animals including 86 domestic cats, 9 dogs and 4 monkeys. A few wore iron collars and two graves were double burials, containing both a cat and kitten.The largest pet cemetery or not a pet cemeteryThe jury’s out on the largest concentration of ancient animals: The site of Ashkelon on Israel’s Mediterranean coast contains somewhere between 500 to 1500 dogs buried over the course of a century, beginning about 2,500 years ago.The exact count is unclear because many skeletons were jumbled and incomplete, and others may have been eroded into the sea. But even the lower estimates indicate “an extraordinary number of burials,” says Paula Wapnish-Hesse, an archaeologist who studied the remains.When Persian and early Greek cultures inhabited the city, dead canines of all ages were buried in shallow pits under streets and living quarters. Aside from being lain on their sides with tails tucked between their legs, the dogs showed no signs of funerary treatment. “There were never any grave goods, never any markers, never anything to indicate to us these burials were special outside of the burials themselves,” Wapnish-Hesse says.The dogs were mostly puppies and old individuals—what you would expect from death by natural causes. There’s no evidence they were intentionally killed, at least by methods that leave marks on bones. Based on the shape and size of the skeletons, Wapnish-Hesse believes they were free roaming street mutts, rather than kept pets. And she does not consider the burials a cemetery per se, because they are found throughout a neighborhood, rather than in a separate area.So perhaps the dogs were not pets, and graves did not constitute a cemetery. The question, though, remains, why were they buried in such abundance?Other researchers have suggested the dogs were interred as part of a religious cult or after a catastrophe, like an epidemic, but Wapnish-Hesse does not find these explanations convincing. After more than three decades researching these dogs, she says, “We’ve never answered the question of why they were burying them.”Some cases remain a mystery.This article was originally published on DiscoverMagazine.com on September 17, 2018.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15262_19c81ddc9575bacf2a6f73b428065821.png",
    "title": "The Strange Numbers That Birthed Modern Algebra",
    "description": "Posted by Charlie Wood on September 18, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.Imagine winding the hour hand of a clock back from 3 o’clock to noon. Mathematicians have long known how to describe…",
    "category": "Numbers",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.Imagine winding the hour hand of a clock back from 3 o’clock to noon. Mathematicians have long known how to describe this rotation as a simple multiplication: A number representing the initial position of the hour hand on the plane is multiplied by another constant number. But is a similar trick possible for describing rotations through space? Common sense says yes, but William Hamilton, one of the most prolific mathematicians of the 19th century, struggled for more than a decade to find the math for describing rotations in three dimensions. The unlikely solution led him to the third of just four number systems that abide by a close analog of standard arithmetic and helped spur the rise of modern algebra.The real numbers form the first such number system. A sequence of numbers that can be ordered from least to greatest, the reals include all the familiar characters we learn in school, like –3.7, √‾5 and 42. Renaissance algebraists stumbled upon the second system of numbers that can be added, subtracted, multiplied, and divided when they realized that solving certain equations demanded a new number, i, that didn’t fit anywhere on the real number line. They took the first steps off that line and into the “complex plane,” where misleadingly named “imaginary” numbers couple with real numbers like capital letters pair with numerals in the game of Battleship. In this planar world, “complex numbers” represent arrows that you can slide around with addition and subtraction or turn and stretch with multiplication and division.Hamilton, the Irish mathematician and namesake of the “Hamiltonian” operator in classical and quantum mechanics, hoped to climb out of the complex plane by adding an imaginary j axis. This would be like Milton Bradley turning “Battleship” into “Battlesubmarine” with a column of lower case letters. But there was something off about three dimensions that broke every system Hamilton could think of. “He must have tried millions of things and none of them worked,” said John Baez, a mathematician at the University of California, Riverside. The problem was multiplication. In the complex plane, multiplication produces rotations. No matter how Hamilton tried to define multiplication in 3-D, he couldn’t find an opposing division that always returned meaningful answers.To see what makes 3-D rotation so much harder, compare turning a steering wheel with spinning a globe. All the points on the wheel move together in the same way, so they’re being multiplied by the same (complex) number. But points on the globe move fastest around the equator and slower as you move north or south. Crucially, the poles don’t change at all. If 3-D rotations worked like 2-D rotations, Baez explained, every point would move.The solution, which a giddy Hamilton famously carved into Dublin’s Broome Bridge when it finally hit him on October 16, 1843, was to stick the globe into a larger space where rotations behave more like they do in two dimensions. With not two but three imaginary axes, i, j and k, plus the real number line a, Hamilton could define new numbers that are like arrows in 4-D space. He named them “quaternions.” By nightfall, Hamilton had already sketched out a scheme for rotating 3-D arrows: He showed that these could be thought of as simplified quaternions created by setting a, the real part, equal to zero and keeping just the imaginary components i, j, and k—a trio for which Hamilton invented the word “vector.” Rotating a 3-D vector meant multiplying it by a pair of full 4-D quaternions containing information about the direction and degree of rotation. To see quaternion multiplication in action, watch the newly released video below by the popular math animator 3Blue1Brown.Everything you could do with the real and complex numbers, you could do with the quaternions, except for one jarring difference. Whereas 2 × 3 and 3 × 2 both equal 6, order matters for quaternion multiplication. Mathematicians had never encountered this behavior in numbers before, even though it reflects how everyday objects rotate. Place your phone face-up on a flat surface, for example. Spin it 90 degrees to the left, and then flip it away from you. Note which way the camera points. Returning to the original position, flip it away from you first and then turn it to the left second. See how the camera points to the right instead? This initially alarming property, known as non-commutativity, turns out to be a feature the quaternions share with reality.But a bug lurked within the new number system too. While a phone or arrow turns all the way around in 360 degrees, the quaternion describing this 360-degree rotation only turns 180 degrees up in four-dimensional space. You need two full rotations of the phone or arrow to bring the associated quaternion back to its initial state. (Stopping after one turn leaves the quaternion inverted, because of the way imaginary numbers square to –1.) For a bit of intuition about how this works, take a look at the rotating cube above. One turn puts a twist in the attached belts while the second smooths them out again. Quaternions behave somewhat similarly.Upside-down arrows produce spurious negative signs that can wreak havoc in physics, so nearly 40 years after Hamilton’s bridge vandalism, physicists went to war with one another to keep the quaternion system from becoming standard. Hostilities broke out when a Yale professor named Josiah Gibbs defined the modern vector. Deciding the fourth dimension was entirely too much trouble, Gibbs decapitated Hamilton’s creation by lopping off the a term altogether: Gibbs’ quaternion-spinoff kept the i, j, k notation, but split the unwieldy rule for multiplying quaternions into separate operations for multiplying vectors that every math and physics undergraduate learns today: the dot product and the cross product. Hamilton’s disciples labeled the new system a “monster,” while vector fans disparaged the quaternions as “vexatious” and an “unmixed evil.” The debate raged for years in the pages of journals and pamphlets, but ease of use eventually carried vectors to victory.Quaternions would languish in the shadow of vectors until quantum mechanics revealed their true identity in the 1920s. While the normal 360 degrees suffice to fully rotate photons and other force particles, electrons and all other matter particles take two turns to return to their initial state. Hamilton’s number system had been describing these as-yet undiscovered entities, now known as “spinors,” all along.Still, physicists never adopted quaternions in their day-to-day calculations, because an alternative scheme for dealing with spinors was found based on matrices. Only in the last few decades have quaternions experienced a revival. In addition to their adoption in computer graphics, where they serve as efficient tools for calculating rotations, quaternions live on in the geometry of higher-dimensional surfaces. One surface in particular, called a hyperkähler manifold, has the intriguing feature that it allows you to translate back and forth between groups of vectors and groups of spinors—uniting the two sides of the vector-algebra war. Since vectors describe force particles while spinors describe matter particles, this property holds extreme interest to physicists who wonder if a symmetry between matter and forces, called supersymmetry, exists in nature. (However, if it does, the symmetry would have to be severely broken in our universe.)For mathematicians, meanwhile, quaternions never really lost their shine. “As soon as Hamilton invented the quaternions, everyone and his brother decided to make up their own number system,” Baez said. “Most were completely useless, but eventually … they led to what we now think of as modern algebra.” Today, abstract algebraists study a vast array of number systems in any number of dimensions and with all manner of exotic properties.One not-so-useless construction turned out to be the fourth and final number system that permits a multiplication analog and an associated division, discovered shortly after the quaternions by Hamilton’s friend, John Graves. Some physicists suspect that these peculiar, eight-dimensional “octonions” may play a deep role in fundamental physics.“I think there’s still a lot more to discover about geometry based on the quaternions,” said Nigel Hitchin, a geometer at the University of Oxford, “but if you want a new frontier, then it’s the octonions.”Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15294_2ebb6c06bdc16ef37ec965c6b325b5c6.jpg",
    "title": "Insects and the Meaning of Sleep",
    "description": "Posted by Laura Sanders on September 25, 2018  This story was originally published by Knowable Magazine.If you watch an exhausted baby carefully, you may be able to see gravity tug heavy eyelids down. Likewise, a sleeping honeybee’s…",
    "category": "Biology",
    "content": "This story was originally published by Knowable Magazine.If you watch an exhausted baby carefully, you may be able to see gravity tug heavy eyelids down. Likewise, a sleeping honeybee’s usually perky antennae droop.This adorable sign of insect repose may seem unremarkable. But studying insect slumber may ultimately help solve some of sleep’s greatest mysteries, Charlotte Helfrich-Förster of the University of Würzburg in Germany argues in a 2017 review in Annual Review of Entomology. “Because the insect brain is simpler than a mammal’s brain, the study of sleep in insects promises new insights into its neuronal basis and functional role,” she writes. Experiments on the fruit fly, the easily-manipulated lab darling, could be particularly helpful in unraveling the precise brain areas that work together to control sleep, Helfrich-Förster adds.When entomologist Barrett Klein talks to people about his research on honeybee sleep, the conversations follow a predictable script: “People almost invariably start with the shocker, ‘Insects sleep?’” Then comes a flurry of follow-up questions: “What does it mean to sleep? How do you identify sleep in an insect? Is it related to human sleep?”To get the preliminaries out of the way, insects do sleep—sort of—says Klein, of the University of Wisconsin–La Crosse. “It depends on how you define sleep,” he says. “Honestly, we still don’t have a firm grasp on what sleep is, in every respect.”Sleep’s definition is particularly hard to pin down for insects. Clues about whether an insect is sleeping are subtle. Honeybees’ droopy antennae, cockroaches’ crouches and fruit flies’ long periods of stillness may all indicate sleep. But such outward signs aren’t foolproof. A motionless fruit fly might be sleeping or might just be resting.Methods that measure the brain’s electrical activity may identify sleep, but those techniques aren’t foolproof, either (not to mention that they’re tricky to implement in tiny insects). Other sleep indicators, such as how hard it is to make an insect respond to an external cue, or whether the need for catch-up sleep after an all-nighter exists, might prove to be useful indicators of insect sleep, Klein says.Definitions aside, more detailed studies on resting insects’ brains reveal parallels to human sleep. Studies of the fruit fly brain have revealed the rhythmic production of proteins that form the basis of the circadian clock. Many of the same proteins are also at work in people and other mammals. Other research on fruit flies has turned up sleep-related jobs for chemical messengers (neurotransmitters) including dopamine, acetylcholine, GABA and others. Mucking with these systems can change fruit flies’ activity and rest cycles, writes Helfrich-Förster.And just as in people, interrupting insects’ sleep can harm brain performance and change behavior, Klein and others have found. Honeybees perform intricate dances to tell each other where to find flowers. But after being kept awake with an evil lab contraption called the “insominator,” honeybees’ dances grew sloppy, Klein and colleagues reported in 2010 in the Proceedings of the National Academy of Sciences.Unanswered questions about sleep—in insects and in people—abound. “We’re spending so much of our time doing this, and not just closing our eyes or turning off the lights, but really shutting down,” Klein says. And yet, no one knows why. Perhaps insects, which Klein describes as “marvelously diverse, exquisite, behaviorally compelling,” might someday help reveal the answer.Laura Sanders is a freelance science journalist whose home base is Corvallis, Oregon. Reach her at lsanders@nasw.org.\n\tThe newest and most popular articles delivered right to your inbox!\nKnowable Magazine is an independent journalistic endeavor from Annual Reviews. "
  },
  {
    "imageUrl": "http://static.nautil.us/15222_7d4a769562e3528950e2d1aebdfb0550.jpg",
    "title": "A Better Way to Cancel Noise",
    "description": "Posted by Brian  Gallagher on August 31, 2018  The other day I stepped into my apartment elevator and saw a neighbor of mine joking around with a construction worker. “You know what you do with these guys?” my neighbor said to me.…",
    "category": "Matter",
    "content": "The other day I stepped into my apartment elevator and saw a neighbor of mine joking around with a construction worker. “You know what you do with these guys?” my neighbor said to me. He grabbed the construction worker by his bright-colored vest and pretended to shove him out the door. For the past few months construction workers have been causing a ruckus renovating the brick exterior of our 17-story building. My neighbor’s retired; he hangs out at home most days, and has to suffer the deafening noise of old bricks being carved out. The worker, smiling sheepishly, shrugged an apology.Yet New York City noise is no joke. Noise is New Yorkers’ single greatest quality-of-life complaint. Which is why I was keen to read about an advance over noise-cancelling headphones Sheng Shen presented last week in Budapest, at SIGCOMM 2018, a prestigious data communications and networking conference. Shen, a Ph.D. student at the University of Illinois, interested in wearable devices and Internet-of-Things tech, gave a talk on his new noise-cancelling earpiece, called MUTE, based on a paper he co-authored. The prototype device is a “glimpse toward the future,” the researchers write.Most popular noise-cancelling headphones block sound in two ways: by providing thick cover over the ears and by emitting an anti-noise signal. The headphones generate that signal after embedded microphones detect noise. The flaw in this design is that noise you want to avoid often reaches your eardrum before the headphones have enough time to generate a counter signal. There’s a tight deadline, in other words, and the “penalty for missing this deadline is a phase error,” the researchers write. In essence, “the anti-noise signal is not a perfect ‘opposite’ of the actual sound, but lags behind.”MUTE avoids this, but at the cost of some mobility. You can’t walk around town with it because the earpiece needs to be paired with a stationary receiver—which my neighbor, if he had the device, might place on his window. It exploits the fact that wireless signals travel faster than sound. The receiver could pick up the construction noise and beam the anti-noise signal to the MUTE earpiece before the sound has enough time to hit his eardrum. An office worker could also stick it on her door or cubicle wall, to mute loud corridor conversations or music. (Alas, just now, MUTE is only a concept. Take note, manufacturers.) Haitham Hassanieh, one of the paper’s co-authors, said, “This is bound to change the way we think of noise cancellation, where networks of [Internet-of-Things] sensors coordinate to enable quieter and more comfortable environments.”It’s also bound to remind us to ask why noise is so annoying in the first place. It’s actually a complex question, says Jonathan Berger, a professor of music at Stanford, where he teaches composition, music theory, and cognition at the Center for Computer Research in Music and Acoustics. “I think there’s a temporal aspect to it: Very quick, very short outbursts of noises are incredibly upsetting, and very long, prolonged noises that obscure clarity, are very upsetting,” he told Nautilus features editor Kevin Berger. Berger (no relation to Kevin), an acclaimed composer himself, explains that context and culture often determine whether one hears noise or not. “If we think about noise as irresoluble, incomprehensible, difficult to pull meaning out of, that would explain the first audiences of Beethoven’s first symphony, who reportedly objected to it as unfathomable noise.”\n\tThe newest and most popular articles delivered right to your inbox!\nBrian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher. "
  },
  {
    "imageUrl": "http://static.nautil.us/15230_17d079a0102e73dbd8c7cd98b6716b5b.jpg",
    "title": "Why the Tiny Weight of Empty Space Is Such a Huge Mystery",
    "description": "Posted by Natalie Wolchover on September 05, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.The controversial idea that our universe is just a random bubble in an endless, frothing multiverse arises logically…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.The controversial idea that our universe is just a random bubble in an endless, frothing multiverse arises logically from nature’s most innocuous-seeming feature: empty space. Specifically, the seed of the multiverse hypothesis is the inexplicably tiny amount of energy infused in empty space—energy known as the vacuum energy, dark energy, or the cosmological constant. Each cubic meter of empty space contains only enough of this energy to light a lightbulb for 11-trillionths of a second. “The bone in our throat,” as the Nobel laureate Steven Weinberg once put it, is that the vacuum ought to be at least a trillion trillion trillion trillion trillion times more energetic, because of all the matter and force fields coursing through it. Somehow the effects of all these fields on the vacuum almost equalize, producing placid stillness. Why is empty space so empty?While we don’t know the answer to this question—the infamous “cosmological constant problem”—the extreme vacuity of our vacuum appears necessary for our existence. In a universe imbued with even slightly more of this gravitationally repulsive energy, space would expand too quickly for structures like galaxies, planets, or people to form. This fine-tuned situation suggests that there might be a huge number of universes, all with different doses of vacuum energy, and that we happen to inhabit an extraordinarily low-energy universe because we couldn’t possibly find ourselves anywhere else.Some scientists bristle at the tautology of “anthropic reasoning” and dislike the multiverse for being untestable. Even those open to the multiverse idea would love to have alternative solutions to the cosmological constant problem to explore. But so far it has proved nearly impossible to solve without a multiverse. “The problem of dark energy [is] so thorny, so difficult, that people have not got one or two solutions,” said Raman Sundrum, a theoretical physicist at the University of Maryland.To understand why, consider what the vacuum energy actually is. Albert Einstein’s general theory of relativity says that matter and energy tell space-time how to curve, and space-time curvature tells matter and energy how to move. An automatic feature of the equations is that space-time can possess its own energy—the constant amount that remains when nothing else is there, which Einstein dubbed the cosmological constant. For decades, cosmologists assumed its value was exactly zero, given the universe’s reasonably steady rate of expansion, and they wondered why. But then, in 1998, astronomers discovered that the expansion of the cosmos is in fact gradually accelerating, implying the presence of a repulsive energy permeating space. Dubbed dark energy by the astronomers, it’s almost certainly equivalent to Einstein’s cosmological constant. Its presence causes the cosmos to expand ever more quickly, since, as it expands, new space forms, and the total amount of repulsive energy in the cosmos increases.However, the inferred density of this vacuum energy contradicts what quantum field theory, the language of particle physics, has to say about empty space. A quantum field is empty when there are no particle excitations rippling through it. But because of the uncertainty principle in quantum physics, the state of a quantum field is never certain, so its energy can never be exactly zero. Think of a quantum field as consisting of little springs at each point in space. The springs are always wiggling, because they’re only ever within some uncertain range of their most relaxed length. They’re always a bit too compressed or stretched, and therefore always in motion, possessing energy. This is called the zero-point energy of the field. Force fields have positive zero-point energies while matter fields have negative ones, and these energies add to and subtract from the total energy of the vacuum.The total vacuum energy should roughly equal the largest of these contributing factors. (Say you receive a gift of $10,000; even after spending $100, or finding $3 in the couch, you’ll still have about $10,000.) Yet the observed rate of cosmic expansion indicates that its value is between 60 and 120 orders of magnitude smaller than some of the zero-point energy contributions to it, as if all the different positive and negative terms have somehow canceled out. Coming up with a physical mechanism for this equalization is extremely difficult for two main reasons.First, the vacuum energy’s only effect is gravitational, and so dialing it down would seem to require a gravitational mechanism. But in the universe’s first few moments, when such a mechanism might have operated, the universe was so physically small that its total vacuum energy was negligible compared to the amount of matter and radiation. The gravitational effect of the vacuum energy would have been completely dwarfed by the gravity of everything else. “This is one of the greatest difficulties in solving the cosmological constant problem,” the physicist Raphael Bousso wrote in 2007. A gravitational feedback mechanism precisely adjusting the vacuum energy amid the conditions of the early universe, he said, “can be roughly compared to an airplane following a prescribed flight path to atomic precision, in a storm.”Compounding the difficulty, quantum field theory calculations indicate that the vacuum energy would have shifted in value in response to phase changes in the cooling universe shortly after the Big Bang. This raises the question of whether the hypothetical mechanism that equalized the vacuum energy kicked in before or after these shifts took place. And how could the mechanism know how big their effects would be, to compensate for them?So far, these obstacles have thwarted attempts to explain the tiny weight of empty space without resorting to a multiverse lottery. But recently, some researchers have been exploring one possible avenue: If the universe did not bang into existence, but bounced instead, following an earlier contraction phase, then the contracting universe in the distant past would have been huge and dominated by vacuum energy. Perhaps some gravitational mechanism could have acted on the plentiful vacuum energy then, diluting it in a natural way over time. This idea motivated the physicists Peter Graham, David Kaplan, and Surjeet Rajendran to discover a new cosmic bounce model, though they’ve yet to show how the vacuum dilution in the contracting universe might have worked.In an email, Bousso called their approach “a very worthy attempt” and “an informed and honest struggle with a significant problem.” But he added that huge gaps in the model remain, and “the technical obstacles to filling in these gaps and making it work are significant. The construction is already a Rube Goldberg machine, and it will at best get even more convoluted by the time these gaps are filled.” He and other multiverse adherents see their answer as simpler by comparison.Natalie Wolchover is a senior writer at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScience and other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award and the 2016 Evert Clark/Seth Payne Award for young science journalists. @NattyOver\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15190_b327c02ddad53e2fbb1c163884ae837f.png",
    "title": "Why Lunar Ice Caps Don’t Change My Moon Base Design",
    "description": "Posted by Andy Weir on August 24, 2018  When I wrote The Martian, the general belief was that Mars had very little water, and what supply it did have would be at the poles. After the book came out, Curiosity landed and discovered…",
    "category": "Ideas",
    "content": "When I wrote The Martian, the general belief was that Mars had very little water, and what supply it did have would be at the poles. After the book came out, Curiosity landed and discovered enormous amounts of ice mixed in with the Martian soil. All that hard work Mark Watney did to make water was a waste of time. He could have just heated up some dirt. My next book, Artemis, features a city on the Moon. It released back in November of last year. And now, a recent announcement by JPL has confirmed the existence of icy patches on the lunar poles. Have I been bitten in the ass yet again by the March of Science?Personally, I’m stoked by the discovery. I’m looking forward to seeing how it will affect space exploration in the real world. It may be possible to send automated systems to collect water, electrolyze it into hydrogen and oxygen, and manufacture rocket fuel. Having a large store of rocket fuel already outside Earth’s gravity well could mean a lot for deep space exploration. But to the fictional citizens of Artemis, it’s largely irrelevant. And the reason is simple economics.The drinking water in Artemis is a closed system.Artemis takes place in a setting where the cost of putting mass into Low-Earth Orbit (LEO) has been driven down by competing space industries. And the cost of soft-landing mass on the Moon is low enough that an entire lunar vacation would run about $70,000 in today’s money. I wrote a whole article about those economics for Business Insider if you want to check it out.Here’s the bottom line: Within the story, you can ship six kilograms of mass from Earth to Artemis for $1,000 (in today’s money). Also, one of Artemis’s main industries is aluminum smelting. Aluminum ore is plentiful on the surface, and the smelting process creates oxygen as a byproduct.Water has two uses for future lunar colonization: Humans drink it, and it can be used to make fuel. Let’s take them one at a time.The drinking water in Artemis is a closed system. People drink it, they urinate it, it gets purified, and it just goes round and round. So the city doesn’t actually consume water. It just moves the water around in an out of humans and treatment facilities. As the population increases, they do need to add more water, but this can be done very cheaply by importing hydrogen. For $1,000 they can bring 6 kg of hydrogen from Earth. Mix that with their ample oxygen supply and it makes 54 kg (which is 54 liters) of water. That means creating water on Artemis effectively costs $18.50 per liter.Artemis’s main source of income is tourism. The city is next to the Apollo 11 landing site, which is right near the lunar equator. So it’s about as far it can be from either lunar pole. Transporting water from a polar facility would require a pipeline 2,700 km long (that’s 1700 miles in Freedom Units). There’s just no way the cost of building that pipeline would be economical compared to the price of importing hydrogen.As for fuel, Artemis already manufactures rocket fuel, just not out of hydrogen and oxygen. They use aluminum and oxygen instead (the products of their aluminum smelters). Powdered aluminum and oxygen was the main propellant used by the Space Shuttle’s solid rocket boosters. It’s an extremely good fuel, and Artemis can make as much as they want.Polar ice is an incredibly important find here in the real world. But in the fictional setting of Artemis’s economy, it’s just not worth the effort to collect.Andy Weir is the author of The Martian and Artemis.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15200_b1370fcd515bccf46591ed09a543d21b.jpg",
    "title": "Black Hole Firewalls Could Be Too Tepid to Burn",
    "description": "Posted by Charlie Wood on August 29, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.Despite its ability to bend both minds and space, an Einsteinian black hole looks so simple a child could draw it.…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.Despite its ability to bend both minds and space, an Einsteinian black hole looks so simple a child could draw it. There’s a point in the center, a perfectly spherical boundary a bit farther out, and that’s itThe point is the singularity, an infinitely dense, unimaginably small dot contorting space so radically that anything nearby falls straight in, leaving behind a vacuum. The spherical boundary marks the event horizon, the point of no return between the vacuum and the rest of the universe. But according to Einstein’s theory of gravity, the event horizon isn’t anything that an unlucky astronaut would immediately notice if she were to cross it. “It’s like the horizon outside your window,” said Samir Mathur, a physicist at Ohio State University. “If you actually walked over there, there’s nothing.”In 2012, however, this placid picture went up in flames. A team of four physicists took a puzzle first put forward by Stephen Hawking about what happens to all the information that falls into the black hole, and turned it on its head. Rather than insisting that an astronaut (often named Alice) pass smoothly over the event horizon, they prioritized a key postulate of quantum mechanics: Information, like matter and energy, must never be destroyed. That change ended up promoting the event horizon from mathematical boundary to physical object, one they colorfully named the wall of fire.“It can’t be empty, and it turns out it has to be full of a lot of stuff, a lot of hot stuff,” said Donald Marolf, a physicist at the University of California, Santa Barbara, and one of the four co-authors. The argument caused an uproar in the theoretical physics community, much as if cartographers suggested that instead of an imaginary line on their maps, Earth’s equator was actually a wall of bright red bricks.The news of a structure at the boundary didn’t shock Mathur, however. For more than a decade he had been arguing that black holes are really balls of strings (from string theory) with hot, fuzzy surfaces. “As you come closer and closer it gets hotter and hotter, and that’s what causes the burning,” he explained.In recent years, Mathur has been refining his “fuzzball” description, and his most recent calculations bring marginally good news for Alice. While she wouldn’t live a long and healthy life, the horizon’s heat might not be what does her in.Fuzzballs are what you get when you apply string theory, a description of nature that replaces particles with strings, to extremely dense objects. Energize a particle and it can only speed up, but strings stretch and swell as well. That ability to expand, combined with additional flexibility from postulated extra dimensions, makes strings fluff up when enough of them are packed into a small space. They form a fuzzy ball that looks from afar like an ordinary black hole—it has the same size (for a given mass) and emits the same kind of “Hawking radiation” that all black holes emit. As a bonus, the slightly bumpy surface changes the way it emits particles and declaws Hawking’s information puzzle, according to Mathur. “It’s more like a planet,” he said, “and it radiates from that surface just like anything else.”His new work extends arguments from 2014, which asked what would happen to Alice if she were to fall onto a supermassive fuzzball akin to the one at the heart of our galaxy—one with the mass of millions of suns. In such situations, the force of gravity dominates all others. Assuming this constraint, Mathur and his collaborator found that an incoming Alice particle had almost no chance of smashing into an outgoing particle of Hawking radiation. The surface might be hot, he said, but the way the fuzzball expands to swallow new material prevents anything from getting close enough to burn, so Alice should make it to the surface.In response, Marolf suggested that a medium-size fuzzball might still be able to barbecue Alice in other ways. It wouldn’t drag her in as fast, and in a collision at lower energies, forces other than gravity could singe her, too.Mathur’s team recently took a more detailed look at Alice’s experience with new calculations published in the Journal of High Energy Physics. They concluded that for a modest fuzzball—one as massive as our sun—the overall chance of an Alice particle hitting a radiation particle was slightly higher than they had found before, but still very close to zero. Their work suggested that you’d have to shrink a fuzzball down to a thousand times smaller than the nanoscale before burning would become likely.By allowing Alice to reach the surface more or less intact (she would still undergo an uncontroversial and likely fatal stretching), the theory might even end up restoring the Einsteinian picture of smooth passage across the boundary, albeit in a twisted form. There might be a scenario in which Alice went splat on the surface while simultaneously feeling as if she were falling through open space, whatever that might mean.“If you jump onto [fuzzballs] in one description, you break up into little strings. That’s the splat picture,” Mathur said. We typically assume that once her particles start breaking up, Alice ceases to be Alice. A bizarre duality in string theory, however, allows her strings to spread out across the fuzzball in an orderly way that preserves their connections, and, perhaps, her sense of self. “If you look carefully at what [the strings] are doing,” Mathur continued, “they’re actually spreading in a very coherent ball.”The details of Mathur’s picture remain rough. And the model rests entirely on the machinery of string theory, a mathematical framework with no experimental evidence. What’s more, not even string theory can handle the messiness of realistic fuzzballs. Instead, physicists focus on contrived examples such as highly organized, extra-frigid bodies with extreme features, said Marika Taylor, a string theorist at the University of Southampton in the U.K.Mathur’s calculations are exploratory, she said, approximate generalizations from the common features of the simple models. The next step is a theory that can describe the fuzzball’s surface at the quantum level, from the point of view of the string. Nevertheless, she agreed that the hot firewall idea has always smelled fishy from a string-theory perspective. “You suddenly transition from ‘I’m falling perfectly happily’ to ‘Oh my God, I’m completely destroyed’? That’s unsatisfactory,” she said.Marolf refrained from commenting on the latest results until he finished discussing them with Mathur, but said that he was interested in learning more about how the other forces had been accounted for and how the fuzzball surface would react to Alice’s visit. He also pointed out that Mathur’s black hole model was just one of many tactics for resolving Hawking’s puzzle, and there was no guarantee that anyone had hit on the right one. “Maybe the real world is crazier than even the things we’ve thought of yet,” he said, “and we’re just not being clever enough.”Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15164_a7ef1270d275a7879bf01af8385dbd91.jpg",
    "title": "The Self-made Beauty of the Centriole",
    "description": "Posted by Tim Vernimmen on August 21, 2018  This story was originally published by Knowable Magazine.You don’t often see the word beautiful in scientific articles. Yet it’s easy to see why cell biologists Niccolò Banterle…",
    "category": "Biology",
    "content": "This story was originally published by Knowable Magazine.You don’t often see the word beautiful in scientific articles. Yet it’s easy to see why cell biologists Niccolò Banterle and Pierre Gönczy used the word when describing a crucial cell structure called the centriole in a recent review. The scientists, of the Swiss Federal Institute of Technology Lausanne, are helping to provide a more precise view of how our cells construct this microscopic marvel.Seen in the image in cross section, the centriole is a cylindrical cell organelle built of nine elegant groupings of three hollow tubes known as microtubules. Each centriole is about one-300th the width of a thickish human hair (about 250 nanometers) in diameter and as much as twice as long. It sits in an area of the cell that looks distinctly fuzzy under the microscope; that fuzzy spot plus the centriole are together known as the centrosome. Each human body contains trillions of them. Centrosome disorders have been implicated in a variety of birth defects, such as certain forms of dwarfism and microcephaly, and may have a role in cancer.Centrioles have existed for over two billion years and are found in a wide variety of life forms that may use them in different ways. In single-celled organisms such as Euglena or Trypanosoma brucei (the parasite that causes sleeping sickness), centrioles form the core of a flagellum, a whipping appendage that enables cells (including human sperm cells) to propel themselves. That’s probably the job that centrioles first evolved to do, says Gönczy.Over time, though, the same structure was adapted for another function: fine-tuning the orderly division of cells. Every one of us starts out as a single fertilized egg cell, which then divides into two daughter cells that each produce two daughters of their own, and so on. Getting enough divisions done in the right way and in adequate time is crucial for correctly structuring developing tissues.Here’s what goes on as cells divide: At the start of division, the centrosome splits in half, designating two centrioles to each portion. The two resulting centrosomes move to opposite sides of the cell. Microtubules sprouting from each centrosome then grow toward the cell’s center, where the chromosomes containing the cell’s genetic material are lined up, waiting. Microtubules latch onto each chromosome, and then—at the right time—drag it toward the pole they are attached to. This way, the cell makes sure that each daughter gets a copy of every bit of DNA.Centrosomes aren’t strictly required for this, explains cell biologist Anna Akhmanova of Utrecht University in the Netherlands. “Experiments show that many cells can also divide without them, but the entire process then becomes very messy,” she says. For one thing, it takes the cell much longer to properly organize the microtubules for division. For another, chromosomes might end up in the wrong cell or be lost altogether. The process still sort of works, but the centrosome makes it robust, Akhmanova says.Of course, the result of division is that a cell now has only two centrioles. To coordinate the next division, it will need four, so it makes two more before it divides again. Here’s how that happens: A new centriole is born on the side of an old one in a distinct position marked by the accumulation of an unusual, somewhat suicidal protein called PLK4. “Most of the time,” Gönczy explains, “newly formed PLK4 proteins mark themselves for destruction and are recycled by the cell.” But once in a cell’s life, they pile up in one spot on the side of each old centriole. And a brand new centriole is built.To guide the construction of that new centriole, the cell first builds a scaffold around the old centriole. That scaffold captures nine pairs of a protein called SAS-6, which is the originator of the centriole’s striking ninefold symmetry. The SAS-6 proteins form a circular hub. On that hub, nine spokes—part SAS-6, part other proteins—then attach, translating the symmetry of the SAS-6 hub into the developing structure. At the “pinheads” at the end of the spokes, first one, then two, then three microtubules form and then merge together into a sturdy, full-grown centriole.That sturdiness is important, explains Gönczy, because the centrioles undergo a lot of wear and tear. Last year, a group of scientists led by biologist Tim Stearns of Stanford University studied mutant human cells that had centrioles with single, instead of triplet, microtubules. They observed that during cell division, many centrioles were torn apart.Human cells can’t survive without centrioles or centrosomes, but some of us do manage to get by with fairly faulty ones, explains cell biologist Fanni Gergely of the University of Cambridge in the UK. “Some people have mutations in the genes coding for centrosome proteins that lead to the production of rather unusual centrosomes,” she says. Others produce cells containing too many.The results of those errors are usually similar: Cell division occurs, but with a large delay that’s made worse by biological controls that eliminate irregularly or slowly dividing cells. “To a developing embryo, which has only a limited amount of time to grow all the cells it needs, this is a big problem—especially in areas such as the brain where the number of cells is largely fixed at birth,” says Gergely. Low cell numbers mean that people with some of these mutations may be born with very small bodies or, more commonly, just small heads and brains—a condition known as primary microcephaly that is linked to cognitive impairments.Gergely is trying to understand how that happens. She’s intrigued, too, by what similar mutations do if they emerge later in life. “The same centriolar defects that delay cell division in embryos may cause cancer in adults,” she says. A recent screening study, for example, revealed that many cancer cells have extra centrioles. The centrioles also are often abnormally long, causing them to produce too many microtubules, which can lead to an inaccurate partition of the chromosomes and make cancer cells more invasive. Gergely says that some new, experimental therapies aim to sabotage cancer-cell centrioles, though the results of these trials are still to come.Centrioles are not just important during cell division: Microtubules also grow from the centrosomes of cells that aren’t dividing. Instead of dragging chromosomes around, they form part of the cell’s inner skeleton—a mesh of protein filaments that provides strength and support, enables a cell to move and coordinates the transport of materials within it. Scientists are learning how the centrosomes help these microtubules grow.At high concentrations in the lab, the proteins that microtubules are made from—called tubulin—join together into hollow tubes entirely spontaneously. In cells, they never do, Akhmanova and Utrecht University colleague Jingchao Wu wrote in 2017 in the Annual Review of Cell and Developmental Biology.“The cell doesn’t want to grow microtubules everywhere,” Akhmanova says. So it keeps things in check, again by building a scaffold to serve as a guide inside that microscopically fuzzy area surrounding the centriole. The tubulin proteins will click together, “like Lego blocks,” she says, only with this scaffold’s help.The centrosome is not the only place where this happens, Akhmanova adds. As cells specialize, microtubules may sprout at other places. In this way, every cell type grows the skeleton it needs to move around or to transport, absorb or release molecules, without requiring detailed, individual instructions for each item. “Cells build themselves, without any ‘plan’ of where to put what,” Akhmanova says. Yet it all works beautifully.Tim Vernimmen is a freelance journalist based near Antwerp, Belgium. He writes about the science of life.Editor’s note: This story was updated June 4, 2018, to clarify the affiliations of cell biologists Pierre Gönczy and Niccolò Banterle. They work at the Swiss Federal Institute of Technology Lausanne; the Swiss Institute for Experimental Cancer Research, cited in an earlier version of the story, is a department within EPFL.Knowable Magazine is an independent journalistic endeavor from Annual Reviews.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15188_8a20d7c7b4ca634d08739cf614e6063c.jpg",
    "title": "How Insulin Helped Create Ant Societies",
    "description": "Posted by Jordana Cepelewicz on August 27, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.Ants, wasps, bees, and other social insects live in highly organized “eusocial” colonies where throngs of females…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.Ants, wasps, bees, and other social insects live in highly organized “eusocial” colonies where throngs of females forgo reproduction—usually viewed as the cornerstone of evolutionary fitness—to serve the needs of a few egg-laying queens and their offspring. How they got that way has been hard to explain despite more than 150 years of biologists’ efforts. Many researchers have thought the answer would come down to a complex suite of genetic changes that evolved in species-specific ways over a long time.But new results suggest that a surprisingly simple hormonal mechanism—one that can be found throughout the animal kingdom—may have been enough to set eusociality in motion.Last month, a team of researchers led by Daniel Kronauer, an evolutionary biologist at the Rockefeller University in New York, published a paper in Science that many experts are saying provides one of the most detailed molecular stories to date in the study of eusocial behavior.The scientists found that division of reproductive labor in ants arose when an ancient insulin signaling pathway, typically involved in maintaining nutrition and growth, became responsive to social cues. In doing so, they also uncovered deeper insights into “a process underlying how the environment gets under the skin to affect behavior, physiology, and the health and well-being of other members of a society,” said Gene E. Robinson, an entomologist and the director of the Carl R. Woese Institute for Genomic Biology at the University of Illinois at Urbana-Champaign.Kronauer and his colleagues, hoping to uncover a common origin for the ants’ evolutionary journey toward eusociality, first compared which genes were expressed differently in the brains of the queens and workers among seven diverse ant species. They found a particularly strong signal for one gene, ilp2, which codes for the ant version of insulin and was expressed consistently higher in queens. (At least two dozen other genes emerged as important as well, Kronauer noted—many of them also related to insulin production and signaling, or to brain plasticity and other traits.)To determine the role of ilp2, the researchers focused on a single ant species, the clonal raider ants Ooceraea biroi, whose colonies lack fixed queens. Instead, the ants alternate as a group between worker and queen roles, seemingly in response to the presence of larvae: With babies around, all the adult ants stopped reproducing to take care of them.Kronauer’s team found that insulin signaling was responsible for that cycle. Production of the hormone declined when the researchers exposed the ants to larvae, suppressing reproduction and inducing the shift to caretaking behavior. When the larvae were removed, insulin levels rose significantly—and injecting the adults with insulin caused their ovaries to reactivate even when the larvae were still around. “If you think about it, it’s a crazy but also very elegant and simple way to make an organism social, to make it responsive to larvae,” Kronauer said.(And as Allen J. Moore, an entomologist at the University of Georgia, joked, “Anyone who’s a parent knows that your kid manipulates you.”)“That this single gene has such a major effect suggests that the transition from a solitary to a social lifestyle can begin with relatively few changes,” said Andrew Suarez, a biologist at the University of Illinois at Urbana-Champaign. “You don’t need to invoke novel genes. You don’t need to massively change the genomic architecture or gene expression patterns. You can just tweak one or a few things, and start on this path toward advanced reproductive division of labor.”Kronauer’s results vindicate a theory about the origins of eusociality proposed in 1987 by the evolutionary biologist Mary Jane West-Eberhard, now at the Smithsonian Tropical Research Institute. She had observed that solitary wasps cycled through reproductive and caretaking phases in sync with their ovarian activity, and posited that eusocial division of labor emerged when parts of that ovarian cycle became exclusive to each caste: The queens had constantly active ovaries for egg laying, while the workers, whose ovaries stayed suppressed, dedicated themselves to foraging and brood care. West-Eberhard later described this as a model for making major changes in species through “developmental reorganization” and called it a way of “making something new (worker and queen phenotypes) out of old pieces”—in this case, the old behaviors linked to different phases of the solitary wasps’ ovarian cycle.What was missing from West-Eberhard’s theory was a candidate for a potential trigger for that reorganization in the wasps, or in any of the other social insects. Kronauer’s findings now suggest that the culprit, at least in the case of ants, was ilp2, with the larvae manipulating the adults through their insulin pathways to turn most into full-time caretakers and a few into mothers to the community.Insulin’s involvement makes sense in retrospect, Kronauer said, given that the hormone is known to play a crucial regulatory role in both food intake and reproduction. After the initial adaptation, evolutionary forces would have driven innate differences in insulin levels among individuals further apart to cement separate castes. Even among the queenless clonal raider ants, Kronauer and his colleagues observed that some had slightly larger, more active ovaries and foraged less, despite the presence of larvae. Their insulin levels turned out to be higher from the start.“Even in this precursory state, there still seems to be a connection between reproductive behavior and insulin levels,” Kronauer said. Ultimately, individuals with higher insulin levels became queens, and those with lower levels became workers. “It starts out in a population where everyone is very similar, but over evolutionary time those small differences get exacerbated.”What makes this work additionally compelling is that previous research implicated insulin in governing division of labor in honeybees as well—but in a very different way. For those bee species, which evolved eusociality independently from the ants, insulin signaling helps determine whether workers forage or stay behind to nurse larvae, and can govern the kind of food the foragers prefer (which further affects their physiology and the tasks they perform). In both the ants and the honeybees, the insulin mechanism is entwined with sensitivity to offspring.Anyone who’s a parent knows that your kid manipulates you.“It suggests that there’s something very general about how evolution proceeds when it increases the complexity of a system,” Kronauer said. To Suarez, it means that this kind of evolutionary innovation is in some sense “predictable, that there are genetic mechanistic patterns at play. That’s pretty exciting.”That insulin has been used multiple times independently also reinforces an emerging insight that evolution routinely reuses conserved metabolic and developmental pathways to give rise to complex new traits and behaviors. In the case of social insects, the insulin and reproductive pathways were only waiting to be coopted for social functions—or “derived from ancestral ground plans,” said Karen Kapheim, an evolutionary biologist at Utah State University.“Evolution is a tapestry of something old, something new,” Robinson said. “We see this beautifully on display in this work: the ancient, highly conserved insulin pathway with this new piece interposed—that one particular life stage, the babies, can influence the insulin signaling status, and therefore the physiological state of the adults.”Kronauer and others still need to determine how the larvae elicited that signaling response in the adults, and how insulin became responsive to social cues in the first place. More work is also needed on what’s happening in the ants’ brains to mediate this process.The fact that insulin signaling is so important in other animals has wider implications as well. For instance, insect queens are significantly larger than workers and enjoy considerably longer life spans. According to Kronauer, the underlying reasons for these differences remain unknown, but his study implies that insulin signaling could be at play. “People can now start to look at whether insulin signaling in other organisms, including in humans, plays a similar role in modulating something like life expectancy,” he said.And perhaps it plays a role in the evolution of primitive social behaviors in other species as well, added Moore, who studies transitions to social organization in beetles. In his field, “we don’t have such a complete, causative story,” he said. “It really raises the bar for the rest of us.”Jordana Cepelewicz is a staff writer at Quanta Magazine.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15195_59b109c700b500daa9ef3a6769bc8c6f.jpg",
    "title": "Forget Everything You Think You Know About Time",
    "description": "Posted by Brian  Gallagher on August 28, 2018  In April, in the famous Faraday Theatre at the Royal Institution in London, Carlo Rovelli gave an hour-long lecture on the nature of time. A red thread spanned the stage, a metaphor for…",
    "category": "Ideas",
    "content": "In April, in the famous Faraday Theatre at the Royal Institution in London, Carlo Rovelli gave an hour-long lecture on the nature of time. A red thread spanned the stage, a metaphor for the Italian theoretical physicist’s subject. “Time is a long line,” he said. To the left lies the past—the dinosaurs, the big bang—and to the right, the future—the unknown. “We’re sort of here,” he said, hanging a carabiner on it, as a marker for the present.Then he flipped the script. “I’m going to tell you that time is not like that,” he explained.Rovelli went on to challenge our common-sense notion of time, starting with the idea that it ticks everywhere at a uniform rate. In fact, clocks tick slower when they are in a stronger gravitational field. When you move nearby clocks showing the same time into different fields—one in space, the other on Earth, say—and then bring them back together again, they will show different times. “It’s a fact,” Rovelli said, and it means “your head is older than your feet.” Also a non-starter is any shared sense of “now.” We don’t really share the present moment with anyone. “If I look at you, I see you now—well, but not really, because light takes time to come from you to me,” he said. “So I see you sort of a little bit in the past.” As a result, “now” means nothing beyond the temporal bubble “in which we can disregard the time it takes light to go back and forth.”Rovelli turned next to the idea that time flows in only one direction, from past to future. Unlike general relativity, quantum mechanics, and particle physics, thermodynamics embeds a direction of time. Its second law states that the total entropy, or disorder, in an isolated system never decreases over time. Yet this doesn’t mean that our conventional notion of time is on any firmer grounding, Rovelli said. Entropy, or disorder, is subjective: “Order is in the eye of the person who looks.” In other words the distinction between past and future, the growth of entropy over time, depends on a macroscopic effect—“the way we have described the system, which in turn depends on how we interact with the system,” he said.Getting to the last common notion of time, Rovelli became a little more cautious. His scientific argument that time is discrete—that it is not seamless, but has quanta—is less solid. “Why? Because I’m still doing it! It’s not yet in the textbook.” The equations for quantum gravity he’s written down suggest three things, he said, about what “clocks measure.” First, there’s a minimal amount of time—its units are not infinitely small. Second, since a clock, like every object, is quantum, it can be in a superposition of time readings. “You cannot say between this event and this event is a certain amount of time, because, as always in quantum mechanics, there could be a probability distribution of time passing.” Which means that, third, in quantum gravity, you can have “a local notion of a sequence of events, which is a minimal notion of time, and that’s the only thing that remains,” Rovelli said. Events aren’t ordered in a line “but are confused and connected” to each other without “a preferred time variable—anything can work as a variable.”Even the notion that the present is fleeting doesn’t hold up to scrutiny. It is certainly true that the present is “horrendously short” in classical, Newtonian physics. “But that’s not the way the world is designed,” Rovelli explained. Light traces a cone, or consecutively larger circles, in four-dimensional spacetime like ripples on a pond that grow larger as they travel. No information can cross the bounds of the light cone because that would require information to travel faster than the speed of light.“In spacetime, the past is whatever is inside our past light-cone,” Rovelli said, gesturing with his hands the shape of an upside down cone. “So it’s whatever can affect us. The future is this opposite thing,” he went on, now gesturing an upright cone. “So in between the past and the future, there isn’t just a single line—there’s a huge amount of time.” Rovelli asked an audience member to imagine that he lived in Andromeda, which is two and a half million light years away. “A million years of your life would be neither past nor future for me. So the present is not thin; it’s horrendously thick.”Listening to Rovelli’s description, I was reminded of a phrase from his new book, The Order of Time: Studying time “is like holding a snowflake in your hands: gradually, as you study it, it melts between your fingers and vanishes.” Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why the nature of time is such a central issue for theoretical physics. "
  },
  {
    "imageUrl": "http://static.nautil.us/15252_090afe0d4abb5dfdccb84641fe115680.jpg",
    "title": " Why Nuclear Power Professionals Are Serious About Joking Around",
    "description": "Posted by Vincent Ialenti on September 10, 2018  In August 2013, Finland’s young nuclear professionals, under 35 years old, met up for the Summer Games in Mikkeli municipality, put on by the Finnish Nuclear Society’s Young Generation…",
    "category": "Culture",
    "content": "In August 2013, Finland’s young nuclear professionals, under 35 years old, met up for the Summer Games in Mikkeli municipality, put on by the Finnish Nuclear Society’s Young Generation Group. Launched in 1998, the organization helps people interested in nuclear fields network. As usual, the plan was to escape office life, eat, drink, use the sauna, and bond through fun competitions outdoors. The Summer Games events often have a dark humor to them—they play around with nuclear disaster scenarios in ways outsiders might find jarring. Yet this edgy outdoor sporting is a powerful way of raising workforce morale. It’s an aspect of a culture that may, from afar, seem to admire only cold, sterile rationality.Part of what fascinates me as an anthropologist is the way nuclear experts live their lives. After living among them in Finland for almost three years, I found how they communicated in these games—using satire, jokes, and irony—surprising. In past years’ games, they pretended to be emergency responders, spraying Super Soaker water guns at a beach ball they imagined to be a melting-down reactor. Their mission was to cool down its make-believe core. In another sporting event, they used a slingshot—one resembling those in Finland’s famed Angry Birds phone games—to hurl objects at a box with balloons inside. With a nuclear reactor shape painted on the box, they imagined they were terrorists attacking a power plant.In 2013, at Mikkeli, tasks were more challenging. A large candle was placed inside a glass container, which represented a melting-down nuclear reactor and a containment building. Players, standing 16 or so feet away from the candle, had to cool it with a large water bucket and five spoons. One team devised a quirky strategy: They threw the full bucket of water at the glass, breaking it. The containment structure collapsed. The imagined reactor shut down. In yet another game, players carried backpacks full of expensive radiation detection equipment lent to them by a Finnish company that designs technology for special military operations. Their mission was to search for a slightly radioactive object hidden in a hot Finnish sauna.When chatting with Finland’s nuclear youths, I wondered: How could they treat serious threats and grave tragedies, like nuclear meltdowns or terrorist attacks, in such light-hearted ways, with all the silliness of a hide-and-seek game? One of the organizers offered an explanation: The Summer Games alleviated the stresses of daily work life by inverting them into their opposites. Nuclear risks were treated humorously and playfully at the yearly Summer Games retreats precisely because they were treated seriously and sternly on the job every day.Dark humor is deeply human.A behavioral scientist might say Finland’s nuclear youths used low-stakes gaming to create a “benign violation” zone, a detached distance from tragedy. This was an unserious space where meltdowns, war battles, and terrorism could momentarily seem funny. This allowed them to let off steam, kick back, and unwind together by pushing aside, temporarily, grave responsibilities they shared at work. The lightness of nuclear humor became, for them, a rejuvenating counterpoint to the heaviness of nuclear risk.Dark humor is key to many other high-intensity professions as well. A 2013 article explained how dark humor acts as a social “glue” reinforcing camaraderie among police officers and ambulance crews. As criminologist Sarah Charman put it, “By normalizing a situation through humor, a stressful encounter can be made more manageable—humor allows people to control feelings of fear or vulnerability.”And yet, as anthropologist David Graeber argues, there is much more to playing than its strategic power, as a means for achieving a practical end. Play is not just a tool for maximizing self-interest, increasing productivity, or becoming a more-efficient professional. It can also momentarily free us from the cold demands of self-serving calculation and means-ends rationalism altogether. This holds even, and perhaps especially, in serious places one might, at first blush, assume to be devoid of humor. Play can be found in boring bureaucracies, among seemingly uptight police, among mission-driven EMTs, in austere prisons, amidst the tedium of office life, and even among seemingly hyper-rational nuclear engineers.So when you hear a friend or co-worker joking about serious matters, remember to reflect, before passing judgment, on the motivations behind his or her irreverence. Dark humor is not always a callous reaction to life’s tragedies. Often, it sprouts up among people who care so deeply about the day-to-day that they, every now and then, need a fun moment of release. Nor does dark humor mean one has failed to comprehend the full severity of a tragedy. On the contrary, some psychologists see dark humor’s “playful fictions” as indications of “complex information-processing” or high intelligence.Finland’s young nuclear professionals taught me that dark humor is deeply human—it can help one grow into a more open-minded, team-working, and enthusiastic employee.Vincent Ialenti is a MacArthur Nuclear Waste Solutions Fellow at George Washington University’s Elliott School of International Affairs. He holds a PhD in Anthropology from Cornell University and an MSc in “Law, Anthropology & Society” from the London School of Economics.Portions of this article are derived from “The (Un)bearable Lightness of Nuclear Energy in Finland,” published in the 2014 “Forum on Anthropologies of Humor” in Suomen Antropologi: The Journal of the Finnish Anthropological Society 39(4).\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How to understand people better.This classic Facts So Romantic post was originally published in November 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15158_b555da9b21a5a45577bb2bfb58bcfea0.png",
    "title": "7 Awesome Solar System Destinations That Will Kill You",
    "description": "Posted by John Wenz on August 17, 2018  In the why-aren’t-you-watching-this television show The Expanse, humanity has spread out into the solar system. Mars and Earth stand as bitter rivals, with Ceres settlers somewhere in…",
    "category": "Biology",
    "content": "In the why-aren’t-you-watching-this television show The Expanse, humanity has spread out into the solar system. Mars and Earth stand as bitter rivals, with Ceres settlers somewhere in between. A few companies even have settlers in the outer regions of the solar system.You wouldn’t necessarily want to live in the world of The Expanse, as fantastic as it is. Yet the show still plays to the dreams of those of us who long to wander the final frontier. However, the reality of what awaits you beyond Earth is far more dangerous than the show. Even if you make it past the interplanetary radiation, you’re still confronted with any number of hazards, and they don’t stop once you land.Here are a few places from your colonization dreams that might end up actual nightmares.The MoonThe Moon is close—tantalizingly close—and we’ve actually stepped foot there. But as the Apollo astronauts returned to Earth in their moon dust-covered space suits, every moonwalker encountered the same set of symptoms: sneezing, wheezing, sore throats, and nasal congestion. It seemed like they were all allergic to the moon.Harrison Schmitt, an Apollo 17 crew member, called it “lunar hay fever,” according to a recent European Space Agency press release. The big culprit, ESA says, is the presence of silicates—a group of silicon/oxygen compounds. Those compounds cause all of these symptoms in volcanic ash on Earth, and the Moon is littered with the stuff. But because there’s no oxygen on the Moon, the silicates on the surface aren’t eroded, making them even more dangerous: they can embed in and scar lung tissue more easily than their Earth-bound cousin. And not only is it a hazard to lunar explorers’ bodies, but it can also wear away at their equipment and spacesuits, too.So think of it: your day-to-day is surrounded by these nasty silicates. You’re tracking it in everywhere with you. It’s in your equipment. It’s in your clothes. Slowly but surely, you’re breathing it in more and more. Quartz miners have been known to come down with a condition known as silicosis, which can cause permanent lung damage. In some cases, those affected may die of complications from the disease.MarsSure, there’s water on the Red Planet. Mars’ strange and contentious “recurring slope lineae” have shown that. But these gully-like features mix a small amount of water in with a ton of perchlorates, a form of chlorine salts.And that’s dangerous.Perchlorates are incredibly toxic to humans. They’re cause for an industrial location to end up on the EPA’s Superfund site list here on Earth. Table salt—sodium chloride—is a non-toxic material. But perchlorates are chiefly made of oxygen and chlorine, which often fails to stabilize the toxic chlorine for Earth-life. Instead, you get a nasty kind of salt that can harm thyroid function, decrease bone marrow production, and damage lung tissue. Perchlorates also easily make their way into drinking water and food, which is bad if, say, you’re trying to grow potatoes in Martian soil.Microbes exposed to Mars-like conditions—including perchlorates—died shortly after exposure. Sunlight also seems to catalyze the perchlorates. In other words, they might become an environmental hazard, even inside oxygenated environments. That gives a lot of ways for the perchlorates to slowly seep into your system. Mars already has an increased radiation risk due to its lack of a magnetic field and thin atmosphere. But all the radiation protection in the world might not prepare you for a nasty chemical contaminating what drinking water you have on that dry, dry planet. Or maybe it gets into your lungs and you slowly suffocate from exposure. A good chunk of perchlorate-related deaths come from thyroid disruption, as it can induce Grave’s disease.It’s not great to be around. And it’s all over Mars.EuropaPoor old Europa isn’t out to kill you, per se. It’s a big oceanic world, after all, just waiting for us to find life. It seems to have all the right ingredients for it hiding under a thick ice shelf.The problem comes in when you consider Europa’s location: firmly within the radiation belts of Jupiter. Io and Europa are bombarded with lethal amounts of radiation. The future Europa Clipper mission even avoids orbiting Europa directly to lengthen the craft’s lifetime. If you landed Europa’s surface, the radiation dose would kill you—and anything else—within days.The ice shell, however, is thick enough that it might protect any life below. We’ll just never be able to say hi. Perhaps that’s why any far-reaching human exploration plan for the solar system aims to land on Callisto, the outermost of the four large moons of Jupiter. It has an insignificant amount of exposure to Jupiter’s magnetosphere.TitanTitan is such a tempting place. This planet-sized moon of Saturn looks much like early Earth. It has a thick atmosphere—the only moon known to have one so dense—and bodies of liquid all over the planet.Except it’s all hydrocarbons. Where’s the water? If you manage to land on the surface of Titan water has been transformed into the rock all around you, frozen solid by frigid temperatures. And the pitter-patter of rain is actually a chemical closely resembling gasoline. Though the atmospheric pressure on Titan is higher than that of Earth, the actual hydrocarbon lakes are too thin to efficiently swim through. Oh, and it’s muddy everywhere.Dig down, way, way far down, on Titan, though, and you might find a subterranean ocean of water. This means that Titan has two oceans, both closed off from each other, one on the surface and the other deep below. This could also mean Titan has two biospheres, as strange forms of nitrogen-based life using vinyl cyanide as cell walls could theoretically exist in the hydrocarbon lakes. If you stacked up all the potentially habitable places in the solar system together, the nitrogen life would easily be the most alien.Titan wouldn’t kill you in specific ways, unlike Mars or the Moon. But it’s not exactly a place you want to be, either. There aren’t a lot of resources for survival—save digging deep down or using hydrocarbons as fuel to jump to the next rock over. Let’s stick to robotic probes for now.Outer SpaceIn June 1971, the Soyuz 11 capsule and its three cosmonauts landed back on Earth after becoming the first mission to board humanity’s first space station. To their horror, the Russian space agency found the cosmonauts dead inside their spacecraft. Attempts to revive them didn’t work. They’d been exposed to the vacuum of space.No astronaut is going to deliberately walk out an airlock, at least if they’re still in their right mind, which is not necessarily a guarantee. The Soyuz 11 crew are a good way to contrast the fact with the fiction of exposure to space. No, you don’t explode. Instead, you’re rapidly robbed of the air in your lungs, leading to asphyxiation. The official cause of death for the cosmonauts was brain hemorrhaging, as their systems had been rapidly robbed of oxygen. They didn’t die instantly either—they probably had one panicked minute as they died, as lactic acid levels, associated with stress—were extremely high.After the accident, cosmonauts were given pressurized suits. While other space crews have died in the course of their mission, the Soyuz 11 crew remains the only one to die from exposure to the vacuum. So, uhh, make sure your spaceship is extra air tight.EnceladusEnceladus is mostly water, which is great for whatever life arises there. The biggest hurdle is its gravity.An Earth-sized hop on Enceladus could launch you 140 feet in the air. And at its south pole where Enceladus’ ocean is accessible, jets of frozen water vapor launch as high as 100 miles (161 km) at a speed of 1304 miles (2100 km) per hour.So what’s causing those intense jets? Tidal forces from Saturn. That could make for some pretty intense waves on the ocean below. Imagine trying to navigate a submarine through intense, rough waves while risking getting swept up in a plume event. Sure, when you jump at Enceladus you might have a slow path back down, but an ejection at not-quite-escape-velocity could bring you smacking back down to the ice shell of Enceladus. Hard.It might be a good place to refuel, but it’s probably not the best place to settle in.CeresLet’s pull an “actually” here. Actually … the Belters in The Expanse may have it best. Ceres was once a water world that froze over, leaving a few little deposits of liquid water here and there. But it’s mostly an iceball dwarf planet with a thin coat of dust. Peeking out from those bits of ocean are salt deposits. Unlike the salts on Mars, they’re not toxic perchlorates, but rather, magnesium sulfate. That compound isn’t toxic to humans at all. You’ve probably soaked your weary bones in it or taken it as a laxative in the form of epsom salts.And the asteroid belt is more diffuse than The Empire Strikes Back might suggest. While Ceres’ surface certainly shows it hasn’t had an easy time, most asteroids in the present day belt are, on average, about 600,000 miles (940,000 km) apart.Of course, the lack of a substantial atmosphere (most “atmospheric” pockets on Ceres are short-lived and surround sublimating ice) and magnetic fields will leave you unprotected from the harsh radiation of space, but that’s the same as on the Moon and on Mars. One of the biggest hurdles might be the abundance of ammonia on Ceres. All told, ammonia is probably a lesser hazard than some of the others we’ve discussed, but it’s hard to separate ammonia from water, and exposure to it is harsh on your respiratory system.So it might not be life friendly, but it’s certainly less out to kill you than Mars or the Moon.John Wenz is a science and technology editor and writer.\n\tThe newest and most popular articles delivered right to your inbox!\nThis article was originally published on DiscoverMagazine.com on July 9, 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/15227_16db016ed3a5b8d7c596928306161ada.jpg",
    "title": "How to Use the Large Hadron Collider to Search for Dark Matter",
    "description": "Posted by Antonio Boveia & Christopher S. Hill on September 03, 2018  While cosmologists may be fascinated by what dark matter does, particle physicists are fascinated by what dark matter is. For us, dark matter should be—naturally—a particle, albeit…",
    "category": "Matter",
    "content": "While cosmologists may be fascinated by what dark matter does, particle physicists are fascinated by what dark matter is. For us, dark matter should be—naturally—a particle, albeit one that is still lurking hidden in our data. For the last few decades, we’ve had a tantalizing guess as to what this particle might be—namely, the lightest of a new class of supersymmetric particles. Supersymmetry is an extension to the Standard Model of particles and forces that nicely addresses lingering questions about the stability of the mass of the Higgs boson, the unification of the forces, and the particle nature of dark matter. In fact, supersymmetry predicts a vast number of new particles—one for each particle we already know about. Yet while one of those new particles could constitute dark matter, to many of us that would be just a happy byproduct.But after analyzing data from the first (2010–2012) and second (2015–2018) runs of the Large Hadron Collider (LHC), we haven’t found supersymmetric particles yet—indeed, no new particles at all, beyond the Higgs boson. So, while we continue to hunt for supersymmetry, we’re also taking a fresh look at what our cosmology colleagues can tell us about dark matter. It is the strongest experimental evidence for new physics beyond the Standard Model, after all.   In fact, some might say that a principal goal of the LHC and future colliders will be to create and study dark matter. For that to happen, there must be a means for the visible universe and the dark universe to communicate with each other. In other words, the constituents of the particles that we collide must be capable of interacting with the putative dark-matter particles via fundamental forces. A force requires a force carrier, or boson. The electromagnetic force is carried by the photon, the weak nuclear force by so-called vector bosons, and so on. Interactions between dark matter and normal matter should be no different: They could happen by exchanging dark bosons.Even if our detectors are oblivious to the dark bosons themselves, we have some hope of identifying them if they have some tiny interaction with observable particles—in other words, if they are not completely dark. Given how feeble these interactions would be, the Large Hadron Collider could already be producing these particles and we simply haven’t been able to notice them yet.   After being created in the LHC when two protons collide, a dark boson might decay into dark-matter particles, which would escape our detectors without leaving a trace. But we could deduce their presence by adding up all the particles we did observe and looking for an imbalance of momentum, indicating that something had gone missing. Alternatively, dark bosons could decay into ordinary particles, such as quarks, and leave clear patterns in our data. We could do some particle forensics to infer the properties of the unseen bosons. This is just the sort of job for which the LHC detectors were designed, and we are continually scouring our collider data for these signals.In doing dark boson searches this way, though, we have made one assumption that might not be warranted: that the dark boson decays instantaneously. What if it doesn’t? The dark universe, in order to be dark, has to be sequestered from the normal universe in some way. This can cause dark bosons to survive for a short—but measurable—moment before disintegrating back into normal matter. The debris of the disintegration would not show up in our experiments at the point where the two protons collided, but displaced by some significant distance.   The LHC experiments were designed to look for particles originating from the interaction point. Tracing the trajectories of long-lived particles (dark or not) is complicated by several factors. They would be composed of fewer measurements, making it harder to connect the dots; they would follow atypical geometric paths, further hampering our pattern-recognition algorithms; and they could produce signals that would arrive much later than the usual algorithms anticipate.   But this is just the kind of challenge physicists embrace. By reviving decades-old tricks and inventing brand new methods, we have modified our algorithms to be sensitive to these atypical particle patterns. We think we can now detect dark bosons that decay up to several meters away from the place of origin, which covers most plausible scenarios. It almost doesn’t even matter what the dark boson decays into, as long as particles of normal matter, which our detectors will register, end up in the debris.So far, we have found nothing in the data from the first, low-energy run of the LHC. But we are still working on data from the second, higher-energy run. With the addition of these techniques to the supersymmetric searches that have come before it, we now have an excellent chance to discover dark matter, a dark force, or both. Considering that it has so far delivered only 1 percent of the total amount of data it will ultimately produce, the LHC’s search for dark particles has really only just begun.Antonio Boveia is a physics professor at Ohio State University in Columbus. He searches for dark matter and other new particles and forces with the ATLAS experiment at the Large Hadron Collider.Christopher S. Hill is a physics professor at Ohio State University in Columbus. In 2012–2013 he was deputy physics coordinator of the CMS Experiment at the Large Hadron Collider. He is currently the project scientist for the U.S. CMS HL-LHC Upgrade.\n\tThe newest and most popular articles delivered right to your inbox!\nThis article was originally published in February 2017 in Nautilus Cosmos. "
  },
  {
    "imageUrl": "http://static.nautil.us/15080_4b5739d494ab72c2a54540e67fc1c856.jpg",
    "title": "Many of Our Beliefs Are Unconscious: A Response to Nick Chater",
    "description": "Posted by Jim Davies on July 30, 2018  Nick Chater has put forward a bold claim in his recent book, The Mind Is Flat, as well as in an article and interview in Nautilus: that we don’t have any unconscious thoughts. A metaphor…",
    "category": "Biology",
    "content": "Nick Chater has put forward a bold claim in his recent book, The Mind Is Flat, as well as in an article and interview in Nautilus: that we don’t have any unconscious thoughts. A metaphor that Chater, a behavioral scientist, dislikes is that of the iceberg, the tip of which is our consciousness, and the vast, submerged part is our unconscious. As Chater says in the Nautilus interview, this suggests that unconscious and conscious processes use the same kinds of representations, and that the kinds of things we are unconscious of we could be conscious of.He’s certainly right that many brain processes go on that we’re unaware of, and can’t be aware of. Let’s take visual recognition as an example. We can recognize that something is an image of an animal with astounding speed. We can even do this with cartoon animals. Chater says, “With each thought, you’re taking up massive fragments of information and trying to pull them together.” But we’re not aware of how these fragments work together to constitute dogness. So how does Chater say that all thought is conscious? By tailoring the definition of “thought” to suit his conclusion.Mental health psychology uses the term for things like “unwanted thoughts,” and here is where Chater’s argument is strongest. But in cognitive psychology and cognitive science, “thought” isn’t really a technical term. We kind of know what it means, but in general, cognitive psychologists and cognitive scientists don’t use the term “thought” to represent any key concept in their theories and explanations. If Chater were to start using more technical terms, he’d find himself in much deeper water. He might even see more of the iceberg.Let’s take a term that is fairly technical in cognitive psychology: memory. There is no doubt that all of us have memories that we are never conscious of. The most uncontroversial example is what is known as “procedural memory”: the memory of how to do things. Your ability to ride a bike involves memories encoded and layed down during a long learning process. These memories are accessed and exploited whenever you ride a bike. When you learned how to ride, something changed in your brain, and whatever those changes are we call memories.Importantly, we are largely unconscious of the memories we use to we ride a bike. In fact, we are often wrong when we try to describe how we do physical activities. For example, when you want to turn right on a bike, you often actually steer left slightly first, which causes your bike to tilt right. But most people not only don’t know that they’re doing this, but they don’t believe you when you tell them!We can only be conscious of a few things at once.One thing that is well known in psychology is that practice results in “automatization.” This means that the activity becomes faster and less conscious. The first time you try to drive a car, it is experienced as a bewildering confusion of pedals, the steering wheel, and too many things to pay attention to. But after a few years of driving, you are able to hold conversations while navigating a busy city. How is this possible without unconscious thought?Chater might say that your procedural memories of how to drive aren’t “thoughts.” But he also says that unconscious things are things that cannot be conscious. When you’re learning to drive, everything is conscious, and the limits of your attention and consciousness is what makes it so hard. You can only drive safely when you’ve automatized, and made unconscious, much of what you have to do. So here we seem to have an example of a conscious thing becoming unconscious. Were they thoughts when you were conscious of them, but then ceased to be thoughts once they were learned?Perhaps. There is a large scientific literature showing that our opinions of how we move are counter to how we actually move, including crawling and catching fly balls in baseball. In my own laboratory, my student Jay Jennings ran an experiment that showed that while people (wrongly) believe that a ball spiraling within a tube will continue to spiral once it emerges, their hands move to the right place when they go to catch the ball.When there is a conscious, explicit memory that is in conflict with an unconscious memory, we have clear examples that some memories are unconscious. So Chater can’t say that all “memories” are conscious. Well, what about facts? What cognitive scientists call “declarative memories” are fact-like beliefs about the world, like your belief that peanut butter is brown. Are all declarative memories conscious?In one sense, certainly not, because you are not conscious of all of your memories all the time. We’ve all had the experience of being asked something, and responding with something like “I haven’t thought about that in years.” This means that you have memories that you had not been retrieving lately. The stored memories you are not retrieving are still memories, yet you are not (currently) conscious of them.In Chater’s own experiment, he had people retrieve in memory as many foods as they could. People quickly ran out of ideas, meaning that new ideas came more and more slowly. Let’s say it takes someone two minutes to finally come up with eggplant. They were not conscious of the thought that eggplant was a food until two minutes into the task. So what was the nature of their belief that “eggplant is a food” before conscious retrieval? I’d say it was an unretrieved declarative memory. What would Chater say? He’d have to say that it wasn’t a thought until it was conscious.But here we run into his problem with the iceberg: If the unconscious is made up of fundamentally different kinds of representations from the representations of conscious thought, then you should not be able to bring to mind thoughts that aren’t already conscious. So Chater would have to say that your memory that eggplant is a food is an unconscious something-or-other that only becomes a “thought” after it’s retrieved. But even that still feels to me like you can retrieve an unconscious belief into consciousness.This does not seem to square with his statement that we have no unconscious beliefs (in his book he claims, “The inner, mental world, and the beliefs, motives, and fears it is supposed to contain is, itself, a work of the imagination.”) Perhaps Chater thinks of a “thought” as something being used in some kind of thinking process, and unaccessed memories don’t count. In some of his examples, he talks about how your mind isn’t solving problems unconsciously. So do we ever process unconscious memories?When you’re driving, you’re using automatized, unconscious processes to do much of the work. It’s common to have driven home and have no memory of the drive, because your mind was elsewhere. There is a vast amount of visual processing that goes into driving, and this is largely unconscious.So let’s assume that Pat is thinking about what to make for dinner while driving home. She approaches a red light, and will pass through the intersection when the light turns green. She has a choice of two lanes. In the leftmost lane sits a car with a left-turn signal on. Pat enters the right lane. It is likely that some part of her mind saw the turn signal, inferred that it was deliberate, that the driver wants to turn left, that turning left at an intersection often takes time because of oncoming traffic, and that if she were to get behind this car, it would make her trip longer. Some part of her mind concludes that being in the right lane is better.Now, even if Pat were giving all of her attention to this driving situation, rather than dinner, it is very unlikely that all of these thoughts would consciously go through her head. And yet the conclusion is drawn, and the action is taken. The idea that the thoughts involved in this inference consciously go through Pat’s head while she’s planning dinner is absurd. We can only be conscious of a few things at once.However, it’s also true that if she had reason to, she could very well attend to these unconscious perceptions. Suppose she suddenly recognized the car as belonging to her friend. She might immediately be aware of the turn signal, and try to guess where her friend might be going. If there is an unconscious thing that can become conscious, then Chater’s view of the iceberg is flawed, because for this to happen there must be some unconscious memories that can become conscious, which means that they are represented in the same form.So it appears we have lots of unconscious declarative memories that seem to be processed unconsciously. If we try to cast Chater’s claims about “thoughts” into the more technical term “memory,” his claim is much less convincing.Jim Davies is a professor at the Institute of Cognitive Science at Carleton University in Ottawa, and author of Riveted: The Science of Why Jokes Make Us Laugh, Movies Make Us Cry, and Religion Makes Us Feel One with the Universe. His sister is novelist JD Spero.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The mind is a book that can be opened and read, if you know the language in which it’s written. "
  },
  {
    "imageUrl": "http://static.nautil.us/15114_45d0bb417c9fab94dae50e5974f30ec7.jpg",
    "title": " Each Piece of Trashed Plastic Can Find a New Life as Art",
    "description": "Posted by Heather Sparks on August 07, 2018  In one important way, grocery stores were very different during my childhood. Catsup was only packaged in glass bottles. Soda came in either aluminum cans or glass bottles, and there was…",
    "category": "Culture",
    "content": "In one important way, grocery stores were very different during my childhood. Catsup was only packaged in glass bottles. Soda came in either aluminum cans or glass bottles, and there was no bottled water—no Fuji, Poland Spring, or Evian. Crackers were wrapped in waxed paper. Everything was bagged in paper. Now, some 30 years later, grocery stores are full of plastic. Nearly every product is plastic-wrapped, then toted away in plastic bags. Indeed, plastic production and consumption is seemingly ever soaring. Americans disposed of over 40 billion single-use plastic water bottles in 2010, about triple the figure from 2003. Of those billions, only one-third are recycled. The rest, barely biodegradable, stay in our dumps for centuries or wind up in our waterways.With market experts predicting an enormous growth of plastic use and production in the developing world, it seems there will be no end to the PVC, PET, and polypropylene pile-up. Swept reluctantly along with this landslide, artist Sayaka Ganz converts consumer castoffs into meaningful work. She makes sculptures entirely of second-hand plastics that are in sum much greater than their parts. “Because plastic objects are quite cheap, people don’t tend to value them, but there’s a lot of thought that went into designing them as well as getting the chemical compound,” Ganz told me.Ganz created her galloping horses in Emergence by fitting together over 1,000 black and white plastic cooking utensils, hangers, and various other thrown-out ephemera. “When I fit the objects together, it’s like a puzzle to me. When you look at the sculptures up close, because each object doesn’t quite fit, they’re quite small and tied together, and so there is an open surface between each piece,” she says. “I try to align them so that they are all flowing in the same direction to create a sense of harmony. I hope it gives a hopeful message.”Ganz’s first made her sculptures with scrap metal but made the switch after a trip to a thrift store where she bought a yellow plastic chain. “I thought, ‘This is just like the metal chains I use,’ and thought maybe plastic would be good to explore. I noticed a lot of kitchen utensils are similar to the shapes I’d already been using, like shovels, garden, and farm equipment. All of the features that make plastic an ideal packaging material make it a good sculpting material.” Plastic is lightweight, cheap, and can be formed into an endless variety of shapes and colors, which explains why it’s used so much by manufacturers and consumers.In rural Nebraska, where Ganz was once at an artist-residency program, there is no trash-pickup service; the residents deal with the garbage themselves. Plastics are collected and taken to a recycling center. Glass is stored in wide, 20-foot-deep silos, and metal is saved for artists to use. The rest is burnt.Within this process, Ganz has found inspiration.“Having to live and watch the pile of trash grow has made me so much aware of what we’re putting out in the world…“If I think about it, I get too overwhelmed,” she says. “I am not even not making a dent in the plastic that is thrown out. I would just like to change people’s thinking.”Indeed, there are signs of a broad plastics backlash. Several cities, including Los Angeles, have banned styrofoam or polystyrene food containers. In 2010, Italy banned the sale of plastic bottles along their UNESCO World Heritage coastline of Cinque Terre, where up to 400,000 plastic bottles were discarded each month. Similarly, the sale of water bottles has been banned in Zion National Park in Utah. Not long ago, the town of Concord, Massachusetts, banned single-serving water bottles of 16 ounces or less. San Francisco later eliminated the use of plastic bags in stores and restaurants. The city had estimated that it spent 17 cents to handle each discarded bag. “We have strange relationship with plastic. Many people hate it but they aren’t able to eliminate it from their lives,” she says. Ganz began exploring work with a different kind of hard-to-handle refuse: discarded tires and inner tubes. Her home town of Fort Wayne, Indiana, asked Ganz to create something of the refuse collected from the Maumee River every Earth Day. “At first, I was asked to use metal, but there were very few metal pieces. But the tires, oh my god. There was a pile eight feet high, the size of a one-car garage, and they get that much ever year. Just tires.”“I feel very sad for [pieces of] plastic, because they’re the orphans of our consumer society. We create them, and then just cast them aside.”Heather Sparks explores art and science at Science Sparks Art. She studied molecular genetics at The Ohio State University, has a graduate degree in science journalism from New York University, and currently lives in Brooklyn.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: If architecture is seen as art, does it diminish engineering to be seen as a trade?This classic Facts So Romantic post was originally published in November 2013. "
  },
  {
    "imageUrl": "http://static.nautil.us/15082_12cfbd88070f29ee643ba6a9d614f79b.jpg",
    "title": "Announcing a Black Hole Essay Competition from Harvard",
    "description": "Posted by The Black Hole Initiative on August 01, 2018  The Black Hole Initiative (BHI) at Harvard University announces the first-ever Black Hole Essay Competition, inviting submissions that explore novel connections and new perspectives on…",
    "category": "Matter",
    "content": "The Black Hole Initiative (BHI) at Harvard University announces the first-ever Black Hole Essay Competition, inviting submissions that explore novel connections and new perspectives on black hole research. The BHI awards, including a $10,000 First Prize, will be given to authors of highly engaging 1,500-word articles that effectively connect a non-expert audience with the growing field of black hole science. The deadline for submissions has been extended to September 1, 2018. As regions of spacetime with a gravitational fields so intense that not even light can escape, black holes are fascinating to scientists and the general public alike. Capturing the attention of world-renowned researchers, the understanding of black holes is at the nexus of the BHI’s worldwide research effort. By combining expertise in the fields of Astronomy, Mathematics, Philosophy, Physics, and History, the BHI is focusing new attention on black holes in hopes of illuminating their nature. “The Black Hole Initiative offers a unique environment for thinking about the topic of black holes more creatively and comprehensively,” says BHI director, Avi Loeb. “This is the approach we want to encourage from competition authors that boldly explore the topic and make it approachable for a wider audience,” adds Shep Doeleman, who is a senior member of the BHI and director of the Event Horizon Telescope project.Funded by the John Templeton Foundation and drawing on a network of researchers, academics, and philosophers, the BHI was established as an independent Center in 2016 within the Faculty of Arts & Sciences at Harvard. Together with the Smithsonian Astrophysical Observatory, BHI team members include Principal Investigators from the fields of Astronomy (Sheperd Doeleman, Avi Loeb and Ramesh Narayan), Physics (Andrew Strominger), Mathematics (Shing-Tung Yau) and Philosophy (Peter Galison). Now, more than a century after black holes were first mathematically described, the Essay Competition is a BHI effort to expand on the public’s understanding of black holes and generate continuing interest in such scientific research. The $10,000 First Prize will include the opportunity to publish the winning article in Nautilus, a leading online and print magazine that blends science, culture, and philosophy. Reaching new audiences with thoughtful, yet accessible, research is a goal of the BHI. Annual conferences, weekly colloquia, and regular research updates on social media attract both academic and non-academic attention.Essays submitted to the competition should explore the intersection of multiple fields, including Mathematics, Philosophy, Astronomy, Physics, and History. The competition is open to all, and researchers with active projects as well as science writers with cross-disciplinary perspectives are encouraged to apply. The intent is not to replicate traditional academic reporting, but to expand on the multitude of connections that the Black Hole Initiative is focused on through its mission. Figures and equations should be accessible to a non-expert audience. Second, Third, Fourth, and Fifth place winners will be recognized with awards ranging from $2,000-$5,000. Essays will be judged anonymously and the decision of the panel will be final. Further details on the competition, including specifics about cover letters and abstracts, can be found at https://bhi.fas.harvard.edu/news.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why people are fascinated by black holes. "
  },
  {
    "imageUrl": "http://static.nautil.us/15105_b72809a20c3f308b5631168203e1c5ec.jpg",
    "title": "Should We Let English Eat the World?",
    "description": "Posted by Brian  Gallagher on August 03, 2018  Hideo Kojima is the Japanese creator of the 2015 video game, Metal Gear Solid V: The Phantom Pain. He evidently chose “phantom pain” as a subtitle because he thought it captured the…",
    "category": "Culture",
    "content": "Hideo Kojima is the Japanese creator of the 2015 video game, Metal Gear Solid V: The Phantom Pain. He evidently chose “phantom pain” as a subtitle because he thought it captured the experience of being exiled, so to speak, from one’s first language. Kojima hints at its importance from the start of the game, with an epigraph from the Romanian philosopher Emil Cioran’s 1986 book Anathemas and Admirations: “It is no nation we inhabit, but a language. Make no mistake; our native tongue is our true fatherland.” At one point, a villain in the game, Skull Face, says of another character, “He doesn’t know the pain of losing his own language—not yet.”That our native tongue is key to our identity is a plausible claim. Research has revealed the “depth of the relationship all of us have with our native tongues—and how traumatic it can be when that relationship is ruptured,” as psycholinguist Julie Sedivy explained in her Nautilus feature “The Strange Persistence of First Languages.” Her relationship with the Czech language—she was born in Czechoslovakia—ruptured slowly after starting school in East Montreal, in English. “When a childhood language decays, so does the ability to reach far back into your own private history,” Sedivy wrote. “Language is memory’s receptacle. It has Proustian powers. Just as smells are known to trigger vivid memories of past experiences, language is so entangled with our experiences that inhabiting a specific language helps surface submerged events or interactions that are associated with it.”English is not “an infestation.”Skull Face is radicalized by the notion that the English language is taking over the world and aligning human thought with Anglo-American interests. A few dominant languages do, indeed, seem to be taking over. Every two weeks, a language disappears. By 2100, up to 90 percent of languages might be gone, overtaken by the likes of Spanish, Russian, Mandarin—and English. A headline last week from The Guardian read: “Behemoth, bully, thief: how the English language is taking over the planet.” For even a fraction of that 90 percent of languages to survive, The Guardian suggests, “we’re going to have to start thinking of smaller languages not as endangered species worth saving, but as equals worth learning.”But try telling that to children in developing countries. In his 2013 book Does Science Need a Global Language?, the geoscientist Scott Montgomery quotes an Ethiopian boy saying, “It is the language of the world, and I want to know the world.” Montgomery goes on to ask whether “the planetary advance of this tongue [qualifies] as a kind of rising tyranny, threatening to bring Anglo ways of thought and culture to every society, like an invading force?”Montgomery says no. For one thing, English adapts to the needs of people speaking it more than it shapes those people’s ideas or ideals. “At the spoken level,” he wrote, “the language has become decidedly, inevitably plural.” As a result, linguists no longer discuss “world English” but “World Englishes” and “New Englishes” instead. “Distinct, nativized varieties are found in South Asia (India, Pakistan, Sri Lanka, Bangladesh), West Africa (Nigeria, Liberia, Ghana, Gambia), East Africa (Uganda, Kenya, Tanzania), southern Africa (South Africa, Zimbabwe, Namibia), Hong Kong, Singapore, and the Caribbean,” he wrote. They are more than “mere dialects”—all vary from one another in “pronunciation, vocabulary, and, at times, grammar.” In addition, as The Guardian piece pointed out, the “expansive notion that different languages inculcate fundamentally different ways of thinking has not been proven.”Still, languages do influence the way we think to some extent. The psychologist Daniel Casasanto points out that “...grammatical packaging of information about motion events can direct attention to different aspects of the perceptible world, influencing what people remember about their experiences, at least so long as they can encode these experiences in words.” Also, using “different spoken metaphors can strengthen some implicit associations in memory while weakening others.” A particularly strong difference is observed between languages that include exact number concepts, and those that don’t. In a 2016 paper, Casasanto argues that this seems to have a “dramatic and transformative effect on thought.” Yet second languages can crowd out first ones, with consequential results. “Losing your native tongue unmoors you not only from your own early life but from the entire culture that shaped you,” Sedivy wrote. The Guardian piece argues that the Englishization of the world causes this to happen all too often: English is “inescapable” and everywhere “leaves behind a trail of dead: dialects crushed, languages forgotten, literatures mangled.” Memoirist Eva Hoffman attested to the effect of feeling alienated from her native Polish while learning English in Canada: “This radical disjointing between word and thing is a desiccating alchemy, draining the world not only of significance but of its colors, striations, nuances—its very existence. It is the loss of a living connection.”Montgomery is clear-eyed about this fact, but not despairing: “The stresses and challenges posed by global English cannot be denied, belittled, or dismissed. They can, however, be exaggerated, misinterpreted, and misapplied.” English is not “an infestation,” as Skull Face calls it—rather it represents, Montgomery wrote, the “oldest dream for a better world: the dream of a universal language that allows people everywhere to commune and work together.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: When language fails us. "
  },
  {
    "imageUrl": "http://static.nautil.us/15133_a47ad724599e11d59272b02d08d0dbd7.jpg",
    "title": "How Genes Refract Chance",
    "description": "Posted by Brian  Gallagher on August 10, 2018  In February, for my birthday, I was gifted a 23andMe genetic test kit. I enjoyed this coincidence: Here was a technology, contra astrology, that would have some real purchase, however limited,…",
    "category": "Biology",
    "content": "In February, for my birthday, I was gifted a 23andMe genetic test kit. I enjoyed this coincidence: Here was a technology, contra astrology, that would have some real purchase, however limited, on who I am. Holding the small cardboard box containing a tube for my saliva, I considered my sign, Aquarius, which my mother would bring up from time to time to explain or predict my youthful behavior. How remarkable, I thought, that science is fulfilling, in some sense, that ancient aspiration to decipher some measure of our personal nature and fate.My results weren’t too surprising—no health scares or rude ancestry awakenings—though the report on my muscle composition resonated with me: I’ve got a gene variant common among elite power athletes—sprinters as opposed to endurance runners—who tend to have fast-twitch fibers. Perhaps this explains, by some degree, why I played sprint-heavy sports like football and basketball and hated jogging long-distance.The geneticist Siddhartha Mukherjee has a metaphor for this: Our genes are lenses through which chance is refracted. Genetics “allows plenty of space for us to interact with environments and with random acts of fate and chance, and thereby, in a kaleidoscopic way, create different outcomes, different cells, different beings, different individuals,” he told Nautilus editor in chief Michael Segal. Take the natural experiment of twins reared in different environments, Mukherjee said. “Two exactly identical genomes superposed on small vagaries of chance and environment yield two very different individual beings.”This disproves genetic determinism in a strict—but not in a loose—sense. We need no longer speak of destiny as if it were a “gray cloud,” he said. Rather, “we can begin to speak about it—about destiny, about self, about future propensities, however you want to call it—in terms of very incisive information about particular genomes correlating or coexisting with particular environments; and you might find that this end of destiny, this end of the self, has aspects that are strongly genetically determined.”For Mukherjee, the consequences of understanding the outcomes of particular gene-environment interactions are nothing but life-changing. “The appropriate dissection of these, and the appropriate understanding of the autonomy of each of these interactions allows us, I think, to manipulate human beings and understand human beings to profound effect, and with profoundly dangerous consequences.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15140_ee3bf295b9cf5a7ad4297868b069c91f.jpg",
    "title": " This Will Help You Grasp the Sizes of Things in the Universe",
    "description": "Posted by Dan Garisto on August 14, 2018  Caleb Scharf wants to take you on an epic tour. His latest book, The Zoomable Universe, starts from the ends of the observable universe, exploring its biggest structures, like groups of…",
    "category": "Matter",
    "content": "Caleb Scharf wants to take you on an epic tour. His latest book, The Zoomable Universe, starts from the ends of the observable universe, exploring its biggest structures, like groups of galaxies, and goes all the way down to the Planck length—less than a billionth of a billionth of a billionth of a meter. It is a breathtaking synthesis of the large and small. Readers journeying through the book are treated to pictures, diagrams, and illustrations all accompanied by Scharf’s lucid, conversational prose. These visual aids give vital depth and perspective to the phenomena that he points out like a cosmic safari guide. Did you know, he offers, that all the Milky Way’s stars can fit inside the volume of our solar system?Scharf, the director of Columbia University’s Astrobiology Center, is a suitably engaging guide. He’s the author of the 2012 book Gravity’s Engines: How Bubble-Blowing Black Holes Rule Galaxies, Stars, and Life in the Universe, and he speculated in Nautilus about whether alien life could be so advanced as to be indistinguishable from physics.In The Zoomable Universe, Scharf puts the notion of scale—in biology and physics—center-stage. “The start of your journey through this book and through all known scales of reality is at that edge between known and unknown,” he writes. Nautilus caught up with him to talk about our experience with scale and why he thinks it’s mysterious. (Scharf is a member of Nautilus’ advisory board.)Why is scale interesting?Scale is fascinating. Scientifically it’s a fundamental property of reality. We don’t even think about it. We talk about space and time—and perhaps we puzzle more over the nature of time than we do over the nature of scale or space—but it’s equally mysterious. What’s mysterious about scale? It’s something we all have direct experience of, even intuitively. We learn to evaluate the size of things. But we’re operating as humans in a very, very narrow slice of what is out there. And we’re aware of a very narrow range of scales: In some sense, we know more about the very large than we do about the very small.We know about atoms, kind of, but if you go smaller, it gets more uncertain—not just because of intrinsic uncertainty, but the completeness of our physics gets worse. We don’t really know what’s happening here. That leads you to a mystery at the Planck scale. On the big scale, it’s stuff we can actually see, we can actually chart.At certain scales, there’s not much happening. Does that hint at some underlying mystery?I think that is something worth contemplating. There’s quarks and then there’s 20 orders of magnitude smaller where—what do you say about it? That was the experience for the very small, but on the larger scale there’s some of that too…the emptiness of interstellar space. It is striking how empty most of everything is on the big scale and the small scale. We have all this rich stuff going on in the scale of the solar system and the earth and our biological scale. That’s where we’ve gained the most insight, accumulated the most knowledge. It is the scale where matter seems to condense down, where things appear solid, when in fact, it’s equally empty on the inside. But is that a human cultural bias? Or is that telling us something profound about the nature of the universe? I don’t really know the answer to that. But there’s something about the way we’re built, the way we think about the world. We’re clearly not attuned to that emptiness. Yet we’re drawn to it.We are drawn to it—like the example in the book with the stars packed together. Taking all the stars from the galaxy put together and being able to fit them inside the volume of the solar system? It is shocking. Trust me, I had to run the numbers a couple of times just to go, “Oh wow, okay, that really does work.”How did you represent things that we don’t have pictures of, like the surface of an exoplanet, or things at really small scales?That’s something we definitely talked a lot about in putting the book together. Ron Miller, the artist, would produce a landscape for an exoplanet. As a scientist, my inclination is to say, “We can’t do that—we can’t say what it looks like.” So we had this dialogue. We wanted an informed artistic approach. It became tricky when we got down to a small scale. I wanted to avoid the usual trope, which is an atom is a sphere, or a molecule is a sphere connected by things. You can’t have a picture of these things in the sense that we’re used to. We tried to compromise. We made something people kind of recognize, but we avoid the ball and stick models that are glued in everyone’s head. Dan Garisto is a science writer.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Do we implicitly think that the rest of the universe should look like our own part of it?This classic Facts So Romantic post was originally published in November 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15062_d5215023c7e116864efd4f4965fe6b91.jpg",
    "title": "How Artificial Intelligence Can Supercharge the Search for New Particles",
    "description": "Posted by Charlie Wood on July 25, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.The Large Hadron Collider (LHC) smashes a billion pairs of protons together each second. Occasionally the machine…",
    "category": "Numbers",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.The Large Hadron Collider (LHC) smashes a billion pairs of protons together each second. Occasionally the machine may rattle reality enough to have a few of those collisions generate something that’s never been seen before. But because these events are by their nature a surprise, physicists don’t know exactly what to look for. They worry that in the process of winnowing their data from those billions of collisions to a more manageable number, they may be inadvertently deleting evidence for new physics. “We’re always afraid we’re throwing the baby away with the bathwater,” said Kyle Cranmer, a particle physicist at New York University who works with the ATLAS experiment at CERN.Faced with the challenge of intelligent data reduction, some physicists are trying to use a machine learning technique called a “deep neural network” to dredge the sea of familiar events for new physics phenomena.In the prototypical use case, a deep neural network learns to tell cats from dogs by studying a stack of photos labeled “cat” and a stack labeled “dog.” But that approach won’t work when hunting for new particles, since physicists can’t feed the machine pictures of something they’ve never seen. So they turn to “weakly supervised learning,” where machines start with known particles and then look for rare events using less granular information, such as how often they might take place overall.In a paper posted on the scientific preprint site arxiv.org in May, three researchers proposed applying a related strategy to extend “bump hunting,” the classic particle-hunting technique that found the Higgs boson. The general idea, according to one of the authors, Ben Nachman, a researcher at the Lawrence Berkeley National Laboratory, is to train the machine to seek out rare variations in a data set.Consider, as a toy example in the spirit of cats and dogs, a problem of trying to discover a new species of animal in a data set filled with observations of forests across North America. Assuming that any new animals might tend to cluster in certain geographical areas (a notion that corresponds with a new particle that clusters around a certain mass), the algorithm should be able to pick them out by systematically comparing neighboring regions. If British Columbia happens to contain 113 caribous to Washington state’s 19 (even against a background of millions of squirrels), the program will learn to sort caribous from squirrels, all without ever studying caribous directly. “It’s not magic but it feels like magic,” said Tim Cohen, a theoretical particle physicist at the University of Oregon who also studies weak supervision.By contrast, traditional searches in particle physics usually require researchers to make an assumption about what the new phenomena will look like. They create a model of how the new particles will behave—for example, a new particle might tend to decay into particular constellations of known particles. Only after they define what they’re looking for can they  engineer a custom search strategy. It’s a task that generally takes a Ph.D. student at least a year, and one that Nachman thinks could be done much faster, and more thoroughly.The proposed CWoLa algorithm, which stands for Classification Without Labels, can search existing data for any unknown particle that decays into either two lighter unknown particles of the same type, or two known particles of the same or different type. Using ordinary search methods, it would take the LHC collaborations at least 20 years to scour the possibilities for the latter, and no searches currently exist for the former. Nachman, who works on the ATLAS project, says CWoLa could do them all in one go.Other experimental particle physicists agree it could be a worthwhile project. “We’ve looked in a lot of the predictable pockets, so starting to fill in the corners we haven’t looked in is an important direction for us to go in next,” said Kate Pachal, a physicist who searches for new particle bumps with the ATLAS project. She batted around the idea of trying to design flexible software that could deal with a range of particle masses last year with some colleagues, but no one knew enough about machine learning. “Now I think it might be the time to try this,” she said.The hope is that neural networks could pick up on subtle correlations in the data that resist current modeling efforts. Other machine learning techniques have successfully boosted the efficiency of certain tasks at the LHC, such as identifying “jets” made by bottom-quark particles. The work has left no doubt that some signals are escaping physicists’ notice. “They’re leaving information on the table, and when you spend $10 billion on a machine, you don’t want to leave information on the table,” said Daniel Whiteson, a particle physicist at the University of California, Irvine.Yet machine learning is rife with cautionary tales of programs that confused arms with dumbbells (or worse). At the LHC, some worry that the shortcuts will end up reflecting gremlins in the machine itself, which experimental physicists take great pains to intentionally overlook. “Once you find an anomaly, is it new physics or is it something funny that went on with the detector?” asked Till Eifert, a physicist on ATLAS.Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15076_c1cca9b367623e090aa3ea860644ecd4.jpg",
    "title": " What If Only Females Could See Color?",
    "description": "Posted by Pierre Bienaimé on July 27, 2018  Have you ever wondered how your life might be different if you could see beyond the visible light spectrum—into ultraviolet or infrared? For one thing, you might be immune, or less susceptible,…",
    "category": "Biology",
    "content": "Have you ever wondered how your life might be different if you could see beyond the visible light spectrum—into ultraviolet or infrared? For one thing, you might be immune, or less susceptible, to implicit racial bias. Inna Vishik, an applied physicist at U.C. Davis, says if you weren’t limited to the typical range of colors most humans see, “everyone would be the same color (except for people with a fever)”—yellowish. You’d also be able to know which places have great wifi and cell phone reception, she says, and whether you “*really* should wear sunscreen today.” No doubt if you had this special ability, it would benefit not just yourself, but your family and friends, too. Something like this scenario has actually been discovered in nature, albeit not with humans. Within a certain population of tree-dwelling primates in Madagascar—Verreaux’s sifaka, a kind of lemur, to be precise—a recent study found, nearly one in four females has trichromatic color vision (like humans). Unlike most other members of their sex—and all the males—these females can tell red and green apart, perceiving color much as we humans do. And the perks of this genetic gift may extend to the entire lemur group, says Carrie Veilleux, a biological and molecular anthropologist at the University of Texas at Austin, and the study’s lead author. “I wanted to look at other kinds of power—more leverage, economic kinds of power.”“If you’re a dichromat, you can’t tell ripe red fruit against a green background. It looks just like the green background,” Veilleux says. “The female that’s trichromat would be able to detect that popping out and go toward it.” In their paper, Veilleux and her co-authors argue that the existence of these lemurs support the “benefit of mutual association” hypothesis: These special females can spot juicy patches of fruit in the boughs of their leafy habitats their group might otherwise miss, and swing over to them; and when they do, the rest of the group tags (and snacks) alongside them.Trichromatic color vision is typically a trait species either have or lack. Today’s humans, apes, and (some) monkeys, for instance, all have three types of cone cells in the retina. They’re sensitive to different parts of the visible light spectrum. The female sifaka, though, show how some animals can chance on trichromacy even if most of the other members are dichromats, meaning they have only two types of cone cells. “In New World monkeys, and in some lemurs, there’s only one gene that codes for the two different cones,” Veilleux says. If a female sifaka inherits a different form of the gene on each of its X chromosomes—as in humans, only females have two X chromosomes—its eyes are endowed with all three cones, in a phenomenon called “polymorphic trichromacy.” For a female sifaka to be trichromatic, in other words, she needs to inherit each of these versions of the gene, as opposed to, redundantly, two of the same. “It’s crazy to think that one single base-pair difference is what can make something be able to tell red and green apart,” Veilleux says. That’s why the male sifaka is doomed to what we would call color blindness; with just one X chromosome, it has no chance of scoring both complementary versions (which code for “medium” and “long” wavelength perception) of the critical gene. As for the third cone (“short”), it’s generated autosomally—that is, in a chromosome that all lemurs inherit, regardless of sex. The study’s authors analyzed nine years’ worth of data on dozens of lemurs for “three potential fitness proxies,” to gauge how much the female trichomats benefited their fellow dichromats: “body mass index during the dry season, reproductive output, and infant survival.” The gains were clear. Lemurs who were either trichromatic, or living alongside trichromats, ended up shedding less body mass than those in dichromatic groups during the May-to-November dry season, a time “when they’re so energetically stressed” from rearing their young with less food available, says Veilleux. Pre-existing observation data also showed that one group of trichromat females and their hanger-ons—the only trichromat group to be tracked in the data set—spent more time feeding on fruit compared to three groups of dichromat lemurs.The researchers looked at the lemurs’ genetic material to figure out which of them had richer color vision. Enafa Jaonarisoa, a Malagasi specialist, sedated the lemurs from a distance, using a specially treated dart expelled from a blow pipe. (He “can hit a matchbox at 30 feet,” says an anthropologist who has worked with Jaonarisoa, though not on this study). After a minute of wooziness, the lemur drops perhaps a dozen feet from a tree into a sheet held open by a few attendants. Members of the research team take two small biopsy punches from the critter’s ear, conduct measurements and a health check, and “then we release ‘em, and they’re back in maybe three hours, back in their group and doing fine,” says Rebecca Lewis, one of the study’s co-authors who runs the Ankoatsifaka Research Station within the national park these lemurs call home. Lewis thought the team’s findings lined up pretty well with what they knew about lemurs’ social hierarchy: Among Verreaux’s sifaka, it’s the females who call the shots. They initiate foraging expeditions more often, draw more followers, and take them farther afield than males do. That power discrepancy in color vision is part of what attracted her to participate. “In animals we think of who’s biggest and baddest in terms of power,” Lewis says. “And I wanted to look at other kinds of power—more leverage, economic kinds of power.”The researchers were quick to explain that their sample size was relatively small, and that they had to be “careful” with their conclusions, according to Lewis. But Brenda Bradley, who directs the Primate Genomics Lab at George Washington University, calls the study exciting. She sees color vision in primates as a textbook example of evolution in progress. “There aren’t that many studies where people actually can look at that in the wild,” Bradley says. “It’s great that they find at least some suggestive evidence that maybe trichromatic color vision is providing some kind of advantage.”Coincidentally, Bradley had sequenced the genomes of nearly 100 Verreaux’s sifaka in a different Malagasy forest in 2005, as a postdoc. She and her colleagues were specifically looking for color vision, without any luck. “We went out there, collected all these samples, and genotyped them and then we realized, ‘Oh, there is no variation. There is no polymorphism. They’re all red-green colorblind,’” Bradley says. She’s since discovered six more examples of polymorphic trichromacy among other lemur species with her colleagues; the study was published in Biology Letters. “As primatologists, we have this tendency to characterize what seems biologically true for a species at one site. And you think, ‘Oh well, this is what is true for this species.’ And then to find out that no, it’s completely different when you go a couple hundred miles north into a different type of forest…I think that’s really exciting.”Pierre Bienaimé is a writer and musician based in New York. He also works on a podcast at Slate. Follow him on Twitter @ScribblerSounds.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: A philosopher on what perceiving color is.Philosopher at the University of Pittsburgh” data-credits=”” style=“width:733px”>This classic Facts So Romantic post was originally published in February 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/15041_549d841c3704e2b6a273a258dd0b6f17.png",
    "title": "Why We Should Think Twice About Colonizing Space",
    "description": "Posted by Phil Torres on July 23, 2018  There are lots of reasons why colonizing space seems compelling. The popular astronomer Neil deGrasse Tyson argues that it would stimulate the economy and inspire the next generation of…",
    "category": "Ideas",
    "content": "There are lots of reasons why colonizing space seems compelling. The popular astronomer Neil deGrasse Tyson argues that it would stimulate the economy and inspire the next generation of scientists. Elon Musk, who founded SpaceX, argues that “there is a strong humanitarian argument for making life multiplanetary…to safeguard the existence of humanity in the event that something catastrophic were to happen.”  The former administrator of NASA, Michael Griffin, frames it as a matter of the “survival of the species.” And the late astrophysicist Stephen Hawking has conjectured that if humanity fails to colonize space within 100 years, we could face extinction.To be sure, humanity will eventually need to escape Earth to survive, since the sun will make the planet uninhabitable in about 1 billion years. But for many “space expansionists,” escaping Earth is about much more than dodging the bullet of extinction: it’s about realizing astronomical amounts of value by exploiting the universe’s vast resources to create something resembling utopia. For example, the astrobiologist Milan Cirkovic calculates that some 1046 people per century could come into existence if we were to colonize our Local Supercluster, Virgo. This leads Nick Bostrom to argue that failing to colonize space would be tragic because it would mean that these potential “worthwhile lives” would never exist, and this would be morally bad.But would these trillions of lives actually be worthwhile? Or would colonization of space lead to a dystopia?In a recent article in Futures, which was inspired by political scientist Daniel Deudney’s forthcoming book Dark Skies, I decided to take a closer look at this question. My conclusion is that in a colonized universe the probability of the annihilation of the human race could actually rise rather than fall.The argument is based on ideas from evolutionary biology and international relations theory, and it assumes that there aren’t any other technologically advanced lifeforms capable of colonizing the universe (as a recent study suggests is the case).Consider what is likely to happen as humanity hops from Earth to Mars, and from Mars to relatively nearby, potentially habitable exoplanets like Epsilon Eridani b, Gliese 674 b, and Gliese 581 d. Each of these planets has its own unique environments that will drive Darwinian evolution, resulting in the emergence of novel species over time, just as species that migrate to a new island will evolve different traits than their parent species. The same applies to the artificial environments of spacecraft like “O’Neill Cylinders,” which are large cylindrical structures that rotate to produce artificial gravity. Insofar as future beings satisfy the basic conditions of evolution by natural selection—such as differential reproduction, heritability, and variation of traits across the population—then evolutionary pressures will yield new forms of life.But the process of “cyborgization”—that is, of using technology to modify and enhance our bodies and brains—is much more likely to influence the evolutionary trajectories of future populations living on exoplanets or in spacecraft. The result could be beings with completely novel cognitive architectures (or mental abilities), emotional repertoires, physical capabilities, lifespans, and so on.In other words, natural selection and cyborgization as humanity spreads throughout the cosmos will result in species diversification. At the same time, expanding across space will also result in ideological diversification. Space-hopping populations will create their own cultures, languages, governments, political institutions, religions, technologies, rituals, norms, worldviews, and so on. As a result, different species will find it increasingly difficult over time to understand each other’s motivations, intentions, behaviors, decisions, and so on. It could even make communication between species with alien languages almost impossible. Furthermore, some species might begin to wonder whether the proverbial “Other” is conscious. This matters because if a species Y cannot consciously experience pain, then another species X might not feel morally obligated to care about Y. After all, we don’t worry about kicking stones down the street because we don’t believe that rocks can feel pain. Thus, as I write in the paper, phylogenetic and ideological diversification will engender a situation in which many species will be “not merely aliens to each other but, more significantly, alienated from each other.”But this yields some problems. First, extreme differences like those just listed will undercut trust between species. If you don’t trust that your neighbor isn’t going to steal from, harm, or kill you, then you’re going to be suspicious of your neighbor. And if you’re suspicious of your neighbor, you might want an effective defense strategy to stop an attack—just in case one were to happen. But your neighbor might reason the same way: she’s not entirely sure that you won’t kill her, so she establishes a defense as well. The problem is that, since you don’t fully trust her, you wonder whether her defense is actually part of an attack plan. So you start carrying a knife around with you, which she interprets as a threat to her, thus leading her to buy a gun, and so on. Within the field of international relations, this is called the “security dilemma,” and it results in a spiral of militarization that can significantly increase the probability of conflict, even in cases where all actors have genuinely peaceful intentions.So, how can actors extricate themselves from the security dilemma if they can’t fully trust each other? On the level of individuals, one solution has involved what Thomas Hobbes’ calls the “Leviathan.” The key idea is that people get together and say, “Look, since we can’t fully trust each other, let’s establish an independent governing system—a referee of sorts—that has a monopoly on the legitimate use of force. By replacing anarchy with hierarchy, we can also replace the constant threat of harm with law and order.” Hobbes didn’t believe that this happened historically, only that this predicament is what justifies the existence of the state. According to Steven Pinker, the Leviathan is a major reason that violence has declined in recent centuries.The point is that if individuals—you and I—can overcome the constant threat of harm posed by our neighbors by establishing a governing system, then maybe future species could get together and create some sort of cosmic governing system that could similarly guarantee peace by replacing anarchy with hierarchy. Unfortunately, this looks unpromising within the “cosmopolitical” realm. One reason is that for states to maintain law and order among their citizens, their various appendages—e.g., law enforcement, courts—need to be properly coordinated. If you call the police about a robbery and they don’t show up for three weeks, then what’s the point of living in that society? You’d be just as well off on your own! The question is, then, whether the appendages of a cosmic governing system could be sufficiently well-coordinated to respond to conflicts and make top-down decisions about how to respond to particular situations. To put it differently: If conflict were to break out in some region of the universe, could the relevant governing authorities respond soon enough for it to matter, for it to make a difference?Probably not, because of the immense vastness of space. For example, consider again Epsilon Eridani b, Gliese 674 b, and Gliese 581 d. These are, respectively, 10.5, 14.8, and 20.4 light-years from Earth. This means that a signal sent as of this writing, in 2018, wouldn’t reach Gliese 581 d until 2038. A spaceship traveling at one-quarter the cosmic speed limit wouldn’t arrive until 2098, and a message to simply affirm that it had arrived safely wouldn’t return to Earth until 2118. And Gliese 581 is relatively close as far as exoplanets go. Just consider that he Andromeda Galaxy is some 2.5 million light-years from Earth and the Triangulum Galaxy about 3 million light-years away. What’s more, there are some 54 galaxies in our Local Group, which is about 10 million light-years wide, within a universe that stretches some 93 billion light-years across.These facts make it look hopeless for a governing system to effectively coordinate law enforcement activities, judicial decisions, and so on, across cosmic distances. The universe is simply too big for a government to establish law and order in a top-down fashion.But there is another strategy for achieving peace: Future civilizations could use a policy of deterrence to prevent other civilizations from launching first strikes. A policy of this sort, which must be credible to work, says: “I won’t attack you first, but if you attack me first, I have the capabilities to destroy you in retaliation.” This was the predicament of the US and Soviet Union during the Cold War, known as “mutually-assured destruction” (MAD).But could this work in the cosmopolitical realm of space? It seems unlikely. First, consider how many future species there could be: upwards of many billions. While some of these species would be too far away to pose a threat to each other—although see the qualification below—there will nonetheless exist a huge number within one’s galactic backyard. The point is that the sheer number would make it incredibly hard to determine who initiated a first strike, if one is attacked. And without a method for identifying instigators with high reliability, one’s policy of deterrence won’t be credible. And if one’s policy of deterrence isn’t credible, then one has no such policy!Second, ponder the sorts of weapons that could become available to future spacefaring civilizations. Redirected asteroids (a.k.a., “planetoid bombs”), “rods from God,” sun guns, laser weapons, and no doubt an array of exceptionally powerful super-weapons that we can’t currently imagine. It has even been speculated that the universe might exist in a “metastable” state and that a high-powered particle accelerator could tip the universe into a more stable state. This would create a bubble of total annihilation that spreads in all directions at the speed of light—which opens up the possibility that a suicidal cult, or whatever, weaponizes a particle accelerator to destroy the universe.The question, then, is whether defensive technologies could effectively neutralize such risks. There’s a lot to say here, but for the present purposes just note that, historically speaking, defensive measures have very often lagged behind offensive measures, thus resulting in periods of heightened vulnerability. This is an important point because when it comes to existentially dangerous super-weapons, one only needs to be vulnerable for a short period to risk annihilation.So far as I can tell, this seriously undercuts the credibility of policies of deterrence. Again, if species A cannot convince species B that if B strikes it, A will launch an effective and devastating counter strike, then B may take a chance at attacking A. In fact, B does not need to be malicious to do this: it only needs to worry that A might, at some point in the near- or long-term future, attack B, thus making it rational for B to launch a preemptive strike (to eliminate the potential danger). Thinking about this predicament in the radically multi-polar conditions of space, it seems fairly obvious that conflict will be extremely difficult to avoid.The lesson of this argument is not to uncritically assume that venturing into the heavens will necessarily make us safer or more existentially secure. This is a point that organizations hoping to colonize Mars, such as SpaceX, NASA, and Mars One should seriously contemplate. How can humanity migrate to another planet without bringing our problems with us? And how can different species that spread throughout the cosmos maintain peace when sufficient mutual trust is unattainable and advanced weaponry could destroy entire civilizations? Human beings have made many catastrophically bad decisions in the past. Some of these outcomes could have been avoided if only the decision-makers had deliberated a bit more about what could go wrong—i.e., had done a “premortem” analysis. We are in that privileged position right now with respect to space colonization. Let’s not dive head-first into waters that turn out to be shallow.Phil Torres is the director of the Project for Human Flourishing and the author of Morality, Foresight, and Human Flourishing: An Introduction to Existential Risks.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Should we pessimistic about the deep future? "
  },
  {
    "imageUrl": "http://static.nautil.us/15110_777125b977ddc0317d0533782d3c27b5.jpg",
    "title": "Swarming Bacteria Create an “Impossible” Superfluid",
    "description": "Posted by Charlie Wood on August 06, 2018  Reprinted with permission from Quanta Magazine‘s Abstractions blog.Outside of the imaginations of physics teachers, frictionless devices are hard to come by. But putting a bunch of…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine‘s Abstractions blog.Outside of the imaginations of physics teachers, frictionless devices are hard to come by. But putting a bunch of swimming bacteria into a drop of water achieves just that: a fluid with zero resistance to motion. Incredibly, that resistance (or viscosity, as it’s properly known) can even go negative, creating a self-propelling liquid that might, say, turn a motor in a way that seems to defy the laws of thermodynamics. Recent work explains how bacteria conspire to pull off the improbable.“For a normal fluid it’s impossible because the whole thing would be unstable,” said Aurore Loisy, a physicist at the University of Bristol in the United Kingdom and a coauthor of one of the new studies, “but for bacteria somehow it works.”Physicists have long dreamt of getting something for nothing, even if only in outlandish thought experiments. In the 1860s James Maxwell conjured up an all-knowing demon who could shunt fast air molecules to one side of a room and slow molecules to the other, creating a temperature difference that could power an engine. With marginally more practicality, in 1962 Richard Feynman lectured about a microscopic gear that, when jostled by air molecules, would turn in only one direction, driving a motor. But such ideas are dashed by the Second Law of Thermodynamics, which insists that the sorting or the turning must generate heat that dooms both schemes. As the poet Allen Ginsberg put it, you can’t win, and you can’t break even.Recently, evidence has been mounting that while a free lunch is off the table, a cheap snack might be feasible with a system built around a living fluid. Experimental oddities began to surface in 2015 when a French team confirmed that solutions of E. coli and water could get unnaturally slick. Sandwiching a drop between two small plates, they recorded the force needed to make one plate slide at a certain speed. Liquids usually get harder to stir, or more viscous, when they contain additional suspended particles (think water vs. mud), but the opposite turns out to be true when the particles can swim. When the solution was around half a percent E. coli by volume, keeping the plate moving required no force at all, indicating zero viscosity. Some trials even registered negative viscosity, when the researchers had to apply a bit of force against the plates’ motion to keep them from speeding up. The liquid was doing work, which for any inert fluid would have meant a violation of the Second Law.The straightforward conclusion was that the organisms were swimming in a way that neutralized the solution’s internal friction to produce something like a superfluid, a liquid with zero resistance. The apparent thermodynamics violation was an illusion because the bacteria were doing the work to offset or overcome the viscosity.“Each individual bacterium is extremely weak, but there’s strength in numbers,” said Jörn Dunkel, a mathematician at the Massachusetts Institute of Technology who was not involved in the experiment.If you had enough bacteria in the right setup, you could actually get them to move structures around.But E. coli don’t typically all swim in the same direction, so subsequent research has tried to figure out what might be coordinating their movements. One answer, according to research published in July in the Proceedings of the National Academy of Sciences, is the interactions between individuals.“When you have high density, they start to swarm,” said Xiang Cheng, a physicist at the University of Minnesota and coauthor of the paper. But unlike the swarming seen in schools of fish and flocks of birds, the swarming of E. coli is driven purely by their physical characteristics, not an animated response.The researchers’ setup resembled the French team’s, but an attached microscope allowed them to track the bacteria’s behavior. Sure enough, when the E. coli cocktail reached 10 to 20 percent bacteria by volume, swirls formed. As bacteria plowed through the water, which feels honey-thick at their microscopic scale, they produced shockwaves that buffeted their companions both near and far.“It’s a bit like if you have a lot of stars in a galaxy and they can affect each other,” Dunkel said. Those forces encouraged local groups of swimming E. coli to align their pill-shaped bodies.Then the motion of the plates makes that local behavior global. Dragging the top plate sends shearing forces rippling through the fluid, which in effect organize and orient the swarms.“Without shear, the direction of swarming is random,” Cheng said. “Under shear, you get the tendency to have all the bacteria lining up in certain directions.”Once the influence of the plates helps the bacteria settle into an average alignment, their swimming pushes on the water and generates local flows that transform the solution’s large-scale properties.Cheng’s experimental results are largely consistent with a new theoretical model, published just a week earlier in Physical Review Letters. Aiming to develop a mathematical framework to describe the 2015 experiment, the researchers modified equations used for liquid crystals with new terms accounting for the bacteria’s activity.For a normal fluid it’s impossible because the whole thing would be unstable, but for bacteria somehow it works.Their theory reproduced the low and negative viscosities seen in experiments and also predicted that the bacteria could collectively orient themselves in multiple stable patterns under the pressure of the plates. “You find that you actually have two possible states, two possible equilibrium solutions,” Loisy said.Dunkel likened the effect to holding a piece of paper along its top and bottom edges and bringing your hands together: As the paper bends, it folds into either a C or an S shape. It is then unlikely to change from one of those two configurations until released. Cheng’s work also suggests two large-scale orientations, but he expects that both are present simultaneously in different groups of bacteria, and the observed behavior represents an average.Details about how these effects contribute to the collective superfluidic behavior remain to be worked out, but no one disputes that the transfer of energy from the microscopic to the visible is real, and peculiar. “Usually you cannot do this. You cannot power a motor with a fluid,” Loisy said.But with bacterial energy, apparently, you can.“If you had enough bacteria in the right setup, you could actually get them to move structures around,” said Dunkel, which raises the tantalizing possibility of harnessing the plates’ motion to turn a turbine.In addition to driving a very small motor at bacteria speed, other potential applications include “smart liquids” that could infiltrate underground channels to force out oil or pollutants, according to Harold Auradou, a physicist at the University of Paris-Sud and coauthor of the 2015 paper.Of course, by all accounts, the laws of thermodynamics remain in full effect.“You’re not doing anything magic here,” Loisy said.Two factors let the bacteria solutions succeed where demons and microgears don’t. First, the E. coli act as little engines themselves, metabolizing energy from sugar and oxygen in the water. To keep them moving, researchers take great care to get the balance of nutrients just right. Too little, and they starve. Too much, and they get lazy. “They’re like humans,” Cheng said with a laugh.But all the energy in the world won’t help if it’s too smoothly distributed, or too disorganized. A system needs asymmetry to coax energy from one location to another. Heat engines require a hot fluid and a cold fluid, for instance, and hydropower turbines need water flowing from a high place to a low place. For bacteria, it comes down to their elongated shape, which responds to the forces in the water.“Just the fact that they align, that there is a preferred direction, breaks the symmetry,” Loisy said. “If they were spherical it wouldn’t work.”Charlie Wood is a journalist covering developments in the physical sciences both on and off the planet. His work has appeared in Scientific American, The Christian Science Monitor and LiveScience, among other publications. Previously, he taught physics and English in Mozambique and Japan, and he has a bachelor’s in physics from Brown University.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14984_d8a7031d63e3762cfac9ac452816f6fe.jpg",
    "title": "What Is the Sun Made Of and When Will It Die?",
    "description": "Posted by Natalie Wolchover on July 09, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.Like any star in its prime, the sun consists mainly of hydrogen atoms fusing two by two into helium, unleashing immense…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Like any star in its prime, the sun consists mainly of hydrogen atoms fusing two by two into helium, unleashing immense energy in the process. But it’s the sun’s tiny concentration of heavier elements, which astronomers call metals, that controls its fate. “Even a very small fraction of metals is sufficient to alter the behavior of a star completely,” explained Sunny Vagnozzi, a physicist at Stockholm University in Sweden who studies the “metallicity” of the sun. The more metallic a star, the more opaque it is (since metals absorb radiation), and how opaque it is in turn relates to its size, temperature, brightness, life span, and other key properties. “Metallicity basically also tells you how the star will die,” Vagnozzi said.But the sun’s metallicity, beyond revealing its own story, also serves as a kind of yardstick for calibrating measurements of the metallicity of all other stars, and thus the ages, temperatures, and other properties of stars, galaxies, and everything else. “If we change the solar yardstick, automatically it means that our understanding of the cosmos has to change,” said Martin Asplund, an astrophysicist at Australian National University. “So having an accurate knowledge of the solar chemical composition is extremely important.”Twenty years ago, astronomers thought they had the sun sorted.Yet, ever more precise measurements of the sun’s metallicity have raised more questions than they’ve answered. Astronomers’ inability to solve the mystery known variously as the solar metallicity, solar abundance, solar composition, or solar modeling problem suggests there could be “something fundamentally wrong” with their understanding of the sun, and therefore of all stars, said Vagnozzi. “That would be huge.”Twenty years ago, astronomers thought they had the sun sorted. Direct and indirect ways of inferring its metallicity both gauged the sun as approximately 1.8 percent metal—a happy convergence that led them to believe they understood not only the length of their solar yardstick but also how the sun works. However, throughout the 2000s, increasingly precise spectroscopic measurements of sunlight—a direct probe of the sun’s composition, since each element creates telltale absorption lines in the spectrum—indicated a far lower metallicity of just 1.3 percent. Meanwhile, helioseismology, the competing, indirect approach for inferring metallicity based on the way sound waves of different frequencies propagate through the sun’s interior, still said 1.8 percent.But if astronomers’ theory of the sun, called the “standard solar model,” is correct, spectroscopy and helioseismology should agree. That is, astronomers should be able to use the helioseismological measurements to calculate the depth of an important boundary layer in the sun where radiation gives way to convection. And this depth relates, according to the equations, to the sun’s opacity, and therefore to its metallicity. This sequence of calculations should predict the same value for the metallicity as spectroscopers measure directly from sunlight. It does not.“This is a problem not only for solar physics, but by extension for astronomy as a whole,” said Asplund, who led the team behind the precise spectroscopic measurements. “Either astronomers do not understand how to measure elemental abundances of stars using spectroscopy, or our understanding of stars’ interiors and how they oscillate is incomplete,” he said. “Either way, it has major ramifications, since stars are the fundamental probes of the cosmos, with stellar astrophysics providing much of the foundation for modern astronomy and cosmology.”After years of talking about what might be going wrong—including speculations about dark matter in the sun—the debate has reached “a bit of a stalemate,” said Sarbani Basu, a solar astrophysicist at Yale University. But there’s hope. Recently, a weak hint about the solar metallicity has come from fleeting particles emanating from the sun called solar neutrinos. Different nuclear fusion reactions produce solar neutrinos of different energies, and so the particles carry information about the sun’s composition. At a conference last month in Heidelberg, Germany, the Borexino experiment based at Italy’s Gran Sasso National Laboratory reported detections of solar neutrinos that marginally favor the higher, 1.8 percent estimate of the sun’s metallicity.If this high-metallicity estimate is indeed correct, this raises questions about what, exactly, went wrong with Asplund and collaborators’ spectroscopic measurements. “If the problem is with spectroscopy, then we likely make similar errors when analyzing other stars,” he said, which would impact interpretations of the chemical evolution of stars and galaxies like the Milky Way.But Asplund stands by his 1.3 percent spectroscopic estimate. He points to a 2015 study in Nature indicating that metals might increase opacity even more than previously thought in the high-pressure conditions of the sun’s core. Correcting for this difference in the standard solar model could bring the helioseismological and neutrino estimates of metallicity down to 1.3 percent, he said.In the coming years, the Borexino team expects to detect rare solar neutrinos produced in the CNO cycle, a fusion reaction in the sun in which carbon, nitrogen, and oxygen atoms serve as catalysts for fusing hydrogen into helium. “The CNO neutrinos are greatly affected by metallicity, so measuring these neutrinos could be definitive,” said Andrea Pocar, a physicist at the University of Massachusetts Amherst and a member of the Borexino collaboration.If it turns out that the sun is, in fact, only 1.3 percent metal, this would mean the standard solar model really does have opacity wrong. “This impacts essentially all of astronomy,” Asplund said, “since an accurate understanding of stellar evolution underpins almost everything we do.” Estimated ages of stars and galaxies would have to be revised by as much as 10 to 15 percent. Unfortunately for the sun itself (and future life on Earth), low-metallicity stars burn fuel faster than high-metallicity stars, so our sun would die about a billion years sooner than we thought.Natalie Wolchover is a senior writer at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScienceand other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award and the 2016 Evert Clark/Seth Payne Award for young science journalists. @NattyOver\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15052_c932714a29aa065c0f86b43e9abf6d40.jpg",
    "title": "Bo Burnham and the Illusion of Meritocracy",
    "description": "Posted by Brian  Gallagher on July 20, 2018  In a WTF with Marc Maron podcast episode from 2012, musical comedian Bo Burnham said his fortune felt unreal, as if his life were a futuristic VR game. “I could die, take off a helmet,…",
    "category": "Culture",
    "content": "In a WTF with Marc Maron podcast episode from 2012, musical comedian Bo Burnham said his fortune felt unreal, as if his life were a futuristic VR game. “I could die, take off a helmet, and, look: It’s the Bo Burnham 2000. There’s a whole line of people crying waiting in line,” he told Maron. “I really do think: What’s more likely? That I’m this lucky, or that I’m living in the future.” He’s only gotten luckier. Burnham put out two Netflix specials, what. in 2013 and Make Happy in 2016 (which Sam Harris, host of the Waking Up podcast, said was “wonderful…the final song is one of the best things I have ever seen.”) Last weekend, at 27 years of age, he made his screenwriting and directorial debut with the film Eighth Grade, an anti social media comedic-drama that has 99 percent on Rotten Tomatoes. In the run-up to its release, The New Yorker profiled Burnham: “The former YouTube star turns on the medium that made him famous.” (His fame began in 2006 when a song of his went viral.)Burnham’s remark on Maron’s podcast recurred to me last week after reading a 2018 paper on talent, luck, and “the role of randomness in success and failure” by a trio of physicists and economists. Burnham confronts his own luck and the feeling of having unearned respect in his work directly, as if he’s intuited what the authors—A. Pluchino, A. E. Biondo, and A. Rapisarda—found. “We show that, if it is true that some degree of talent is necessary to be successful in life, almost never do the most talented people reach the highest peaks of success, being overtaken by mediocre but sensibly luckier individuals.” This isn’t to say Burnham is mediocre. For example, in a song from his 2009 musical comedy album Words Words Words that entranced Ray Romano, Burnham says:I must be psychoticI must be dementedTo think that I’m worthy of all this attentionOf all of this money you worked really hard forI slept in late while you worked at the drugstoreMy drug’s attentionI am an addictBut I get paid to indulge in my habitIt’s all an illusionI’m wearing makeup, makeup, makeupAnd more recently in a moment of candor before Make Happy’s final song, Burnham diagnoses the cause of his fame. Millennials were raised to believe in the value of self-expression, but “I think we found out no one gives a shit what we think,” Burnham says. “So we flock to performers by the thousands because we’re the few that have found an audience, and then I’m supposed to get up here and say ‘Follow your dreams,’ as if this is a meritocracy? It is not. I had a privileged life. And I got lucky. And I’m unhappy.” Naive meritocracy—the presumption that success is an accurate proxy for talent, skill, and determination—“fails to give honors and rewards to the most competent people,” Pluchino, Biondo, and Rapisarda conclude, “because it underestimates the role of randomness among the determinants of success.”They built a mathematical model that allowed them to simulate luck and quantify its effect on the success of a group of individuals. At the beginning, 1,000 20-year-olds have the same amount of capital, this model’s measure of success, but not the same amount of talent, which remains static for each person. Over a 40-year timespan, with 6-month timesteps, random events—500 lucky and 500 unlucky during each timestep—affect the capital growth of various people. Getting lucky doubles your capital—but only if you have a sufficient amount of talent to exploit the opportunity. Getting unlucky halves your capital regardless of your talent. At the end of 100 trials, the best performer, in terms of total capital, had a slightly better-than-average talent level (0.6 on a scale from 0 to 1). The authors write that their model “seems able to account for many of the features characterizing the…largely unequal distribution of richness and success in our society, in evident contrast with the Gaussian”—or normal—“distribution of talent among human beings.” In other words, luck explains why there is a gross mismatch between the range of wealth among people and the range of talent and intelligence. “Average IQ is 100, but nobody has an IQ of 1,000 or 10,000,” as the MIT Technology Review put it in an article on the paper.Their result is consistent with the argument the economist Robert H. Frank made in his 2016 book, Success and Luck: Good Fortune and the Myth of Meritocracy. “The whole process of constructing life narratives is biased in ways that almost guarantee that people won’t recognize the role of chance events adequately,” he told interviewer Bob Henderson for Nautilus. We remember the moments or months of perseverance that contributed to our triumphs, but forget, or even fail to notice, when Lady Fortune smiles on us: a great teacher, a chance encounter, or—in Breaking Bad actor Bryan Cranston’s case—having other A-list actors not take up the role of Walter White. “He is today one of the very most sought after actors in his age group,” Frank said. “But I still wouldn’t have heard of him except that [John] Cusack and [Matthew] Broderick turned the role down first.”But don’t tell your successful friends that they’re lucky, Frank warns. That always seems to backfire. People take it as a put-down: You don’t deserve what you have. To have people grasp the role of luck in their lives, Frank said ask them instead “if they can think of any examples of times when they might have been lucky along their path to the top.” Rather than get angry or defensive, people’s “eyes light up,” Frank said. “They try to think of examples, they recount one to you, and that prompts them to remember another one, they tell you about that one too, and soon they’re talking about investments we ought to be making” in helping the most talented people with less or average luck realize their potential.New technology has increased the significance of chance events even further, Frank told Nautilus features editor Kevin Berger. “Technology lets the people who are good at what they do extend their reach much more broadly than before. If you’re the best author of tax software, now you can do the taxes for tens of millions of people everywhere,” he said. “So what those kinds of markets do, is they set up huge tournaments. Thousands, tens of thousands of people try to become anointed as the best at what they do and so the prize, if you win, is much, much bigger than before even if the person who wins is only one-tenth of 1 percent better, or just a little luckier, than the next best contestant who didn’t win.”Frank read and made comments on Pluchino, Biondo and Rapisarda’s paper, which they acknowledge in a note at the paper’s end. The authors echo Frank’s support of ways to raise the average level of luck, like replacing the income tax with a progressive consumption tax, “to counterbalance the unpredictable role of luck and give more opportunities and resources to the most talented [people]—a purpose that should be the main aim of a truly meritocratic approach.” Frank said, to him, good luck seems to matter more than it did when he was growing up in a family without much money. He graduated from Georgia Tech without any debt. “Today to graduate from a good school I’d be $40,000 in debt coming out of college. That’s if I got to college,” he said. “The poor kids today don’t get to participate in music programs, art programs, sports programs—there’s extra fees for those.”On Maron’s podcast, Burnham admitted that he’s struggled with dealing with the meaning of those dynamics. But Maron eventually had enough of Burnham’s modesty. “What it really comes down to is, when you get your break, and you’re a performer: Can you take advantage of it? Can you build a career out of it?” Maron said. When Burnham acknowledged that he had, Maron added, “So give yourself a break on that one.” Luck isn’t everything.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why Bill Gates is lucky. "
  },
  {
    "imageUrl": "http://static.nautil.us/14951_b47a38d37ff1ee67b87f6c6a33a84145.jpg",
    "title": " How Nuclear Explosions Were Used to Save the Environment",
    "description": "Posted by Amos Zeeberg on July 01, 2018  In the late spring of 2010, the world watched, often in real time, a new kind of environmental disaster unfold: An oil rig operating deep under the seafloor of the Gulf of Mexico exploded,…",
    "category": "Matter",
    "content": "In the late spring of 2010, the world watched, often in real time, a new kind of environmental disaster unfold: An oil rig operating deep under the seafloor of the Gulf of Mexico exploded, and the well under it began spewing oil copiously into the waters above. BP’s Deepwater Horizon had previously drilled the deepest oil well in the world, through nearly a mile of water and seven miles of rock. After the explosion, the rig’s impressive reach became its greatest flaw, as the well proved impossible to seal under such depths. It became the biggest marine oil spill in history.As BP cycled through various unsuccessful fixes, some observers quietly discussed a different, rather unconventional approach: setting off an underground nuclear bomb to seal the gash with rubble. “Seafloor nuclear detonation is starting to sound surprisingly feasible and appropriate…I never thought I would hear myself write that out loud,” wrote University of Texas engineer Steven Webber. But the nuclear option seems to have never become a serious possibility. An explosion might have destroyed the well without sealing it, making closure a permanent impossibility. A task force assembled by energy secretary Steven Chu dismissed the possibility out of hand; a senior official said, “It’s crazy.”The idea may not have been as crazy as it seemed—or, at least, its craziness was not altogether unprecedented. The Soviet Union had successfully used underground nuclear explosions to snuff out fires at out-of-control natural-gas wells four times in the 60s and 70s. This was just one part of a large Soviet program to use nuclear explosions for a variety peaceful ends; the U.S. had a similar yet smaller program. Much of the information about these Cold War-era efforts was little known until 1998, when Milo Nordyke, a former director of the Lawrence Livermore National Laboratory, put out an authoritative report on this historical curiosity.One plan involved using a string of bombs to carve out a replacement for the Panama Canal.It seems strange to us now to think of nuclear bombs as just another tool for engineers to shape our environment. We have to think back to the post-War mindset, enthusiastic for all things nuclear, when people on both sides of the Iron Curtain thought our cities, cars, and lives would soon draw their power from splitting or combining atoms. The Soviet representative to the UN channeled this nuclear boosterism when he proclaimed, “The Soviet Union did not use atomic energy for the purpose of accumulating stockpiles of atomic bombs…it was using atomic energy for purposes of its own domestic economy: blowing up mountains, changing the course of rivers, irrigating deserts, charting new paths of life in regions untrodden by human foot.” President Dwight Eisenhower sounded similar notes in his “Atoms for Peace” speech, also at the UN: “It is not enough to take this weapon out of the hands of their soldiers. It must be put into the hands of those who will know how to strip its military casing and adapt it to the arts of peace.” Of course, at the time, the U.S. and USSR were also accumulating stockpiles of atomic bombs big enough to wipe out humanity many times over.The first concrete step toward the use of peaceful nuclear explosions came in 1957, when the U.S. carried out the world’s first underground nuclear explosion, 900 feet (270 meters) below the Nevada desert. The test went exactly according to plan, producing little damage or radioactivity above ground, giving a “tremendous boost of enthusiasm and confidence that a variety of peaceful uses for nuclear explosions were possible and could be implemented safely.” Over the next 16 years, the U.S.’s Plowshare Program set off 12 more explosions, most to test the use of nukes for extracting natural gas or excavating the Earth’s surface. One plan involved using a string of bombs to carve out a replacement for the Panama Canal.The Soviets came to the game later but with more enthusiasm. In 1965, they set off their first civilian nuclear explosion, this one placed much closer to the surface in order to create a crater near a river in Kazakhstan. The idea was to divert water into the crater to create a reservoir that could be drawn upon for irrigation in dry seasons. The test was successful, and the project’s director, Efrim Slavskiy, reportedly hopped into the new lake, proudly becoming the first person to swim in it.A year later, Soviet engineers found another creative use for the bomb. At the time there was an out-of-control gas well in Uzbekistan that had been burning for almost three years, spewing out 420 million cubic feet (12 million cubic meters) per day, enough to supply all of St. Petersburg. To make matters worse, the gas had a high concentration of poisonous hydrogen sulfide, making it dangerous for workers trying to seal the well and for nearby residents should a botched attempt send the gas their way. After every conventional approach failed, the decision was made to use a nuclear bomb to try to pinch the well closed far underground. A new hole was drilled down near the borehole, a specially designed bomb was put in place, and the hole was filled with concrete. Twenty-three seconds after the bomb went off, 33 months after it ignited, the inferno was finally extinguished. A contemporary video captured the build-up and dramatic conclusion:These early successes kicked off an active program that included 122 nuclear explosions and stretched all the way through late 1988, as the Soviet Union started crumbling. Engineers used nuclear bombs to not only seal gas fires and create lakes, canals, and dams, but also to increase oil extraction, create underground cavities, find geological resources, and create new elements. Perhaps the most surprising use was creating large underground spaces where especially toxic waste was disposed of, isolated from the biosphere and water sources. In the post-Soviet years, Russian scientists have suggested getting rid of waste from nuclear plants by putting it in a chamber deep underground and detonating a bomb there, fusing the waste and rock together in a stable block whose radioactivity would safely dissipate over the course of millennia. Funny to think that nuclear bombs might actually be the most effective way to get rid of nuclear waste.In the 70s and 80s, nuclear power made a dramatic flip in the public mind, changing from a futuristic miracle to an environmental disaster. The U.S. and Soviet Union wound down their programs, which had come to be seen as politically radioactive. This helps explain the visceral resistance to even the peaceful use of nuclear bombs. The invention that once symbolized humanity’s world-beating ingenuity had become an emblem of our abiding hubris.Amos Zeeberg is a freelance science journalist based in Tokyo.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in October 2014. "
  },
  {
    "imageUrl": "http://static.nautil.us/15004_780bc6caf343bb06a4372c0821012624.jpg",
    "title": "Our Strange Relationship to World Cup Probabilities",
    "description": "Posted by Marco Altamirano on July 13, 2018  This year’s World Cup has been full of surprises. Tournament mainstays such as the Netherlands and Italy didn’t even qualify, and Germany, the reigning world champions, finished last…",
    "category": "Culture",
    "content": "This year’s World Cup has been full of surprises. Tournament mainstays such as the Netherlands and Italy didn’t even qualify, and Germany, the reigning world champions, finished last in their group after upsets by Mexico and South Korea. Statisticians favored powerhouses Spain and Argentina to drive into late stages of the tournament, only to see them lose to sleepers like Russia and Croatia.Yet what this World Cup reveals isn’t that the stats were wrong—far from it, they were insightfully calculated—but rather that we relate to stats and probabilities in strange ways. Most fans, for example, enthusiastically bring up “x factors” and players who are “on fire,” while stat-wielding commentators coolly remind them that what appears to be a hot run is actually statistically regular and that a victory for the underdog remains forbiddingly unlikely. But then the whistle blows and the bizarre alchemy of the world takes over. Suddenly, a typically underwhelming team like Mexico starts to dazzle and, sensing an advantage, topples a giant.To be sure, soccer is a sport that notoriously resists predictions. The batting averages and shooting percentages in baseball and basketball are far more reliable stats than anything in soccer for divining contest results, perhaps because the collective performance of a soccer team, as opposed to a baseball or basketball team, greatly outweighs any individual contribution from its players. This isn’t to say that probabilities in soccer are unreliable; it’s just that these probabilities apply better to classes of outcomes, like a set of coin tosses, than to this or that particular outcome, like whether the next coin flip will be heads or tails.We say, for example, that there’s a 50 percent chance of a coin landing heads or tails. But technically, all this probability states is that in the immense class of coin-toss events, given enough tries, and all other things being equal, the coin will land heads half the time. Nevertheless, the coin could land heads 99 times in a row out of 100, and you can even expect that to happen, given enough tries.Of course, sports matches are considerably more complex than coin-tosses. There’s no need to develop statistics about how different coin-tossers performed against others in the history of coin-tossing to calculate the probability of a coin landing tails. But the underlying nature of the probability remains the same—equally weighted teams should win about half the time against each other, and teams that have performed well against other teams in the past should, in general, perform well against them in the future.This explains the difference in perspective between the fan and the statistician. The statistician is interested in the grand scheme of events, where “x factors” and “hot runs” simply become a part of an average, whereas the fan is interested in the unlikely series of flourishes that might make this particular match exceptional. These two conflicting perspectives are part of what makes sports matches such passionate events. We know what to expect, but we also know something else is possible, so we hope, despite the odds.So we can, with confidence, expect Germany to beat South Korea and Spain to beat Russia, but only most of the time. However, for better or worse, “most of the time” is patently not “today” or even “this World Cup.” And this makes the upcoming match between France, the 1998 World Champions, and Croatia, first-time finalists and the second smallest nation to ever reach the championship match, all the more thrilling.Marco Altamirano is a writer based in New Orleans and the author of Time, Technology, and Environment: An Essay on the Philosophy of Nature.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/15008_7448bfbe44167441e18ad91ea71ed19a.jpg",
    "title": " The Logic Puzzle You Can Only Solve with Your Brightest Friend",
    "description": "Posted by Brian  Gallagher on July 16, 2018  You’ve been caught snooping around a spooky graveyard with your best friend. The caretaker, a bored old man fond of riddles (and not so fond of trespassers), imprisons each of you in…",
    "category": "Ideas",
    "content": "You’ve been caught snooping around a spooky graveyard with your best friend. The caretaker, a bored old man fond of riddles (and not so fond of trespassers), imprisons each of you in a different room inside the storage shed, and, taking your phones, says, “Only your mind can set you free.” To you, he gestures toward a barred window. Through it, you can see 12 statues. Out of your friend’s window, which overlooks the opposite side of the graveyard, she can see eight. Neither of you know the other’s count.The caretaker tells you each, individually, that together you can see either 18 or 20 statues. Unfortunately, there’s no way to tell your friend how many you can spot. The only way for you both to escape is for one of you to give the total number of visible statues. Get it wrong, and neither of you ever leave. The caretaker asks you each one at a time, once a day, and you can choose to answer or to pass. Both of you know that you’re always asked first.* If you both pass on a given day, the question—are there 18 or 20?—is posed to each of you again the next day, and the next, and so on, until you get it right or wrong. The caretaker cackles, “If you need me, I’ll be out preparing your graves.”How do you escape?If you want to puzzle through it yourself, don’t read on. The solution follows.Presh Talwalkar, author of The Joy of Game Theory: An Introduction to Strategic Thinking, fleshed out the solution to the problem last year, on his blog “Mind Your Decisions.” The puzzle was posted with no attribution on a forum hosted by the Stanford engineer and Quantitative Engineering Design co-founder William Wu, by a user who had only heard of the riddle in Swedish. It originally featured an evil king and trees in a castle’s courtyard, but is presented here in a more modern scenario.To survive the encounter with the crazy caretaker, he may have to feed you, unless you were both captured on very full stomachs. That’s because, in the worst case, it will require five days of detainment to decamp. Why?First, you and your friend have to realize that each “pass” counts as a signal. Given that you haven’t had time to consult with each other before being separated, this will require some degree of intelligence—and confidence in each other. And second, it will require five days of signalling to deduce the number of statues with certainty. In game theoretic terms, this establishes the needed “common knowledge” to escape. Many logic riddles rely on this concept, like “Blue Eyes,” described on the website xkcd.com—how can you figure out your eye color if other people won’t tell you and you can’t see your reflection? The social scientist Simon DeDeo describes another logic puzzle whose solution relies on building common knowledge in his Nautilus article “The Bitcoin Paradox.”Day 1If you saw 19 or 20 statues, then you could conclude there are 20 statues. But you only see 12, so you pass. This signals to your friend that you see at most 18 statues.If your friend saw 0 or 1 statues, knowing that you see at most 18, she could conclude there would have to be 18 statues. But your friend sees 8, so passes. This signals to you that she sees at least 2 statues.Day 2If you saw 17 or 18 statues, then you could conclude there are 20 because your friend must see at least 2 statues. But you only see 12, so you pass. This signals to your friend that you see at most 16 statues.Now if your friend saw 2 or 3 statues, knowing that you see at most 16, she could conclude there would have to be 18 statues. But your friend sees 8 statues, so she has to pass. This signals to you that she sees at least 4 statues.Day 3If you saw 15 or 16 statues, then you could conclude there are 20 because your friend must see at least four statues. But you only see 12, so you pass. This signals to your friend that you see at most 14 statues.Now if your friend saw four or five statues, knowing that you see at most 14 statues, she could conclude there would have to be 18 statues. But she sees eight statues, so she has to pass again. This signals to you that your friend sees at least six statues.Day 4If you saw 13 or 14 statues, then you could conclude there are 20 because your friend must see at least six statues. But you only see 12, so you pass. This signals to your friend that you see at most 12 statues.Now if your friend saw six or seven statues, knowing that you see at most 12 statues, she could conclude there would have to be 18 statues. But your friend sees eight statues, so she has to pass again. This signals to you that she sees at least eight statues.Day 5Since your friend sees at least eight statues, and you see 12, you know there are 20. You guess 20 and get freed!Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.* This post was edited to clarify that the question is not posed simultaneously, and that both prisoners know who’s asked first.WATCH: How to define intelligence in terms of the Rubik’s Cube.A systems theorist and provocateur who readily jumps across academic boundaries.” data-credits=”” style=“width:733px”>This classic Facts So Romantic post was originally published in January 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/14978_fa7410de86471342fc198f32104ccb31.png",
    "title": "The Biggest Misapprehension About Human Origins",
    "description": "Posted by Brian  Gallagher on July 06, 2018  Archaeologist Ticia Verveer recently posted a thread on Twitter showing that customer complaints go way back. And I mean way back. Verveer referred to a letter inscribed on a 3,700-year-old…",
    "category": "Biology",
    "content": "Archaeologist Ticia Verveer recently posted a thread on Twitter showing that customer complaints go way back. And I mean way back. Verveer referred to a letter inscribed on a 3,700-year-old Babylonian clay tablet. In the letter, Verveer writes, “The copper merchant Nanni details at length his anger at a sour deal, and his dissatisfaction with the quality assurance and service of Ea-nasir. Nanni complained that the wrong grade of copper ore has been delivered after a gulf voyage and that there was a misdirection and a delay of a further shipment.” Damn, some things never change.The complaint letter was translated by A. Leo Oppenheim, a cultural anthropologist. “His aim was to make Mesopotamian records as commonly understood as classical ones, which when quoted can stay in the original Latin or Greek,” a colleague of Oppenheim’s wrote. In one of his books, Letters From Mesopotamia, Oppenheim explains that clay-tablet letters deal with, well, nearly everything. “Law codes, royal edicts, and legal documents of impressive variety,” Oppenheim writes, “combine with an abundance of administrative records and letters, both private and official, to rough out the functioning of the main social institutions and the role of the individual in that constantly changing society.”Today, of course, Mesopotamian civilization, like many impressive societies after it, are gone. But the human complaint remains. We may have Yelp to help us shop for copper ore. But if we’re sold the wrong grade, we are going to dash off an angry letter, though probably not on a clay tablet. Certain features of human behavior recur regardless of culture.Does that mean that we are in some sense fine-tuned by natural selection to be a particular kind of creature? Nope, says Ian Tattersall, a paleontologist and the former chairman of the department of anthropology at the American Museum of Natural History. The notion that evolutionary forces sculpted humans in a certain way is misleading. In fact, he says, it’s the biggest misapprehension about human origins. “We can basically blame evolution for our shortcomings and look upon ourselves as somewhat optimized, and therefore not have to change our behaviors,” he told Nautilus. “We are not the product of perfectionizing. We are, in many ways, totally accidental. That to me is the big lesson. If we’re accidental, then we have the responsibility to exploit our own abilities in the most responsible way.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14956_e77db99f963f25326605bcf75f3eb130.jpg",
    "title": "The Young Milky Way Collided With a Dwarf Galaxy",
    "description": "Posted by Ramin Skibba on July 03, 2018  Reprinted with permission from Quanta’s Abstractions blog.As the Milky Way was growing, taking shape, and minding its own business around 10 billion years ago, it suffered a massive…",
    "category": "Matter",
    "content": "Reprinted with permission from Quanta’s Abstractions blog.As the Milky Way was growing, taking shape, and minding its own business around 10 billion years ago, it suffered a massive head-on collision with another, smaller galaxy. That cosmic cataclysm changed the Milky Way’s structure forever, shaping the thick spirals that spin out from the supermassive black hole at the galaxy’s core. Two new studies—one published last month, another still under peer review—describe the evidence for this previously unnoticed event.“This is a big step forward,” said Elena D’Onghia, an astrophysicist at the University of Wisconsin who is unaffiliated with the new research. “It’s interesting because we can finally see what the history of the Milky Way is.”To uncover evidence of the collision so many eons later, astronomers have to work like galactic archaeologists, sifting through myriad sources of surviving information to piece together a story consistent with the available evidence. Both research teams relied on data from the European Space Agency’s Gaia space telescope, which has spent years gathering exceptionally rich biographies of millions of stars—not only their locations and motions, but for many, their brightnesses, temperatures, ages, and composition as well. They essentially created high-resolution and multidimensional maps of the Milky Way and used these maps to find anomalous populations of old stars that appear to retain a memory of the long-ago collision. “The Gaia results really are allowing us to see things in the galaxy that we maybe suspected were there but haven’t seen,” said Kathryn Johnston, an astrophysicist at Columbia University.Hints of a dramatic collision had been seen before, but the indications had been inconclusive. A distinct clump of unique stars would have been a giveaway that they’re interlopers from elsewhere, but no such evidence exists. The long-ago collision so thoroughly shook things up that the telltale stars have been strewn throughout the galaxy. “There’s debris everywhere,” said Vasily Belokurov, an astronomer at the University of Cambridge and a leader of one of the two teams. “You’re basically surrounded by that debris now.”He and his team found a large number of stars that aren’t moving in step with the galaxy’s rotation. Instead, they move in radial orbits, streaming toward or away from the center of the galaxy. These stars are also rich in “metals”—the catch-all description astronomers give to any element heavier than hydrogen, helium, or lithium. Metal-rich stars likely descend from many previous generations of stars. They’re the scions of stars from a long-ago galaxy that smacked into the Milky Way, their orbits still reflecting the odd trajectory of that cosmic agitator.“If you throw a stone in a pond, those ripples last for awhile. In an analogous way, if you shake the Milky Way disk, even billions of years ago, it can take awhile for that response to settle down,” said Johnston.Belokurov’s group also modeled different collision scenarios, as well as a possible quieter history without significant collisions. An impact of a small “dwarf” galaxy indeed could have deposited a cloud of stars like the ones seen today, they found. Their work was published online last month in the Monthly Notices of the Royal Astronomical Society.The other group, led by Amina Helmi, an astronomer at the University of Groningen in the Netherlands, based its study on a newer, larger data set from Gaia and included a more detailed analysis of the chemical properties of the stars. The abundance of iron, produced by supernova explosions, relative to elements like magnesium, generated by massive yet short-lived stars, yields clues about the history of the galaxy up until the present day. Helmi and her team used this data to conclude that the Milky Way’s inner region contains hints of debris from an ancient galactic impact. They named this ancient galaxy Gaia-Enceladus.The collision could help resolve a longstanding question about the structure of the Milky Way. The galaxy’s spiral disk of stars is actually made of two parts: a thinner, denser region encompassed by a thicker, more diffuse region. Astronomers aren’t sure how this thick disk came about. Perhaps those stars came from another galaxy, or they’re stars from the thin disk that have interacted with one another and migrated outward over time. Helmi and Belokurov’s work suggests that instead, the Gaia-Enceladus collision ejected thin-disk stars out into the thick disk. “If this collision happened to the young Milky Way, then it would damage the stellar disk, smash it up, and send stars up to high galactic heights,” Belokurov said.The investigation continues. Both groups are uncertain about how big Gaia-Enceladus likely was and exactly when it fell into the Milky Way. And no one can say for sure how our galaxy’s disk got heated and puffed up into a thicker one. “We don’t understand how important the impact is alone, but now we have a culprit” that could have created the thick disk, Johnston said. “What would be really exciting would be to look carefully in the disk and trace back this event and see if we’re able to find a more direct effect that’s still going on, a leftover echo.”Ramin Skibba is an astrophysicist turned science writer and freelance journalist. His work has also appeared in Newsweek, Slate, Nature, Science, Scientific American, and New Scientist.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14989_0b49b88c68f7ecbdb73f50496c084a1e.jpg",
    "title": "Why It’s Hard to Recognize a Black Hole",
    "description": "Posted by Summer Ash on July 11, 2018  Astronomers can sometimes be literal to a fault. We like to call things as we see them. For example, if it’s red and it’s huge: “Red Giant.” White and small: “White Dwarf.”…",
    "category": "Matter",
    "content": "Astronomers can sometimes be literal to a fault. We like to call things as we see them. For example, if it’s red and it’s huge: “Red Giant.” White and small: “White Dwarf.” Massive explosion: “Big Bang.” Dark and sucks everything in: “Black Hole.” Most of the time, classifying objects this way works fine—either it’s new, or it’s something we already know of. But sometimes, as with Pluto, we make new observations that force us to question the name, reassess the object, and identify it differently. You might think this never happens with something as clearly defined as a black hole, but you’d be wrong.Though we can’t observe them directly, we can see how the two types of black holes—stellar mass and supermassive—affect their surroundings. Stellar mass black holes, the product of a dying star going supernova and collapsing on itself, are the more familiar, predicted nearly a century ago by Einstein’s theory of general relativity; they usually only affect the behavior of the nearest star or two. Supermassive black holes, on the other hand, are over a million times more massive. We still don’t know how these form, but we believe they exist at the center of almost every galaxy, sometimes having the power to alter the appearance of their entire galaxy.This capacity for mass distortion makes characterizing supermassive black holes particularly tricky. As the stars, gas, and dust in the center of a galaxy get closer and closer to a supermassive black hole, they get packed tighter and tighter into a smaller and smaller space, heating up until, at a critical distance, everything is ripped apart, reduced to atomic particles. When we spot supermassive black holes, it’s this heat radiating away from the orbiting debris—known as an accretion disk—that we actually see, not the black hole itself. Some supermassive black holes “eat” more than others and, in the process, give off significantly more light than their less active brethren. These “active galactic nuclei,” or AGN for short, are some of the most powerful, most energetic forces in the Universe. Not only do they give off heat, they also often eject material in the form of collimated (beamed) jets, perpendicular to the plane of the disk, which blast their way out of the galaxy’s core—dwarfing in size not just the accretion disk, but also the galaxy itself. What’s more, some AGN have a dusty torus, the geometric equivalent of a donut, in the same plane as their accretion disk, but much, much bigger and thicker. So thick, in fact, that if you looked at them from the side, you wouldn’t see the disk at all, much less the black hole in the center (as seen in the image above). Despite having this standard model of an AGN—a supermassive black hole surrounded by an accretion disk with jets streaming out in opposite directions, all encompassed by a dusty torus—making sense of our observations is still a challenge: The light we see doesn’t always paint the same picture. Sometimes we see jets, sometimes we don’t. Sometimes we see the torus, sometimes we don’t. Sometimes we see light so concentrated and bright that we can’t even tell if there’s a galaxy there at all. We label these sightings accordingly: AGN at great distances with cores so bright, they outshine all their stars in optical light, are called quasars (for “quasi-stellar”), like the one pictured above; AGN that glow strongly in the infrared are called Seyferts, after the astronomer Carl Seyfert, who first identified them in 1943; And AGN, with cores and jets whose emitted light dominates in the radio spectrum, are called radio galaxies. If they are all fueled by supermassive black holes, why don’t all AGN look the same? One reason could be our point of view. The theory of AGN unification posits that all AGN have the same basic building blocks (accretion disk, jets, torus); The striking differences we observe, according to this theory, are all due to their orientation in space. Here on Earth, we only have one vantage point from which to observe the cosmos. We see galaxies randomly distributed around us, some of them edge-on, some of them face-on, and the rest at all the angles in-between. We cannot fly around to look at these galaxies from any other angle than the one they present to us. But with the advent of supercomputers, we can now simulate these galaxies better than ever before and virtually fly around them as much as we like, enjoying the sights from any angle. We can take an AGN and turn it so we’re looking straight down one of the jets, towards the galactic core, making it resemble a blazar, sort of a blazing quasar. Start tilting the AGN until the jet is rotated ninety degrees away from us, and it appears to morph from a blazar to a quasar to, finally, a Seyfert. Yet AGN unification is far from a settled problem in astrophysics. There could be other factors at play than just our point of view, like physical processes in and around black holes we don’t fully understand or measurements we haven’t thought to take. As we build better telescopes and amass new data, we can only hope that we’ll see these active galactic nuclei for what they really are. Otherwise, we might need a lot more names.Summer Ash is the Director of Outreach for Columbia University’s Department of Astronomy. She is also the “In-House Astrophysicist” for The Rachel Maddow Show and tweets as @Summer_Ash.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Did the movie Interstellar portray black holes correctly?This classic Facts So Romantic post was originally published in November 2015. "
  },
  {
    "imageUrl": "http://static.nautil.us/14953_8bf2aed40c2e4aa48c5dd56f16e6f2de.jpg",
    "title": "Why Enceladus’ Ice Is Part of the Climate Change Conversation",
    "description": "Posted by Brian  Gallagher on July 02, 2018  Beneath the icy surface of Saturn’s sixth-largest moon, Enceladus, an ocean dwells. Traces of it get expelled skyward through cracks in the crust via cryo-volcanic plumes. What’s found…",
    "category": "Matter",
    "content": "Beneath the icy surface of Saturn’s sixth-largest moon, Enceladus, an ocean dwells. Traces of it get expelled skyward through cracks in the crust via cryo-volcanic plumes. What’s found in the ice grains and vapor are nothing less than the rudiments of life. “We are, yet again, blown away by Enceladus,” Christopher Glein, a space geoscientist at the Southwest Research Institute, in Texas, said, remarking on the findings he and his colleagues reported this month in Nature. They found in the data gathered by the Cassini spacecraft evidence of complex organic molecules with masses well above those discovered in prior analyses. “We must be cautious,” Glein went on, “but it is exciting to ponder that this finding indicates that the biological synthesis of organic molecules on Enceladus is possible.”The news reminded me of a line from Ice: Nature and Culture, a new book by Klaus Dodds, a professor of geopolitics, that surveys our relationship with ice. “There is no reason to think that we will not make further discoveries of ice throughout the solar system,” he writes, “and perhaps these will act as an imaginative counter-weight to ongoing stories about earthly losses.” He means that the vanishing of our own store of ice in the polar regions and elsewhere, as a result of climate change, is also a depletion of a source of the numinous. “High and cold places are intimately connected to the imaginative, even the spiritual,” he writes. “Up and above the cloud line, mountains offered a sort of transcendence from earthly concerns and the white landscapes of the Alpine, Arctic and Antarctic environments provoked in the Western Romantic traditions an appeal to the sublime.”To imagine the absence of these landscapes produces something like the opposite feeling, a pang of emptiness and a longing to appreciate that terrain in person before it passes. The prospect is not necessarily a far-off one. Christine Dow, a glaciologist at the University of Waterloo, said, commenting on a new study she co-authored, “We are learning that ice shelves”—thick, floating sheets of ice permanently attached to a landmass—“are more vulnerable to rising ocean and air temperatures than we thought.”Another new paper, titled “Choosing the future of Antarctica,” conveys a similar sense of alarm. It outlines two possible, polar-opposite paths we could take—one good, the other not so much—toward the year 2070. It’s also written from the perspective of someone from each future, sizing up what went right and wrong. The lead author, Steve Rintoul, an Australian oceanographer and climate scientist, said we don’t have much wiggle room on the path to a good future. “Greenhouse gas emissions must start decreasing in the coming decade to have a realistic prospect of following the low emissions narrative and so avoid global impacts associated with change in Antarctica, such as substantial sea level rise.” His co-author, Martin Siegert, a British glaciologist, added, “Damage there will cause problems everywhere.”Some of the damage is already irreversible, like the loss of ice shelves. Once they calve off, it is almost impossible to put them back. In the bad, high-emissions future, we’ve allowed this to ramp up. “The large number of icebergs produced by collapsing ice shelves all around Antarctica,” the authors write, “is now carefully monitored to manage the risks to the greatly expanded fishing, tourism and commercial shipping fleets, and Antarctic operations by Antarctic Treaty nations.” Watching a withering continent, we find a silver lining in economic productivity.In the good, low-emissions future, we have no need to exploit the area. “Motivated by a clearer appreciation of the threats to the region and the global value of better understanding Antarctica and its links to lower latitudes, the parties [to the Antarctic Treaty] reaffirmed the commitment to maintain Antarctica as a natural reserve for peace and science…”Which future we choose for ourselves depends on our capacity for something psychologists call self-transcendence, the ability to care about the distant future at least as much as your own, more proximal, future. In a recent neuroimaging study, researchers investigated the neural mechanisms that underlie our ability to evaluate future events. “Thirty-six participants viewed a series of events, consisting of potential consequences of climate change, which could occur in the near future (around 2030), and thus would be experienced by the participants themselves, or in the far future (around 2080),” they write. They saw the ventromedial prefrontal cortex, a brain region above the eyes that encodes the personal significance of future events, increase its activation when the subjects saw images of a far-off future wrecked by climate change. “Importantly, this activation increase was observed only in participants with pronounced self-transcendence values measured by self-report questionnaire, as shown by a statistically significant interaction of temporal distance and value structure,” the researchers write. “These findings suggest that future projection mechanisms are modulated by self-transcendence values to allow for a more extensive simulation of far future events.”Tobias Brosch, a psychologist at the University of Geneva and the study’s lead author, says people can get better at envisioning the ramifications of climate change. “We could imagine a psychological training that would work on this brain area using projection exercises,” he said. “In particular, we could use virtual reality, which would make the tomorrow’s world visible to everyone, bringing human beings closer to the consequences of their actions.”A less direct but perhaps also effective use of V.R. might be to bring Enceladus to life. What better spur to self-transcendence could there be than visiting its distant and mysterious ice fields and geysers, and being reminded of the universal connection between the future of ice and the future of our own planet?Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Stories about a life spent searching for life among the stars. "
  },
  {
    "imageUrl": "http://static.nautil.us/14986_a2bb46e362d70a83673a822206f5062e.jpg",
    "title": "The Painful Wait for a Hangover Pill",
    "description": "Posted by Troy Farah on July 10, 2018  From freezing showers to ingesting prickly pear to smoking joints, everyone has a home remedy for alcohol’s notorious afterglow: the hangover. Mongolian men swear by pickled sheep…",
    "category": "Biology",
    "content": "From freezing showers to ingesting prickly pear to smoking joints, everyone has a home remedy for alcohol’s notorious afterglow: the hangover. Mongolian men swear by pickled sheep eyes, ancient Egyptians wore necklaces of Alexandrian laurel, and one 17th century English physician even sold a hangover “cure” made with human skulls and dried vipers. Hangovers are a problem that even predates writing. But today with the aid of modern medicine we can treat diarrhea or headaches with over-the-counter drugs—so why not hangovers too? “Each year, many people die because they drink too much,” Yunfeng Lu, a chemical engineering professor at UCLA, said in a phone call. “And currently, we have no antidote.”But that could change. New research from Lu and his colleagues published in the journal Advanced Materials demonstrates a “hangover pill” that can mitigate some of the damaging effects of alcohol. The antidote mimics the work of hepatocytes, or liver cells, and helps speed up the body’s alcohol metabolism. It’s basically supercharging your liver’s ability to clear alcohol from the bloodstream, resulting in far lower levels of intoxication.To test their treatment, scientists got mice drunk by inserting tubes into their mouths and pumping ethanol directly to the stomach. Within a few minutes, the rodents became intoxicated and fell asleep. Then, the researchers injected nanocapsules of blood cells loaded with enzymes that help break down alcohol into less harmful byproducts. Afterwards, the mice were sacrificed and their livers were examined with fluorescent imaging to measure toxicity. The blood alcohol content of the mice dropped by 45 percent within four hours and caused less organ damage than it would have normally. But this treatment probably isn’t coming to your local CVS anytime soon. Human trials are still a few years off, and so far, these nanocapsules are only being developed for emergency room settings, which saw a 61 percent jump in alcohol-related visits between 2006 and 2014. And it’s not just problem drinkers that could use some relief. One survey of 2,000 people found that if you have only one hangover a month, it adds up to two years of total sick time over the course of a lifetime.That’s why Lu and others are racing to find alternatives to help make alcohol’s aftermath suck less. But before we get to that, we need to understand what makes a hangover such a grating experience. Though nature may never have intended us to drink as much as we do today, the ability to digest ethanol, a tasteless liquid produced by fermented sugars, might have been crucial to our survival as a species 10 million years ago. Back then, the climate was rapidly changing, and fruit on the ground was more likely to ferment. The capacity to digest liquor would have been advantageous for our primate ancestors, who relied heavily on fruit for sustenance.We largely owe our drinking abilities to ADH4, or alcohol dehydrogenase 4, an enzyme that lets us harvest the caloric traits of alcohol. But during the oxidation process that breaks alcohol down, ADH4 creates acetaldehyde, a known carcinogen that damages DNA. ALDH2 (aldehyde dehydrogenase), a liver enzyme that also protects the heart against heart attacks, breaks the acetaldehyde down into less toxic acetate, which later becomes carbon dioxide and water.But the more you drink, the more acetaldehyde builds up, faster than the body can metabolize, creating noxious, even cancerous conditions. Long-term exposure can lead to health problems including high blood pressure, depression, and a leaky gut. In addition, the effects appear to be worse for women.Yet, after all this time, the exact mechanisms of what causes a hangover still elude scientists. Acetaldehyde is a common culprit, but the most unpleasant hangover symptoms occur when acetaldehyde levels are low. Others blame dehydration, low blood sugar, or inflammatory proteins called cytokines, but the jury is still out.So while Lu’s hangover antidote may be promising, it may not address all of the multiple pathways that can lead to alcohol toxicity. Other experimental chemicals have been found to have potential hangover-leavening effects, such as ampelopsin, also known as dihydromyricetin, a compound found in the Japanese raisin tree. It dampens the effects of alcohol withdrawal and can even reportedly sober you up faster by interacting with structures in the brain, called GABAA receptors, that alcohol normally interferes with.Other drugs for mitigating alcohol withdrawal or excessive drinking are thought to act on these same GABAA receptors. They include naltrexone, acamprosate, benzodiazepines like Valium, and clomethiazole, which The Who’s drummer Keith Moon famously overdosed on. Yet, all of these drugs have limited success. Maybe it’s smarter to circumvent ethanol altogether by creating a drug that gives a buzz without the fuss.The U.S. is currently in the grip of a drug overdose crisis, which killed 64,000 people in 2016—42,000 from opioids alone. But alcohol-related deaths exceed 88,000 per year and have every year since at least 2006—far outpacing the mortality of the opioid crisis. Moderate drinking is probably fine. It even has some health benefits, if you ask the alcohol industry. Yet, if we have a drug epidemic, booze plays no small part. That’s why researchers believe using biochemistry to develop a safer alternative to alcohol is crucial. One idea: make a drug with no hepatotoxicity or comedown.That’s the goal of Alcarelle, a U.K.-based startup that aims to develop a new drug to replace alcohol—one without side effects, including hangovers. Alcarelle was founded by neuropsychopharmacologist David Nutt, who was fired from his post as Britain’s chief drug czar, for saying alcohol is more dangerous than LSD. (He seems to be right, though, as some studies have demonstrated.) “We are not aiming to replicate the action of alcohol, as this is very unpredictable and extremely harmful,” Emily Palmer, a researcher at Alcarelle, explained in an email. “Instead, we are aiming to create an alternative substance to alcohol, which would produce a tipsy-like feeling, replicating the enjoyment many people experiencing after drinking a few alcoholic beverages.”But neuroscientist Lindsay Halladay isn’t so sure this can be done. “There is not going to be some magical chemical compound that is rewarding and has zero negative side effects,” she said in a call. Halladay is an assistant psychology professor at Santa Clara University. The bulk of her research has focused on the underlying neural circuits involved in stress and addictive behavior—for example, what parts of the brain encourage us to keep drinking when we know we shouldn’t. “There are a lot of compensatory mechanisms that the brain has,” she said. “If you take some drug that increases your dopamine levels, your brain is able to recognize, ‘Hey, this is too much dopamine, let me tweak some things,’ and lowers the endogenous level of dopamine. So there’s always this homeostasis.”Halladay argued that a “safer” alcohol might encourage riskier drinking. Nutt has said in interviews his product won’t be able to get you drunk, even if you tried. But while the company says they’ve developed over 100 drug candidates, narrowing their choices down to a few contenders, the efficacy of their products has yet to be seen, but the company is confident their creation will deliver. “I have an understanding of the science involved and an appreciation of the years of work and innovation which has got us to where we are now,” Palmer said. “I am confident that this quality and quantity of research will result an effective product.”If Alcarelle can achieve its goal, it could save literally thousands of lives a year. But replacing alcohol in the hearts and minds of drinkers won’t be easy. The global liquor industry is expected to top $1.5 trillion globally by 2022, which means there’s a lot of competition, to say nothing of the regulatory hurdles Alcarelle faces from federal agencies.Nonetheless, Alcarelle are optimistic we could see “alcosynths,” as they call them, on store shelves in about four years or so, if they can raise the estimated $20 million they need. And such products might find a warmer welcome than you might think. David Orren, managing director of Alcarelle, said in an email that their product has actually stirred some interest from alcohol companies eager to diversify. “Drinks companies are already experiencing flat demand and senior executives are generally intrigued by the possibility of widening their product portfolios with an attractive ‘free from’ product that is appealing to health-conscious consumers,” Orren said.There may not yet be a drug that can reverse a hangover or subdue it altogether, but there is a medication that can cause one—a really painful, vomit-inducing one. It’s called disulfiram, or Antabuse, and it inhibits the alcohol-metabolizing enzymes that our bodies rely on to clear alcohol from our systems. Drinking even a small amount of hooch leads to a mass buildup of acetaldehyde, causing nausea, mental confusion, and even fainting. It’s enough to convince almost anyone to give up the sauce.Disulfiram arose in 1881, used as a rubber manufacturing aid. In 1948, two Danish physicians decided to eat it, because that’s what you did back then, and discovered when drinking beers later that it didn’t make them feel so well. Now disulfiram is prescribed to treat chronic alcohol use, but its efficacy is debated. “I’m always surprised that people take Antabuse because it is so horrible,” Halladay said. “You take it voluntarily, but it’s for individuals who have no other option. They’ve tried to quit drinking on their own. Addiction is obviously not a choice. People have tried everything else and eventually decide, ‘Let me take this drug that will make me incredibly sick if I give in.’”But for most people, there’s nothing wrong with enjoying a drink now and again. Drugs like Antabuse are for extreme cases, and it’s likely that the majority of us will continue waking up with fuzzy heads and queasy stomachs after a night on the town for some time.Even if you’re not quite down with the idea of an alcosynth, scientists are continuing to explore options for hangover relief. Some researchers are looking at extracts of the invasive vine kudzu to combat binge drinking and others are getting worms hungover to target new molecular pathways to alleviate alcohol withdrawal symptoms.As for Lu, he’ll continue to test the safety of his enzyme-mimicking hangover antidote on animals—if it seems safe, human clinical trials could begin within a year. In the meantime, there is currently no panacea to avoid the pitfalls of hitting the bottle. So if you’re really tired of waking up with a headache, wishing you hadn’t chugged so many brewskis the night before, it might be best to quit drinking altogether.\n\tThe newest and most popular articles delivered right to your inbox!\nThis article was originally published on DiscoverMagazine.com in May 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/14885_608c52894a47a2e6e35b9c555500b1e8.jpg",
    "title": "Why Social Science Needs Evolutionary Theory",
    "description": "Posted by Cristine H. Legare on June 15, 2018  My high school biology teacher, Mr. Whittington, put a framed picture of a primate ancestor in the front of his classroom—a place of reverence. In a deeply religious and conservative…",
    "category": "Biology",
    "content": "My high school biology teacher, Mr. Whittington, put a framed picture of a primate ancestor in the front of his classroom—a place of reverence. In a deeply religious and conservative community in rural America, this was a radical act. Evolution, among the most well-supported scientific theories in human history, was then, and still is, deliberately censored from biological science education. But Whittington taught evolution unapologetically, as “the single best idea anybody ever had,” as the philosopher Dan Dennett described it. Whittington saw me looking at the primate in wonder one day and said, “Cristine, look at its hands. Now look at your hands. This is what common descent looks like.”Evolution has shaped the human body, but it also shaped the human brain, so evolutionary principles are indispensable for understanding our psychology. Yet many students, teachers, and even social scientists struggle to see how our evolutionary history significantly shapes our cognition and behavior today. “Learning” and “culture” do not explain behavior so completely that turning to ideas from evolution is unnecessary. The lack of willingness to view human cognition and behavior as within the purview of evolutionary processes has prevented evolution from being fully integrated into the social science curriculum.A deeper scientific understanding leads to the view that learning doesn’t compete with evolution as an explanation for human psychology. Learning requires evolved psychological adaptations—general learning mechanisms or mechanisms which may be specific to a particular adaptive problem. Specialized learning mechanisms help us avoid eating toxic food, yet no one is born knowing which particular foods to avoid. Humans have also evolved an aversion to mating with their genetic kin but are not born knowing who their kin are. Solving these adaptive challenges requires evolved psychological learning mechanisms.Human cognition and behavior is the product of the interaction of genetic and cultural evolution. Gene-culture co-evolution has allowed us to adapt to highly diverse ecologies and to produce cultural adaptations and innovations. It has also produced extraordinary cultural diversity. In fact, cultural variability is one of our species’ most distinctive features. Humans display a wider repertoire of behaviors that vary more within and across groups than any other animal. Social learning enables cultural transmission, so the psychological mechanisms supporting it should be universal. These psychological mechanisms must also be highly responsive to diverse developmental contexts and cultural ecologies.Take the conformity bias. It is a universal proclivity of all human psychology—even very young children imitate the behavior of others and conform to group norms. Yet beliefs about conformity vary substantially between populations. Adults in some populations are more likely to associate conformity with children’s intelligence, whereas others view creative non-conformity as linked with intelligence. Psychological adaptations for social learning, such as conformity bias, develop in complex and diverse cultural ecologies that work in tandem to shape the human mind and generate cultural variation.Truly satisfying explanations of human behavior requires identifying the components of human cognition that evolution designed to be sensitive to social or ecological conditions and information. For example, populations in which food resources show high variance (large game hunting is very much hit or miss) tend to evoke cooperative adaptations for group-wide sharing compared to those in which food variance is lower and more dependent on individual effort, like gathered foods. Recent discoveries in the field of cultural evolution have demonstrated that our technological complexity is the outcome of our species’ capacity for cumulative culture. It has set our genus Homo on an evolutionary pathway remarkably distinct from the one traversed by any other species. In a paper last year, I proposed that this was a result of psychological adaptations being universal but sufficiently flexible for innovations to build on each other, supporting the acquisition of highly variable behavioral repertoires.Applying evolutionary theory to social science has the potential to transform education and, through it, society. For example, evolutionary perspectives can help social scientists understand, and eventually address, common social problems. Schoolyard bullying provides one example. Without an evolutionary understanding of the phenomenon, interventions are likely to be ineffective, since they misdiagnose the causes of bullying. Bullying is not merely negative interpersonal behavior; it’s goal-oriented and serves the social function of gaining status and prestige for the bully, which must be understood to combat it. For example, bullying often occurs in front of an audience, suggesting that social attention drives, and may reinforce, the behavior. A 2015 paper suggests most interventions don’t work because they remove the rewards of bullying—increased social status—without offering any alternatives. The researchers recommend that the esteem bullies seek “should be borne in mind when engineering interventions” designed to either decrease a bully’s social status or channel the bully’s social motivations to better ends. A deep understanding of the evolved functions of bullying, in short, provides a fulcrum for potential remedies.If “nothing in biology makes sense except in the light of evolution,” as the evolutionary biologist Theodosius Dobzhansky argued in 1973, then nothing in human psychology, behavior, and culture does either. Social scientific research should reflect this fact.Cristine Legare is an associate professor of psychology and the director of the Evolution, Variation, and Ontogeny of Learning Laboratory at The University of Texas at Austin. Her research examines how the human cognitive system enables us to learn, create, and transmit culture. She conducts comparisons across age, culture, and species to address fundamental questions about cognitive and cultural evolution. Follow her on Twitter @CristineLegare.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The evolutionary purpose of having a teenage brain. "
  },
  {
    "imageUrl": "http://static.nautil.us/14926_490efb66c5016fa88d6fcbc4e505a7ef.jpg",
    "title": " When It’s Good to Be Antisocial",
    "description": "Posted by Silvia Golumbeanu on June 25, 2018  Bees are emblems of social complexity. Their honeycombs—intricate lattices dripping with food—house bustling hive members carrying out carefully orchestrated duties like defending against…",
    "category": "Biology",
    "content": "Bees are emblems of social complexity. Their honeycombs—intricate lattices dripping with food—house bustling hive members carrying out carefully orchestrated duties like defending against predators and coordinating resource collection. Much of our own success is due to this sort of division of labor. Clearly, in the animal kingdom, it pays to be social: Certain neurons make us resent being alone. You could be forgiven for assuming that complex social organization is the—or at least a—pinnacle of evolution.Yet out of the 20,000 known species of bees, only a few are social. Some bee species have even given up social behaviors, opting for the single life. Why?Sometimes everybody wins when you go it alone.For one, as introverts know well, socializing requires lots of energy. Highly complex societies of insects require an elaborate arsenal of chemical and physical signals to direct their communal behavior. Social bees have more highly developed exocrine glands than their solitary cousins, and solitary halictid bees have less sensory hairs on their antennae than their social precursors. Solitary and social halictids also have different odorant systems, which play an important role in social bee communication and recognition. As the environment comes up with new demands, and the genetic makeup of the hive adapts, these features might just stop being worth the investment.For another, being social can be stunting—sometimes bees have to grow up fast to survive. Researchers at Whitman College in Washington found that the region of the newly hatched antisocial orchard bee’s brain responsible for foraging ability is about as developed as the corresponding region in the experienced forager honey bee. Antisociality encourages self-sufficiency. Orchard bees must each fend for themselves, and they emerge into the world knowing how to forage for food. For honey bees, on the other hand, only a portion of the hive has to forage at any given time.How do solitary species evolve to reap these benefits after having been social? After all, antisociality cropping up, in conjunction with other stressors, can mean the collapse of the entire hive—by increasing the minimum amount of social bees needed to sustain a hive, and decreasing the maximum amount of bees a hive can stably carry. So the prevalence of loners is not exactly favorable.Variability in social behavior is one possible answer. H. rubicundus, a sweat bee descended from social ancestors in the Halictidae family, has both solitary and social populations in Europe. Bees living in different environments prefer different behaviors: In warm climates, H. rubicundus populations favor hive-formation, while in the cold, they tend to go solo.It also turns out that, even in a highly coordinated hive, antisocial individuals persist. And they appear to be tolerated by other bees in the colony. If a few loners find themselves in a new situation where solitary behavior is advantageous—say the growing season is short and bees need to get up and go without dividing tasks—an asocial species could arise.Changes in host plants can also lead social bees to revert to solitary behavior. Depending on the bee’s environment and needs, specializing on one plant is usually more beneficial in a hive context, where the whole activity of the hive can be coordinated around a constant resource. Solitary bees are usually generalists—they buzz along from plant species to plant species.Sociality is no pinnacle of evolution. It’s just another result of the process. Reclusive bees and other species are doing just fine—and sometimes, even better. Clearly social behavior has advantages, seeding the survival of species and communities. But being a good neighbor is not the only benefit to the hive. Sometimes everybody wins when you go it alone.Silvia Golumbeanu is a writer living in Los Angeles.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in September 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/14806_da16089c5560ff14541029aceefc54de.png",
    "title": " 30 Weirdly Fascinating Health and Body Facts",
    "description": "Posted by Ellie Summers & Brian Gallagher on June 06, 2018  The camera doesn’t often linger on all the severed heads in Game of Thrones. But if it did, might we see some sign of awareness—at least for a few seconds? A human head doesn’t lose…",
    "category": "Biology",
    "content": "The camera doesn’t often linger on all the severed heads in Game of Thrones. But if it did, might we see some sign of awareness—at least for a few seconds? A human head doesn’t lose consciousness until after about four seconds, post-decapitation. That’s resiliency of a kind. And the acid in your stomach? Strong enough to dissolve razorblades. It’s not a stretch to imagine that, if the torture-loving Northerner Ramsay Bolton knew this, he’d make use of it.Two facts down, 28 more to go. How many are there that you already know?Ellie Summers is a Graphic Designer at The Website Group, a UK based Digital Agency specializing in pay monthly business web design, SEO and Social Media Marketing.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Why our nervous system needs to be out of harmony sometimes.This classic Facts So Romantic post was originally published in August 2017. "
  },
  {
    "imageUrl": "http://static.nautil.us/14852_839541bfa1e1f4a879c4a5d4e5f6d88b.jpg",
    "title": "Evidence Found for a New Fundamental Particle",
    "description": "Posted by Natalie Wolchover on June 11, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.Physicists are both thrilled and baffled by a new report from a neutrino experiment at Fermi National Accelerator…",
    "category": "Culture",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.Physicists are both thrilled and baffled by a new report from a neutrino experiment at Fermi National Accelerator Laboratory near Chicago. The MiniBooNE experiment has detected far more neutrinos of a particular type than expected, a finding that is most easily explained by the existence of a new elementary particle: a “sterile” neutrino that’s even stranger and more reclusive than the three known neutrino types. The result appears to confirm the anomalous results of a decades-old experiment that MiniBooNE was built specifically to double-check.The persistence of the neutrino anomaly is extremely exciting, said the physicist Scott Dodelson of Carnegie Mellon University. It “would indicate that something is indeed going on,” added Anže Slosar of Brookhaven National Laboratory.As for what, no one can say.“I’m very excited about this result, but I am not ready to say ‘Eureka!’” said Janet Conrad, a neutrino physicist at the Massachusetts Institute of Technology and a member of the MiniBooNE collaboration.The existence of a sterile neutrino would revolutionize physics from the smallest to the largest scales. It would finally break the Standard Model of particle physics that has reigned since the 1970s. It would also demand “a new standard model of cosmology,” Dodelson said. “There are other potential cracks in the standard picture,” he added. “The neutrino paradox could point our way to a new, better model.”Neutrinos are tiny particles that pass through our bodies by the billions each second but seldom interact. They constantly oscillate between three known types, or “flavors,” called electron, muon, and tau. The MiniBooNE experiment shoots a beam of muon neutrinos toward a giant oil tank. On the way to the tank, some of these muon neutrinos should transform into electron neutrinos at a rate determined by the difference in mass between the two. MiniBooNE then monitors the arrival of electron neutrinos, which produce characteristic flashes of radiation on the rare occasions when they interact with oil molecules. In its 15-year run, MiniBooNE has registered a few hundred more electron neutrinos than expected.The simplest explanation for the surprisingly high number is that some muon neutrinos are oscillating into a different, heavier, fourth kind of neutrino—a sterile one, meaning it never interacts with anything that isn’t a neutrino—and that some of these heavy sterile neutrinos then oscillate into electron neutrinos. The greater mass difference prescribes a higher rate of oscillations and more detections.The Liquid Scintillator Neutrino Detector (LSND) in Los Alamos detected a similar anomaly in the 1990s, prompting the construction of MiniBooNE. However, other neutrino experiments that work differently from LSND and MiniBooNE have failed to produce a clear sign of the putative sterile neutrino. “It is a curse of this business that some experiments see something while others don’t,” said Werner Rodejohann of the Max Planck Institute for Nuclear Physics in Heidelberg, Germany.If sterile neutrinos do explain the new results, physicists struggle to see how the properties of these new particles can be compatible with everything else we know. Perhaps most troubling of all, cosmological observations of light from the early universe indicate that only three flavors of neutrinos existed then. To make sense of LSND, MiniBooNE and all other experiments to date, “some completely new theoretical framework is needed,” Slosar said.Moreover, the particular sterile neutrino that could hypothetically fit MiniBooNE’s data doesn’t solve any of the mysteries that led physicists to theorize about such particles in the first place. If heavy enough, sterile neutrinos could serve as the invisible “dark matter” that seemingly engulfs galaxies. And they would explain why electron, muon, and tau neutrinos are so lightweight, through a mathematical trick called the seesaw mechanism. But at less than 1 electron volt, the putative MiniBooNE sterile neutrino lacks the heft for these other purposes. “We would have no reason to expect 1-eV sterile neutrinos,” said Matthew Buckley, a particle physicist at Rutgers University. “Not that this has stopped the universe from adding new particles in the past.”The confusion has led many experts to curb their optimism and suspect that both MiniBooNE and LSND have fallen prey to some unknown error. Freya Blekman, a physicist at the Free University of Brussels, argues that the experiments may have systematically underestimated the rate at which particles called neutral pions decay inside MiniBooNE’s oil tank—events that mimic the signal from electron neutrinos.“It’s clear there’s something to be understood, and I certainly hope it’s a fourth neutrino,” said Neal Weiner, a theoretical physicist at New York University. “That said, this would be the first discovered particle beyond the Standard Model, so the threshold for the evidence is obviously very high.” For now, he said, “I am taking a slight wait-and-see approach.”A more definitive answer will come with future experiments, including one called IsoDAR, proposed by Conrad and many of her colleagues. Rather than counting the number of neutrinos of a given flavor at the end of a beam, it will catch sight of neutrinos wiggling back and forth between different flavors as they travel, which will give a fuller picture of the oscillations. “I am not ready to bet my money yet because the excess is kind of a blob” on a plot, Conrad said. “What if something else can make a blob? To be really convinced, I want to see, with high significance, this predicted wiggle.”Natalie Wolchover is a senior writer at Quanta Magazine covering the physical sciences. Previously, she wrote for Popular Science, LiveScienceand other publications. She has a bachelor’s in physics from Tufts University, studied graduate-level physics at the University of California, Berkeley, and co-authored several academic papers in nonlinear optics. Her writing was featured in The Best Writing on Mathematics 2015. She is the winner of the 2016 Excellence in Statistical Reporting Award and the 2016 Evert Clark/Seth Payne Award for young science journalists. @NattyOver\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14929_5243958a762063341dc82d2bbf0f5f33.jpg",
    "title": "Taking Another Person’s Perspective Doesn’t Help You Understand Them",
    "description": "Posted by Brian  Gallagher on June 27, 2018  No moral advice is perfectly sound. The Golden Rule—do unto others as you would have them do unto you—is only as wise as the person following it.A more modern-sounding tip—take the…",
    "category": "Culture",
    "content": "No moral advice is perfectly sound. The Golden Rule—do unto others as you would have them do unto you—is only as wise as the person following it.A more modern-sounding tip—take the perspective of others—can seem like an improvement. It was Dale Carnegie’s eighth principle in How to Win Friends and Influence People (it is “a formula that will work wonders for you”), and Barack Obama trotted it out at the United Nations when discussing Israel and Palestine (“the deadlock will only be broken when each side learns to stand in each other’s shoes”). Perspective-taking avoids the Golden Rule’s flaw—its effect doesn’t hinge on the integrity of the person considering it. And it’s an inducement to selflessness, in that you’re encouraged to exchange your frame of reference for that of another. Perspective-taking increases the odds you’ll emotionally empathize with the person whose shoes you’re stepping into, rely less on your own biases and group-based stereotypes, and avoid automatic expressions of racial bias.In a recent study, authors Tay Eyal, Mary Steffel, and Nicholas Epley say these results, among others, “suggest that being told to put oneself into another’s perspective may result in increased interpersonal accuracy,” an understanding of the thoughts and desires of someone else. It doesn’t. After testing the impact perspective-taking had on the accuracy of interpersonal judgments in 25 experiments, the researchers concluded, “If anything, perspective-taking decreased accuracy overall while increasing confidence in judgment.” Even romantic partners together for a decade on average couldn’t get perspective taking to work when quizzed on their significant other’s preferences or views. They thought, on average, that 13 of their 20 guesses would be accurate after being quizzed. Only five were.This is a counterintuitive finding, Epley explained in a piece for NPR. “The vast majority of people we surveyed predicted that actively adopting another person’s perspective would help them understand another person better in a variety of ways, from understanding another person’s reaction when looking at a picture to predicting movie preferences,” he wrote. “Perspective-taking may work some wonders for your social life, but understanding another person better does not seem to be one of those wonders.”The lesson for Epley can seem “painfully obvious.” To understand someone, we should not imagine their point of view but make the effort to “get” their perspective. “True insight into the minds of others is not likely to come from honing your powers of intuition,” Epley wrote, “but rather by learning to stop guessing about what’s on the mind of another person and learning to listen instead.”This takeaway undermines the idea that moral progress depends on perspective-taking, a thesis that George Eliot built into her classic novel, Middlemarch. “Middlemarch is deeply ethical,” Rebecca Newberger Goldstein, a philosopher, novelist, and author, most recently, of Plato at the Googleplex: Why Philosophy Won’t Go Away, said. “The differences between her characters are ethical differences which are shown as differences in the limits of their capacity for sympathetic imagination. All of her characters are ... after their own wellbeing, but, for some of them, their characters are such that they are able to imagine themselves into others. They are the characters who undergo moral progress and moral expansion. She makes the limits of imagination—not the limits of reason—essential to how much moral progress a character can make.”Epley, by contrast, doesn’t seem as taken by the ability to imagine the inner reality of others, since the data indicates that what we picture is often spurious. He’s advocated conversation over imagination for years. In 2015, Epley told Nautilus features editor Kevin Berger, “Another person’s mind comes through their mouth.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14849_f25c6ac5f918996124bfb5cb65201434.jpg",
    "title": "Larry David and the Game Theory of Anonymous Donations",
    "description": "Posted by Brian  Gallagher on June 08, 2018  In a Curb Your Enthusiasm episode from 2007, Larry David and his wife Cheryl and their friends attend a ceremony to celebrate his public donation to the National Resources Defense Council,…",
    "category": "Culture",
    "content": "In a Curb Your Enthusiasm episode from 2007, Larry David and his wife Cheryl and their friends attend a ceremony to celebrate his public donation to the National Resources Defense Council, a non-profit environmental advocacy group. Little does he know that the actor Ted Danson, his arch-frenemy, also donated money, but anonymously. “Now it looks like I just did mine for the credit as opposed to Mr. Wonderful Anonymous,” David tells Cheryl. David feels upstaged, as if his public donation has been transformed from a generous gesture to an egotistical one. Cheryl says, about Danson, “Isn’t that great? He donated the whole wing. Didn’t want anybody to know.” “I didn’t need the world to know either!” David says. “Nobody told me I could be ‘anonymous’ and tell people!” He would have done it Danson’s way, he says, but, realizing the contradiction, he fumes, “You can’t have it halfway! You’re either anonymous, or you’re not.” What Danson did, David concludes, is “fake philanthropy and faux anonymity!”I thought of this scene this week, after reading a new study in Nature Human Behavior. “People sometimes make their admirable deeds and accomplishments hard to spot, such as by giving anonymously or avoiding bragging,” write the authors—Moshe Hoffman, Christian Hilbe, and Martin A. Nowak, evolutionary biologists from Harvard University and the Institute of Science and Technology Austria. But if “we give to gain reputational benefits, why would we ever wish to hide the fact that we gave?”The answer to this question may seem less mysterious for anyone who’s seen that Curb episode, “The Anonymous Donor.” We “hide” the fact that we gave precisely for the reputational benefits. For example, at the ceremony, when Danson pops over to David, who’s chatting with then-California Senator Barbara Boxer, she calls Danson a “hero” and stands in awe of the altruism of his “anonymous” donation. Danson playfully shushes her—he’s meant to have only told one or two people but everyone seems to know. David can’t believe it, and later resolves to always donate anonymously for the sake of his reputation.The episode hits on exactly what Hoffman, Hilbe, and Nowak describe in their paper. “Donations are never fully anonymous,” they write. “These donations are often revealed to the recipient, the inner circle of friends or fellow do-gooders,” and these “few privy observers, in turn, do not only learn that the donor is generous” but are “also likely to infer that the generosity was not motivated by immediate fame or the desire for recognition from the masses…”—exactly what everyone seemed to figure was true of David, to his chagrin.What’s intriguing about anonymous giving, and other behaviors apparently designed to obscure good traits and acts, like modesty, is that it’s “hard to reconcile with standard evolutionary accounts of pro-social behavior,” the researchers write. Donations fall under a form of cooperation called “indirect reciprocity.” “Direct reciprocity is like a barter economy based on the immediate exchange of goods, while indirect reciprocity resembles the invention of money,” Nowak wrote in his highly cited 2006 paper “Five Rules for the Evolution of Cooperation.” “The money that fuels the engines of indirect reciprocity is reputation.” Donation evolved, in other words, because it granted a good reputation, which helped humans in securing mates and cementing alliances. But if that’s true, how did the practice of anonymous giving arise? The title of the new paper suggests a solution: “The signal-burying game can explain why we obscure positive traits and good deeds.”The signal-burying game is one of the latest examples of scientists gaining insight into human behavior from game theoretic models and signalling theory. These games, the authors write, make sense of “seemingly counterintuitive behaviors by carefully analyzing which information these behaviors convey in a given context.” Geoffrey Miller, an evolutionary psychologist at the University of New Mexico, said recently on Sam Harris’ podcast, “Waking Up,” “Signalling theory is probably the part of game theory I use most often. The idea there is: How do you credibly demonstrate what kind of organism you are through the signals you give out? And what makes those signals honest, and hard to fake, rather than easily faked, like cheap talk?”In the signal-burying game, a sender and a receiver pair up randomly, and are rewarded for the kind of match that is made. There are three types of senders (or donors)—low, medium, and high—and two types of receivers—weakly selecting and strongly selecting, or weak and strong for short. A strong receiver corresponds to one of the donor’s close friends or a fellow altruist, and a weak receiver to a member of the general public. The best payoff results from a strong receiver partnering with a high sender, while the worst payoff results from a weak receiver partnering with a high sender.However, the players know their own type but not each other’s. So the sender aims for an optimum pairing by choosing the kind of signal to send, and the receiver chooses whether to partner with the sender based on the signal. The sender can choose to reveal their type by the costliness of their signal (the donation amount) and how it’s sent (buried or clear, anonymous or public), or whether they send one at all (for simplicity, the authors assume low senders can’t signal, because the cost is prohibitive). Clear signals are always spotted by strong and weak receivers. Buried signals are more likely to be revealed by receivers, and tagged as buried, if they’re sent by a high sender. The end game is for the players to partner up for “some economic interaction,” the authors write. The payoffs for each player in that interaction depend on the sender-receiver types paired (high sender, strong receiver, for example), not the signals sent and received. Senders always want to partner up, no matter the receiver, since the payoff is always positive, but “strong receivers get a positive payoff only from interacting with high senders, and weak receivers get a positive payoff only from interacting with high or medium senders.”In their paper, Hoffman, Hilbe, and Nowak show the conditions under which signal-burying can be maintained. “First, high senders need to prefer sending a buried signal to a clear signal,” they write. “In equilibrium, burying allows high senders to gain access to some strong receivers (who would have rejected the clear signal but accept buried signals when they are revealed). However, burying also causes high senders to lose some weak receivers (who would have accepted the clear signal, but now may fail to notice the buried signal).” Second, medium senders need to prefer signalling clearly over burying. Third, medium and high senders need to be willing to pay the signalling cost.David is a medium sender. He tries to signal his wealth, generosity, and public concern by publicly incurring a substantial cost via donation to an important cause, like protecting the environment. In their paper, the authors suggest that burying this signal—anonymizing the donation—is another way to signal the same thing, but in a way that’s harder to fake, or more difficult for receivers to find dishonest. Their explanation of why someone like Danson would obscure his good deed “is based on the intuition that making a positive signal harder to spot can serve as a signal in itself,” they write. By burying it, Danson may be showing that he doesn’t care about wide public recognition, even if it would come off as impressive; or, “alternatively, burying may also signal confidence that receivers are liable to find out anyway.”Part of what’s so amusing about the “The Anonymous Donor” is that, to David, Danson’s anonymity seems calculated despite his profession of noble intent. “I kept my name off because it’s the exhibit, it’s the issue, that needs to stand forward, not me,” he tells Senator Boxer. Perhaps it’d be cynical to assume that anonymous donors are all as covertly egotistical as Danson seems. Last year, 17 donations in the United States of more than $10 million were given anonymously. But these signal buriers aren’t necessarily being shrewd about their reputation, the authors write. Self-effacing strategies are not necessarily consciously chosen, and can become natural and stable in a population if they prove adaptive, “as is arguably the case for our ideologies, tastes and emotions, including our artistic sense or moral intuitions related to anonymous giving.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14898_722bdebbbc5e6f165bdb8a939be2a35f.jpg",
    "title": "Climate Change Is Making Plants Behave Like Costco Shoppers",
    "description": "Posted by Brian  Gallagher on June 19, 2018  Plants have their own form of money: carbon dioxide. For decades, our fossil fuel industry has been artificially inflating their currency. What happens to plants during inflation—when…",
    "category": "Biology",
    "content": "Plants have their own form of money: carbon dioxide. For decades, our fossil fuel industry has been artificially inflating their currency. What happens to plants during inflation—when CO2 levels in the atmosphere rise?The same thing that happens if you drop money from the sky over Times Square, leaving everyone there with $1,000 in their pockets, says Hope Jahren, a geochemist and geobiologist at the University of Oslo, and author of Lab Girl, a personal memoir of her life in science.* “Some people would save it; some people would run out and buy clothes; some people would gamble it away within 5 minutes,” she told Nautilus editor in chief Michael Segal. Plants face similar choices. “Some build a bunch of new leaves; some make a bunch more flowers; some shunt it into their roots; some stop making defense compounds. I call that the Costco effect. If you go buy 100 rolls of toilet paper, you’re going to use toilet paper at your house very differently than if you’re buying it roll by roll. So plants, if it’s that easy to make a new leaf, you’re going to treat the ones you have very differently, if that sugar is just coming in free.”Not having to endure costs radically changes behavior, in humans and in plants. “It’s a different world when money is free,” Jahren said, “and thinking about a world where plants operate utterly unconstrained by that particular resource is very interesting.” To see why, watch the video below, and catch the rest of our interview with Jahren here.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.* You can read our excerpt of Hope Jahren’s memoir, Lab Girl, here. \n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14854_70522cd467f1001ad2c2d009707f61d9.jpg",
    "title": "Desert Air Will Give Us Water",
    "description": "Posted by Brian  Gallagher on June 12, 2018  Last year, after a punishing four-year drought, California lifted emergency water-scarcity measures in all but four counties. Residents could sigh in relief but not without resignation.…",
    "category": "Matter",
    "content": "Last year, after a punishing four-year drought, California lifted emergency water-scarcity measures in all but four counties. Residents could sigh in relief but not without resignation. “This drought emergency is over, but the next drought could be around the corner,” California Governor Jerry Brown said at the time. “Conservation must remain a way of life.”He’s right. In April, a study in Nature Climate Change, based on climate model simulations, concluded that a 25 percent to 100 percent “increase in extreme dry-to-wet precipitation events is projected” for the rest of this century, “despite only modest changes in mean precipitation. Such hydrological cycle intensification would seriously challenge California’s existing water storage, conveyance and flood control infrastructure.” In 2015, when a record-setting low of California mountain snowpack was being set, Richard Saykally, a water chemist at UC Berkeley, told me it wouldn’t be unprecedented for the drought to last for decades. “There has been a record of far worse droughts than what we’re experiencing now,” he said, referring to tree-ring data which indicates that, some 500 years back, California suffered something like a 150-year drought. “It is fully possible that this could turn into a 50-year drought or 100-year drought, which would be devastating, unless we have reliable sources of water that don’t rely on precipitation.”A partial solution may be to snatch water from the air, Dune-style. A team of six researchers, led by UC Berkeley chemist Omar Yaghi, reported in Science Advances this month a method of producing water from desert air using just the power of natural sunlight. The researchers say the aluminum-based water absorbent they used “potentially” meets the needs of large-scale industrial production, and “should be applicable to various regions of the world.” The device is an update on a proof-of-concept he and his colleagues unveiled in a 2017 paper. It looks like a little microwave in this short video, where Yaghi describes the absorbent materials as “metal-organic frameworks, or MOFs.” It’s a white, powder-like substance that water in the atmosphere can’t help but cling to.For the new study, he and his colleagues tested the water harvester in the Sonoran Desert, in Arizona, in late October, and successfully collected water after a failed first attempt. “The desert experiment uncovered key parameters pertaining to the energy, material, and air requirements for efficient production of water from desert air,” the researchers write. No doubt residents of California, home to three deserts and over a dozen desert cities of populations over 25,000, will find this interesting.Desalination projects, though mostly expensive and inefficient, could meet the challenges of drought. Some of the latest improvements are inspired by Alan Turing, particularly a 1952 paper of his, “The chemical basis of morphogenesis.” That paper, researchers wrote in a Science study last month, “theoretically analyzed how two chemical substances, activator and inhibitor, can, under certain conditions, react and diffuse with each other to generate spatiotemporal stationary structures.” The authors of the study—all based in China—say that it has “profoundly influenced” scientists’ theoretical understanding of how patterns in chemical and biological systems form. As a result, they were able to design a “Turing-type” structure with bubbles or tubes, a “polyamide membrane,” to purify water. “These unusual nanostructures,” they conclude, “...exhibit excellent water-salt separation performance that surpasses the upper-bound line of traditional desalination membranes.”Saykally also holds out hope for desalination projects. “I’ve gotten very interested in the physics and chemistry of these desalination plants,” he said. Carbon nanotubes are used to filter the salt out of seawater, but “it’s possible that that can be done with much less energy input because the resistance to pushing water through these tubes may be much lower than with current technology…We need to study the behavior of ions at the interface of water with these carbon membranes, and it’s possible that the nature of that interface is such that with proper geometry, water can flow through tubes of pure carbon with very low resistance so that you could use much lower pressures to force the seawater through the desalinating membranes.” It’s an “exciting prospect,” he said, “that would greatly mitigate the energy consumption” of current desalination plants.Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14894_d7c603d749620911ae33948d0d688b6a.jpg",
    "title": " Money Doesn’t Buy Happiness—But Time Just Might Do It",
    "description": "Posted by Jeanette Bicknell on June 18, 2018  While on vacation in distant locales, people often find that time moves quite differently than in the places they’re used to. In the tropics, we settle into the grooves of “island time”…",
    "category": "Culture",
    "content": "While on vacation in distant locales, people often find that time moves quite differently than in the places they’re used to. In the tropics, we settle into the grooves of “island time” and relax thanks to a more leisurely rhythm. A trip to a big city can leave us exhilarated but also drained by the energetic whir of life there.The different paces of different communities also seem to be connected to other cultural characteristics. Robert Levine and his colleagues have studied the speed of life in cities around the world and across the U.S. In a series of experiments they measured how fast solitary pedestrians in a downtown core covered a distance of 60 feet (being careful to exclude those who are obviously window shopping), timed how long it took to complete a simple commercial transaction, and recorded the accuracy of randomly selected clocks in the downtown business area. They found that places with a faster pace of life also had more robust economies (as measured by GDP per capita, average purchasing power, and average caloric intake), and that people in larger cities tended to move faster than those in less populated areas. They also found truth to the stereotype that people move slower in hotter places.So as you might expect, fast-moving people are associated with fast-moving economies. But does that faster life translate into greater happiness? In faster places (specifically, economically developed areas of North America, Western Europe, and Asia), people were more likely to smoke, less likely to take the time to help strangers in need, and more likely to die from coronary heart disease. Yet Levine and his colleagues found that residents in faster places tended to report feeling somewhat happier with their lives than those who lived in slower places. A city’s pace of life was indeed “significantly related” to the physical, social, and psychological well-being of its inhabitants.Perhaps the higher reported rates of happiness simply reflect the fact that faster places have more robust economies. But the relationship between income and reported happiness is far from obvious. According to the “Easterlin paradox” (named after economist Richard Easterlin), once people have enough money to meet their basic needs, having more money is not necessarily correlated with higher self-reported happiness. Easterlin’s claims are controversial and not universally accepted; even if his theory is correct, wealthier nations might be happier overall if they address the basic needs to more of their people. In any case, the ongoing debate indicates that we need to tread carefully when making connections between happiness and overall economic factors.Among individuals in a society, busyness—or the feeling of busyness—seems to be an important factor in well-being. That feeling of busyness—of having a lot to do and too little time in which to do it is often associated with stress and anxiety. However in many contexts being “busy” is badge of honor: Busy parents are seen as devoted to their children’s well-being, the busy real estate agent must be closing lots of sales, and the busy lawyer can charge a premium hourly rate. In U.S. studies, the happiest people reported that they were busy, in the sense that they had little excess time, yet did not feel rushed. Like big-city dwellers, they seemed to thrive at a faster pace.Levine’s work raises the intriguing possibility that an individual’s feelings about their use of time contribute as much or more to their happiness as does economics. Now the big challenge is to find out which way the causal chain works: Does the feeling of being active, yet not rushed, contribute to happiness? Or does happiness allow people to perceive their use of time in positive ways?Jeanette Bicknell, Ph. D., is the author of Why Music Moves Us (2009). She lives in Toronto, Canada.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The direction in which time flows.M.I.T. professor of physics” data-credits=”” style=“width:733px”>This classic Facts So Romantic post was originally published in January 2014. "
  },
  {
    "imageUrl": "http://static.nautil.us/14902_2b662c2bc1569e2da1612f9ed47a400a.jpg",
    "title": "Scavenging Russia’s Rocket Graveyard Is Dangerous and Profitable",
    "description": "Posted by Paul Cooper on June 20, 2018  The Altai mountain region of Central Asia is a rugged and remote place. Right in the center of the continental landmass, it forms a crossroads between the Kazakh steppes, the snow forests…",
    "category": "Culture",
    "content": "The Altai mountain region of Central Asia is a rugged and remote place. Right in the center of the continental landmass, it forms a crossroads between the Kazakh steppes, the snow forests of Siberia and the arid plains of Mongolia. It’s a landscape of granite, forced up by the inch-a-year collision of the Indian tectonic plate with Asia, then carved out over millions of years by streams of snowmelt. Siberian Ibex wander here along with musk deer feeding on the lichenous rocks and brown bears that follow the retreating snow fields in spring.This might be one of the most remote places on earth, little accessible by road, but its peace is routinely broken in the most dramatic way. That’s because the Altai region sits right beneath the main flight path of the oldest, largest and busiest spaceport in the world: the Baikonur Cosmodrome. Debris from each rocket launch rains down on these remote hills, and the people of the region are forced to make a living among the falling scraps.Built in 1955 in the grasslands of southern Kazakhstan, the Baikonur Cosmodrome has been the launch site for many historic missions. Earth’s first artificial satellite Sputnik 1 launched here, and Yuri Gagarin’s 1961 flight as the first human into space also took off from Baikonur. Today it’s home to space missions from around the world, including monthly commercial, scientific and military launches. Since the retirement of the US shuttle program, Russian Soyuz capsules launched from Baikonur have also become the only remaining lifeline to resupply and re-man the International Space Station (ISS). Between 2006 and 2018, NASA paid the Russian space agency Roscosmos approximately $3.4 billion USD to ferry astronauts to the station.Even farm tools and sledges for children have been built out of the fragments of rocket hulls.All this traffic creates a huge amount of debris along the corridor of rocket flightpaths. To lift a 6.3 ton satellite into geostationary orbit requires a 4-stage Russian Proton rocket weighing nearly 700 tons. As the rocket streaks off in its northeasterly flight path, booster rockets peel away from the craft in 3 stages and fall back down to Earth. While rockets launched from NASA’s Kennedy Space Center in Florida are able to drop their boosters relatively harmlessly into the Atlantic ocean, Baikonur is about as as far from the ocean as it is possible to get. That means the discarded stages of Russian rockets tumble back down on dry land. The first stage usually falls within 90 kilometers of the launchpad, but the second stage flies for a full 14 minutes more, and lands with potentially devastating impact 1,000 kilometers away in the Altai.Russian media estimates that over 2,500 tons of space debris have rained down on the region since launches began in the 1950s, some segments as large as 10 meters in length. During the Soviet era, the USSR took great pains to recover Baikonur booster rockets, partially due to concerns about leaking secrets related to the space program. However, since the collapse of the Soviet Union, these pieces of space debris have increasingly been left to rust in the grasslands of Kazakhstan and the Altai mountains.The launches have become a familiar sight for those who live beneath the flight paths. Pieces of falling debris look “like an angry red eye in the night,” one resident says. Then there is a great thundering sound, and “a small earthquake” shakes the ground. Roscosmos designates a narrow strip of land across the region in which the rocket stages are supposed to fall. Residents within this zone are given 24 hours’ notice of a launch to get themselves to safety, and it’s only outside this zone that people can claim compensation for damages. However, incidents outside the zone are far from uncommon. In 2008, a piece of debris measuring 4.5 meters in length even landed in a village, narrowly missing a house. Debris rains down even when launches go to plan; failures can have much more serious consequences. In 2011 an unmanned Progress 44 capsule atop a Soyuz-U rocket was headed to the ISS when it failed in the first 5 minutes of its launch. The rocket tumbled back down to Earth with its later stages still full of fuel, and hit the mountains of the Altai. The resulting explosion shattered windows up to 100 kilometers away, though no one was hurt.While many residents fear the monthly rocket launches that pepper the region with debris, others see in it a unique opportunity. Resourceful scrap dealers wait for the announcement of rocket launches and then watch the sky with binoculars. They track the paths of debris and ride out on jeeps and even on horses to the crash sites. With little protective gear other than welding masks, they use blowtorches to strip the wrecks of their valuable light metals, alloys of titanium and aluminium, as well as other useful components like copper wire. Visitors to the region have reported seeing the roofs of chicken coops and sheds built with rocket parts still showing the original Proton insignia. Even farm tools and sledges for children have been built out of the fragments of rocket hulls.It’s dangerous work for the scrap dealers. Rocket parts are frequently still burning when they arrive, leaching noxious vapours and setting off wildfires in the dry steppes. Nevertheless, the precarious economic reality of the region means that the extra source of income is indispensable. Here, a whole industry has sprung up around the waste of the world’s richest nations as it rains periodically from the sky.Residents’ tendency to bring parts of the rockets back to their villages poses serious dangers, though. Rocket fuel, especially of the kind used by Russian and Chinese rockets, contains highly toxic components, and discarded rocket sections can have as much as 10 percent of their fuel remaining when they tumble back down to Earth. Of these, the most feared is heptyl, which is extremely toxic, and has been linked to cancers and birth defects. In 2008, a farmer in the Altai region filed for compensation when four of his horses died after toxic substances from rocket debris allegedly leached into the soil of their grazing lands.The people of the Altai depend heavily on the land, growing crops, raising livestock and foraging for pine nuts in the virgin snow forests of Siberian cedar. Doctors in the region blame increasing health problems on the proliferation of space junk falling down to earth and the toxic compounds that make their way into the food and water. In 2005, a Nature study showed children in affected areas were twice as likely to develop endocrine and blood disorders, with rates of all other diseases markedly higher too. In some villages, virtually every child is reportedly born with jaundice. One farmer has even recounted seeing Siberian deer wandering the snow forests, blinded by toxic chemicals. The Russian government denies any evidence of a connection between the rocket debris and ongoing health problems, but they have elected to keep their research out of the public eye.Despite its long history, Baikonur’s heyday might soon come to an end. Currently under construction is the Vostochny Cosmodrome, a new facility in the far east of Russia intended to reduce dependency on the landlocked Baikonur. At a cost of $7.5 billion USD, it might help to reduce the environmental impact of rocket launches on the people of the Altai. However, reports of corruption and the exploitation of workers has dogged the expensive project, and it will not begin to launch heavier rockets until 2021.Today, the pockmarked and poisoned grasslands of the Altai seem like a striking metaphor for the world’s inequality and the costs of technological progress. While the richest nations send their citizens and machines to explore the further reaches of the solar system, their debris rains down on some of the world’s poorest. The region asks us to contemplate what moral price our progress comes at, and who in the end gets to decide this cost.\n\tThe newest and most popular articles delivered right to your inbox!\nThis article was originally published on DiscoverMagazine.com on June 7, 2018. "
  },
  {
    "imageUrl": "http://static.nautil.us/14790_f26990c534c3b5a550f83fceb0efce05.jpg",
    "title": " The True Story of Medical Books Bound in Human Skin",
    "description": "Posted by Ian Chant on June 01, 2018  In 1868, on a hot, midsummer day, 28-year-old Mary Lynch was admitted to the Philadelphia Almshouse and Hospital, the city hospital for the poor, better known as “Old Blockley.” Lynch…",
    "category": "Biology",
    "content": "In 1868, on a hot, midsummer day, 28-year-old Mary Lynch was admitted to the Philadelphia Almshouse and Hospital, the city hospital for the poor, better known as “Old Blockley.” Lynch had tuberculosis, which was soon to be compounded by the parasitic infection trichinosis. She didn’t recover, dying in Ward 27 the following year, weighing just 60 pounds. The physician who performed her autopsy, John Stockton Hough, had an interest in rare and obscure books, and he was looking to rebind a trio of anatomical texts on human reproduction. So, he removed a section of skin from Lynch’s thigh, tanned it into leather in the hospital’s basement, and repurposed it as the books’ covers. Lynch lives on in these three volumes, now housed at the Mutter Museum of the College of Physicians of Philadelphia. They are among numerous titles bound with human skin. Some of those date back at least as far as the 16th century, though their origins can be difficult to verify. The same goes for other objects made of leather from human skin; tales of them abound in history, but tend to be apocryphal, says librarian Beth Lander, who oversees the Mutter Museum’s human skin books. Lampshades made from the skin of concentration camp victims are a familiar trope, for instance, but their existence has never been confirmed. A newspaper article, from 1888, recounts the author’s interaction with a man proudly wearing shoes crafted from human skin. The writer lists his acquaintance with various people who had human skin cigarette cases, slippers, and matchbook covers, suggesting that he was either taking liberties or ran in some deeply troubling circles. Even the human skin books at the Mutter Museum are in the company of a wallet long-purported to be made of human skin, but later shown to be just cow and sheep leather, says Lander. These human skin books, then, are the rare artifacts that prove that the practice of making leather goods from human skin is more than just a ghoulish legend.According to Richard Hark, a chemistry professor at Juniata College in Huntingdon, Pennsylvania, medical-history and anatomy books are among those most commonly covered using human leather. “Many of the books we’ve encountered are medical books bound by physicians,” says Hark. “It’s a way to honor that individual who didn’t survive but did contribute to medicine.”Hark is part of the Anthropodermic Book Project, a group of researchers that analyzes books rumored to be bound in human skin. He was first pulled into it when librarians at his own college asked him to investigate whether a book in the school’s collection might fall into that category. Scrawled on the inside cover of Biblioteca Politica, a Spanish political tract dating from the 17th century, was a note indicating that the binding was human in origin. The inscription became a well-known piece of campus lore, turning the title into a nuisance for Juniata’s librarians. They found themselves spending an inordinate amount of time fielding questions from students about the book’s provenance, especially around Halloween. The ritual of binding books in human skin seems, at best, a dubious honor for the deceased.“A lot of these titles had very fantastical stories around them and the literature surrounding them was mostly conjecture, rumor, and innuendo,” says Megan Rosenbloom, a medical librarian at the University of Southern California, in Los Angeles, who also works with the Anthropodermic Book Project. “There was almost no hard evidence about any of them.”A Spanish law textbook in Harvard University’s Weissman Preservation Center Library, for example, is inscribed with the following:The bynding of this booke is all that remains of my dear friende Jonas Wright, who was flayed alive by the Wavuma on the Fourth Day of August, 1632. King Mbesa did give me the book, it being one of poore Jonas chiefe possessions, together with ample of his skin to bynd it. Requiescat in pace.Yet when Hark and his collaborators tested it, they found it to be covered in traditional sheepskin. Daniel Kirby, a private conservation scientist based in Milton, Massachusetts, and another member of the Anthropodermic Book Project, says this wasn’t surprising: The titles most likely to be bound in actual human skin are often those with the least exciting origin stories since they turn out to be mostly medical textbooks.Traditionally, if a scientist wanted to determine what species a book’s binding came from, they would take a sample, isolate DNA, and test it. But with the books Hark and his colleagues are studying, such evidence can be difficult to tease out. To turn animal skin into a hide suitable for shoes, coats—or bookbinding—tanners cure it in salt, soak it in water, then treat it with a series of harsh chemicals to soften it and prevent it from decomposing. To tan poor Mary Lynch’s skin, Hough used a bedpan that was likely filled with ammonia-rich human urine—a substance that would have been plentiful in the hospital, and was historically used in tanneries to prepare hides. “After that sort of treatment, you’re not likely to find a lot of usable DNA,” says Hark.Rather than trying to identify DNA, Kirby and Hark use a technique known as peptide mass fingerprinting to analyze the peptide molecules—short chains of amino acids—found in collagen, a protein that can be found in skin, teeth, hooves, and antlers across the animal kingdom. Collagen is also tough, and because it stands up to far more abuse than DNA, it can be analyzed long after the death of the animal it came from. To identify a book cover’s source, Hark generally takes a sample from the binding that’s just barely visible to the naked eye—about the size of a needle prick—then sends it to Kirby. Kirby puts the sample in an enzyme bath, which digests the collagen into its component pieces, called peptides. Each of the different peptides has a unique mass, and each species has a unique peptide combination. Using a mass spectrometer, researchers can determine the different peptides in a sample and identify the animal’s taxonomic family. The method isn’t that precise—Hark and Kirby can’t use it to differentiate between, for example, human and gorilla collagen—but primate samples are sufficiently different from those of cows and sheep for the researchers to determine whether a binding is primate or ungulate in origin. Working with Rosenbloom and Mutter Museum curator Anna Dhody, Hark and Kirby have tested over 30 books since the project began in 2014. They found that 16 were the real deal. Hark and Rosenbloom have identified over a dozen more, in holdings around the world. Back at Juniata College, Hark and Kirby found that the Biblioteca Politica was bound using plain old sheepskin—much to the relief of the school’s librarians who can now discredit all remaining rumors. The ritual of binding books in human skin seems, at best, a dubious honor for the deceased. Rosenbloom says that when she first encountered the practice, she couldn’t comprehend how anyone would see it as veneration. Hark, though, argues that it’s not so different from more accepted means of remembering those who have passed, be it with a lock of hair or an urn of ashes. Lander takes a dimmer view. Instead of a way for the physicians to honor their patients, she suggests the bindings may have reinforced the feelings of doctors as social superiors, especially to their poorer patients like Mary Lynch. It may have been that the physicians regarded their almshouse patients with some contempt, Lander says. “There was the perception that if they could not serve society adequately in their lives, they could then be of service in their deaths.”Ian Chant is a freelance journalist who writes about science, culture, and publishing. His work has appeared in Popular Science, Popular Mechanics, Scientific American Mind, and other places as well.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in June 2016. "
  },
  {
    "imageUrl": "http://static.nautil.us/14760_d45b6d92e35ff59d3f5a1861e431d581.jpg",
    "title": " This Man Memorized a 60,000-Word Poem Using Deep Encoding",
    "description": "Posted by Lois Parshley on May 29, 2018  Of man’s first disobedience, and the fruit of that forbidden tree,” John Basinger said aloud to himself, as he walked on a treadmill. “Of man’s first disobedience…” In 1992,…",
    "category": "Biology",
    "content": "Of man’s first disobedience, and the fruit of that forbidden tree,” John Basinger said aloud to himself, as he walked on a treadmill. “Of man’s first disobedience…” In 1992, at the age of 58, Basinger decided to memorize Paradise Lost, John Milton’s epic poem, as a form of mental activity while he was working out at the gym. An actor, he’d memorized shorter poems before, and he wanted to see how much of the epic he could remember. “As I finished each book,” he wrote, “I began to perform it and keep it alive in repertory while committing the next to memory.”The 12 books of Paradise Lost contain over 60,000 words; it took Basinger about 3,000 hours to learn them by rote. He did so by reciting the piece, line-by-line out loud, for about an hour a day for nine years. When he memorized all 12 books, in 2001, Basinger performed the masterpiece in a live recital that lasted three days. Since then, he’s performed smaller sections for various audiences, eventually attracting the attention of John Seamon, a psychologist at Wesleyan University, in Connecticut. In 2008, “He recited for an hour in the Wesleyan library,” says Seamon. “He’d given out copies of Milton’s book so we could follow along. At the end of the talk I introduced myself and said ‘I’d love to study your memory.’” Basinger agreed, and so Seamon devised a test.Then 74, Basinger came into the lab to perform a series of cued recall tests. Scientists read two successive lines from each of the poem’s 12 books and then asked Basinger to recall the next 10 lines. The results, published in Memory in 2010, were surprising: Despite the amount of elapsed time since his memorization process, Basinger’s recall was, overall, word-perfect 88 percent of the time. When he was prompted with lines that opened one of the 12 books, his accuracy increased to 98 percent. Seamon wondered how he might explain this performance, and realized deliberate practice theory could be useful. Although it was “formulated to account for elite performance in chess, music, and sports, it provides a reasonable basis for interpreting JB’s procedure for memorising Paradise Lost,” says Seamon. “He too, in daily short sessions, devoted thousands of hours of study over a period of years to achieve his mastery of Milton.” But Basinger didn’t just remember the words; it would be a mistake, says Seamon, to interpret Basinger’s performance as “simply a remarkable demonstration of brute force, rote memorisation.” In order to memorize the epic poem, he spent a lot of time repeatedly analyzing its meaning and structure. Acting researchers emphasize this strategy, Seamon notes: “Deep encoding requires actors to attend to the exact wording of lines, and it is the focus on exact wording to gain an understanding of the characters that yields verbatim memory, instead of merely the retention of gist.” Deep encoding involves making “complex semantic judgments” during a given task, as opposed to “shallow” encoding, which typically consists of just perceptual judgments—like determining the color or size of words—that “result in significantly poorer memory performance,” say David Thomson, a psychologist at the University of Waterloo, and colleagues. Actors like Basinger use deep encoding to give “honest, spontaneous performances, ones that focus on communicating the meanings underlying the literal words,” according to psychologists Helga and Tony Noice. “Indeed, when actors do mention memory,” they write, “it is usually within the context of forgetting the lines until they are needed to communicate the feeling of the moment.” Basinger, Seamon says, “really got into the story, what Milton was trying to convey.” Noice and Noice suggest that this would aid his recall: “Bodily action and emotional response, in addition to semantic analysis, can enhance human memory.”The books describe the Biblical story of the fall of man—the temptation of Eve by Satan then her and Adam’s expulsion from the Garden of Eden—as well as the prior civil war between God and the rebellious angels, led by Satan. Milton’s Paradise Lost, said the 18th century writer and critic Samuel Johnson, demonstrates “his peculiar power to astonish”—especially in his portrayal of Satan as a heroic figure. “There is much to be gained from an aesthetic point of view by making evil characters complex, glamorous, and persuasive, and to do so without changing what they are,” says Gordon Teskey, an English professor at Harvard. “The Greeks taught Milton to make a character majestic in evil without diminishing the evil of evil.” (Milton’s purpose, he writes in Book I, is to “justify the ways of God to men.”) “During the incessant repetition of Milton’s words, I really began to listen to them,” says Basinger, “and every now and then as the whole poem began to take shape in my mind, an insight would come, an understanding, a delicious possibility.”Nothing in Basinger’s tests suggested that his memory was otherwise irregular or exceptional. “His memory for everyday tasks appears entirely normal for someone his age,” Seamon says. “He still forgets where he puts his keys.” For those of us who struggle to remember to-do lists, it’s encouraging to know: “Our findings are in agreement with other research on world-class memory performers,” Seamon says, “which indicates that exceptional memorizers are made, not born.”For his part, Basinger says his years of effort have let him explore Paradise Lost as if it were a physical space. “As a cathedral I carry around in my mind,” he says, “a place that I can enter and walk around at will.”Lois Parshley is a freelance journalist. Follow her on Twitter @Loisparshley.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in May 2016. "
  },
  {
    "imageUrl": "http://static.nautil.us/3655_310cc7ca5a76a446f85c1a0d641ba96d.jpg",
    "title": " What’s Worse: Unwanted Mutations or Unwanted Humans?",
    "description": "Posted by Simone M. Scully on May 21, 2018  After a fatal series of errors and malfunctions in the early morning of April 26, 1986, the core of the Chernobyl nuclear facility melted down and then exploded, killing 31 workers at the…",
    "category": "Biology",
    "content": "After a fatal series of errors and malfunctions in the early morning of April 26, 1986, the core of the Chernobyl nuclear facility melted down and then exploded, killing 31 workers at the plant. The accident spewed massive amounts of radioactive material into the surrounding area, forcing a mass evacuation of the nearby villages. Many wild animals died from the direct toxicity of the radiation and almost 1,000 acres of the Red Forest—named for the unusual color its trees turned after the disaster—died within months. The most radioactive human settlements were bulldozed and buried. (See the related story about the most radioactive part of the nuclear plant: “Chernobyl’s Hot Mess, ‘the Elephant’s Foot,’ Is Still Lethal.”)Checkpoints and fences were quickly put up around the vast contaminated region, stretching between northern Ukraine and southern Belarus. This became the exclusion zone, a region that has remained closed to most human activity for the past 28 years.Yet the area is far from a barren wasteland. Instead it is a patchwork of “hot zones” of high radiation next to “clean” areas. Many of the most radioactive isotopes have decayed or washed out of the region in rain. Birds, rodents, elk, lynxes, wolves, wild boar, and deer have all been spotted living within the borders of the zone. A herd of 21 rare Przewalski’s horses that had escaped from the quarantined area were found to be living in the region. By early 2005, the herd had grown to 64.Many of these Chernobyl animals have been found to have high levels of radiation in their bodies, but none of them appear to have mutated into monsters with abnormal numbers of heads (although in the early years, researchers did find plants and mushrooms that were severely mutated, gigantic, malformed, or even glowing). Does the absence of obvious deformities mean that the animals are thriving? Might the habitats around Chernobyl actually be better off after the disaster?“It can be said that the world’s worst nuclear power plant disaster is not as destructive to wildlife populations as are normal human activities.”Though substantial doses of radiation would not be expected to help wildlife, the accident also had another profound effect: It pushed out the people living nearby. Prior to 1986, few animals were able to inhabit the region around the power plant because Soviet dairy farms and pine plantations had impinged heavily on their habitats. “This region was affected by people over a number of centuries,” explains Sergey Gaschak, a researcher at the Chernobyl Center in Ukraine, who has been photographing wildlife in exclusion zone since the early 2000s. “It was very transformed by irrigation, artificial forests, arable lands, infrastructure, [and] garbage.” In the years following the accident, it appears that animals reclaimed the land, and were spotted regularly by researchers working in the area. “Definitely the wildlife are happy in the absence of humans,” says Gaschak, “[And] externally, the animals appear normal.” His photographs have documented a great number of large mammals such as lynxes and wolves. Because wolves are at the top of the food chain, they are usually regarded as positive signs of the overall health of the ecosystem.Some scientists, including Texas Tech University professor Robert Baker, say that removing the human populations has had the unintentional effect of creating a wildlife reserve. “It cannot be said that radiation is good for wildlife,” Baker writes on his research website. “[But] it can be said that the world’s worst nuclear power plant disaster is not as destructive to wildlife populations as are normal human activities.”But some researchers disagree, arguing that the evidence that wildlife is thriving is merely anecdotal. Other studies have shown that animals and the ecosystem continue to be affected by the radiation.According to a study published earlier this year, trees in the Red Forest appear to not be decaying at a normal rate. Dead leaves continue to litter the forest floor, being two to three times thicker in the “hottest” areas. The study’s findings suggest that the decomposers—the microbes, fungi, and insects that drive the process of decay—have been harmed by the radiation, allowing dead trees and leaves to pile up.A 2013 study from some of the same scientists found lower populations of bumblebees, spiders, dragonflies, butterflies, and birds in areas with higher radioactive readings. The two lead researchers, Timothy Mousseau and his Anders Moller, have found a strong correlation between highly contaminated areas and physical changes in barn swallows. The changes included partial albinism, deformed beaks, cataracts, tumors, smaller brain sizes, and malformed, asymmetrical tails. All of these changes would make the birds less successful at catching food, migrating, and breeding.Mousseau’s research also found that birds that migrate great distances were particularly harmed by radiation. He says this is likely because migration is metabolically very demanding. Metabolic activities can release a lot of mutagens that, combined with radiation, become “a double whammy for these migratory species [making them] more susceptible to the effects of the ionizing radiation,” he explains.But Mousseau also recently found that some birds appear to have developed the ability to adapt to their radioactive environment better than others. After taking blood and feather samples from 152 birds from 16 different species at eight sites, they found that birds with a certain type of pigment in their feathers (pheomelanin) were in poorer health. This was because the production of pheomelanin was using up many of the antioxidants in the birds’ bodies. Usually antioxidants help protect animals from dangerous free radicals caused by radiation. But if the antioxidant levels are too low, free radicals can cause genetic damage and oxidative stress, when they overwhelm the body’s defenses. Mousseau found that the two species of birds that used pheomelanin to produce a lot of pink pigment failed to adapt to their Chernobyl home. The other species didn’t use up their antioxidants making melanin; they could fight off the stress of radiation and were in better health.Fish also appear to be surviving quite well in the zone. Jim Smith, a professor of environmental science at Portsmouth University, has conducted studies on the contamination of fish around Chernobyl. “We don’t see obviously deformed fish at Chernobyl,” he says. “Potentially, there may have been [some] due to very high dose rates [of radiation] shortly after the accident, but dose rates dropped about 100-fold in the first few years after the accident. I think it’s likely that dose rates currently just aren’t high enough to cause a significant number of deformities.” He adds that it is also likely that the populations are resilient because mutated fish don’t survive to adulthood.While scientists don’t agree on whether Chernobyl deserves its unofficial title of “wildlife refuge,” the research shows that different species react differently to chronic exposure to radiation. For at least some of them, it’s worth bearing a few extra mutations to once again have access to a land untrammeled by humans.Simone M. Scully is a science and culture journalist based in New York City. Follow her on Twitter at @ScullySimone.\n\tThe newest and most popular articles delivered right to your inbox!\nThis classic Facts So Romantic post was originally published in July 2014. "
  },
  {
    "imageUrl": "http://static.nautil.us/14775_bf0a8e1dc5c86ca9c4ebb1716579ba9a.jpg",
    "title": "The Psychological Challenges of Just Getting to Mars",
    "description": "Posted by Brian  Gallagher on May 30, 2018  Life outside Earth has its own Hobbesian description: isolated, confined, and extreme—or I.C.E. “Space is the quintessential ICE environment,” according to a new paper, published…",
    "category": "Culture",
    "content": "Life outside Earth has its own Hobbesian description: isolated, confined, and extreme—or I.C.E. “Space is the quintessential ICE environment,” according to a new paper, published in American Psychologist. Space includes inhospitable planets like Mars, whose arresting vistas, canyons, and mountains beckon. But only humans sealed inside cumbersome suits, trained to weather such nerve-racking circumstances, can explore them. Just getting to Mars, says Lauren Blackwell Landon, the paper’s lead author and a behavioral performance researcher at NASA, presents a major challenge. “The astronauts will be months away from home, confined to a vehicle no larger than a mid-sized RV”—the still-under-development Orion spacecraft—“for two to three years,” she says. Unlike on the International Space Station, “there will be an up to 45-minute lag on communications to and from Earth.”Orion is NASA’s answer to the call of deep-space exploration. “It will be the safest, most advanced spacecraft ever built,” a NASA document states, “and it will be flexible and capable enough to take us to a variety of destinations,” including the moons of Mars and, by the mid or late 2030s, the red planet itself. Landon and her co-authors, Kelley J. Slack and Jamie D. Barrett, worry about how a crew of four, Orion’s max capacity, will fare on the journey. They will be “operating in extreme isolation and confinement,” the authors write. “Special considerations,” like screening for certain personality traits, for example, “must be made to enhance teamwork and team well-being…”This may be true for astronauts aboard Orion. But perhaps not aboard SpaceX’s Big Falcon Rocket, or B.F.R., which began construction in March. “The BFR is the big dream,” Alan Boyle, a veteran space journalist and a science editor at Geekwire, said. “It’s what will fulfill Musk’s goal of getting to Mars.” With 40 cabins, it can house upwards of a 200-person crew. “You could conceivably have five or six people per cabin, if you wanted to cram people in,” Musk said last year, in a presentation of the the B.F.R. “But, mostly, we would expect to see two to three people per cabin, so normally about a hundred people per flight to Mars.” Those passengers will also, according to current plans, have “large common areas” at their disposal, with at least one dedicated, Musk added, to “entertainment.” In other words, though space may be the quintessential I.C.E. environment, Musk appears to be aiming to make trips there—aspirationally scheduled to commence in 2024—as far away from I.C.E.-y as possible.It might be a good thing that the Orion spacecraft won’t be taking any astronauts to Mars in the near future. It’s a journey NASA may not know how to train for. Landon and her colleagues write:First, few astronauts have participated in long-duration space missions, and there is a limited number of analog missions per year…Second, researchers and astronauts from different cultures may use disparate models. Third, astronauts have a heavy nominal workload, leaving little time for answering surveys or participating in teamwork research related to mission experiences. Fourth, there is a lack of standard measures, both in spaceflight studies and spaceflight analog studies, further restricting total sample sizes and the comparison of findings across isolated, confined, extreme environments. Due to lack of data from spaceflight and spaceflight analog environments, meta-analysis is simply not a viable option for examining many of the different factors that will be critical to teams on a Mars mission.One recently completed analog study, sponsored by NASA, is HI-SEAS—Hawai’i Space Exploration Analog and Simulation. Without real-time communication to the outside world, six crew members lived and worked for over a year 8,000 feet above sea level, in a dome around the size of a 3-bedroom apartment, to help figure out how well an isolated and confined team could perform as a Mars research outpost. After a few months in, Sheyna Gifford, the crew’s physician, wrote for Nautilus about the surprising lack of “asthenia”—a word that the Russian space program used to describe the way the banality of routine can sap a cosmonaut’s strength.“Whether in real or simulated space, the daily and weekly regimentation of everything we do for the sake of safety and efficiency—like swapping air filters, filing reports with ground control, running someone else’s experiments, and so on—as well as our lack of power to break or even affect the pattern, can become oppressive,” she wrote. “Yet, through it all, asthenia has yet to appear. Here’s why, I think: On [simulated] Mars, each of us knows how un-alone we are and how essential each person on our mission is. The doctor, the biologist, the physicist, the soil scientist, the engineer, and the architect count on each other for nearly everything we need.”Psychological reports like these, Landon and her colleagues say, are valuable despite spaceflight itself not being part of the simulation. “Fortunately, even with a disconnect between the physical environments of analogs and spaceflight, psychological fidelity can be achieved,” they write. “When talking to the astronaut corps about his recent trip searching for meteorites on the surface of Antarctica, Don Pettit, an astronaut with experience on two ISS missions, stated that although the ‘physics’ of Antarctica might be ‘wrong,’ the psychological ‘mind-set’ was right.”Along with needing several more years’ worth of data on what promotes and hinders teamwork on long-duration missions in I.C.E. environments, NASA will also need to be more empirical in its team-selection process, the authors write. “NASA currently does not use a scientifically based approach to composing teams, but this knowledge gap is scheduled to be filled by the 2020s.” Going to Mars on Orion is such a high-stress, high-pressure scenario that aspects of personality and background that don’t normally merit close attention start to matter to team functioning. “Given that nuanced and deep-level characteristics of values, culture, and humor are important for successful long-term teamwork, more research is needed into long-duration, international teams and data-driven methods of team composition that may be applied across cultures,” Landon and her colleagues write. “A team living together for multiple years will be required to not only perform task assignments effectively but also fulfill social roles within the team,” like being the one who can be counted on to ease tensions by cracking jokes.If SpaceX’s B.F.R. avoids some of the challenges of an Orion-style mission to Mars, it also creates new ones—like how to have around 200 people, arriving on two B.F.R.s accompanied by two cargo-filled ones, survive on an alien planet while building the rudiments of a settlement. If things go according to plan, those first explorers will also find two other B.F.R.s filled with cargo—to be launched in 2022—waiting for them. “We should—particularly with six ships there—have plenty of landed mass to construct the [rocket] propellant depot, which will consist of a large array of solar panels, and then everything necessary to mine and refine water, draw the CO2 out of the atmosphere, and then create and store deep cryo CH4 and O2,” Musk wrote in a New Space paper. The more granular details of how six ships transform into a Mars base are undecided. But there’s little doubt that it’ll take teamwork, among other things, to start “terraforming Mars,” as Musk says, “and making it a really nice place to be.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14727_494ad0d24e15c7da81c7ea265c7f4cb4.jpg",
    "title": "Are Healthcare Metrics Hurting Healthcare?",
    "description": "Posted by Daniel Bojar on May 20, 2018  In 1975, the British economist Charles Goodhart pointed out that when a measure becomes a target, it ceases to be a good measure. Goodhart’s Law, as it came to be known, is a ubiquitous…",
    "category": "Numbers",
    "content": "In 1975, the British economist Charles Goodhart pointed out that when a measure becomes a target, it ceases to be a good measure. Goodhart’s Law, as it came to be known, is a ubiquitous phenomenon in regulatory affairs, like healthcare. Making healthcare better requires metrics which can be measured and assessed. But measuring the right metric is sometimes the wrong choice.In order to quantify and characterize health and healthcare, hospitals and government agencies collect massive amounts of data. Typically, this data is gathered by patient surveys, such as the Hospital Consumer Assessment of Healthcare Providers and Systems Survey (HCAHPS), or by the hospital itself (for instance regarding in-hospital mortality rate). The metrics present in these surveys are usually easy to measure—they’re partly used exactly for that reason. Government agencies, in turn, demand improvement in metrics like mortality rate or hospital readmission rate. Hospitals focus on these scores, which can be coupled to financial penalties and loss of patients. This procedure is supposed to financially incentivize hospitals to improve the healthcare system. And this is exactly where the trouble starts.One problem is the potential for “gaming the system.” By nominally fulfilling requirements set by the regulators, the regulated actors can pursue their own aims and effectively weaken the connection between the metric and the regulator’s goal (improved healthcare). One classic example comes from 18th century Britain. At that time, taxation was proportional to the number of windows in a given house. Initially, this was a reasonable choice, as the number of windows tended to increase with house size and was easily measurable. But when people started bricking up some of their windows (you can still see the bricks today) in order to pay lower taxes, the metric (number of windows) and the goal (to tax people according to the size of their house) became misaligned. Goodhart’s Law was rearing its head.Like Heisenberg’s uncertainty principle in quantum mechanics, the assessment of performance metrics as regulatory targets changes their statistical relationship with health.Does Goodhart’s Law constitute a problem in healthcare, too? Unfortunately, the answer is a resounding “yes.” Two intuitive metrics—in-hospital mortality and readmission rate—serve as an ideal illustration. Both of these measures correlate with health (intuitively as well as statistically, once rates are adjusted for socioeconomic status) and are therefore regularly chosen as performance metrics to assess hospitals. But then hospital administrators quickly found roundabout ways to satisfy regulators.One approach was refusing to admit the elderly or the very sick, so that mortality rates would drop. Risky procedures were avoided for the same reason, even if they were necessary. As a result, some populations of patients either received reduced care or no care at all, defeating the purpose of the policy. Another popular performance metric is “readmission rate 30 days after hospital discharge.” Again, ideally this metric would be a stand-in for the quality of care, as a lower rate of readmission could indicate that the health issue was solved during the initial hospital stay. But this does not hold true for patients with chronic diseases, as they usually require multiple readmissions. Yet when regulators tried to enforce this measure as a performance metric, hospitals simply delayed readmissions to day 31 after hospital discharge or even discouraged readmission altogether. Perversely, this resulted in a higher mortality rate (which incidentally also results in a lower readmission rate) and a lower standard of care, especially for patients with chronic diseases.The list of misapplied performance metrics could go on and on. Recently, studies discovered a link between patient satisfaction (captured by patient surveys and Yelp ratings) and health metrics such as mortality rate and readmission rate. Patient satisfaction is also one of the factors positively influencing insurance payments, creating another incentive for hospitals. But maximizing patient satisfaction as a regulatory target will not necessarily result in an improved healthcare system—unless you think that HD television, a lobby with free drinks, or a jacuzzi will improve your health. Likewise, directives to reduce waiting times in U.K. emergency rooms to four hours did not reduce overall mortality. The ERs favored younger patients over older patients, redefined what counts as waiting time and redistributed hospital staff from other departments (whose waiting times increased).Forcing the abstract concept of health into the straitjacket of easy-to-measure attributes leads to a loss of information. Like Heisenberg’s uncertainty principle in quantum mechanics, the assessment of performance metrics as regulatory targets changes their statistical relationship with health. One fix might be calculating a score from a plethora of performance metrics—this increases the chances of a continuous correlation with health, as the score will remain useful even if one or two metrics are corrupted by cheating.However, this would only work if the importance of each metric is roughly similar: A group of metrics completely dominated by a single metric is not more robust to cheating than one metric by itself. Metrics could also be kept secret from the entity being evaluated, though this lack of transparency would create its own problems.The least that can be done is simply this: Be aware of Goodhart’s Law, and cast a sceptical eye at that latest healthcare metric.Daniel Bojar is a PhD student at the Swiss Federal Institute of Technology in Zurich, in the Department of Biosystems Science and Engineering. Follow him on Twitter @daniel_bojar.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The moral boundary between using genetics to prevent disease and enhance ability is constantly shifting.Oncologist, geneticist, and author, most recently, of The Gene: An Intimate History” data-credits=”” style=“width:733px”> "
  },
  {
    "imageUrl": "http://static.nautil.us/14796_8c9b0580ebd12c014a772c9cec371011.jpg",
    "title": "To Persuade Someone, Look Emotional",
    "description": "Posted by Scott Koenig on June 05, 2018  Asked at the start of the final 1988 presidential debate whether he would support the death penalty if his wife were raped and murdered, Michael Dukakis, a lifelong opponent of capital…",
    "category": "Culture",
    "content": "Asked at the start of the final 1988 presidential debate whether he would support the death penalty if his wife were raped and murdered, Michael Dukakis, a lifelong opponent of capital punishment, quickly and coolly said no. It was a surprising, deeply personal, and arguably inappropriate question, but in demonstrating an unwavering commitment to his principles, Dukakis had handled it well. Or so he thought. “The reporters sensed it instantly,” wrote Roger Simon about the scene at the debate immediately after Dukakis gave his response. “Even though the 90-minute debate was only seconds old, they felt it was already over for Dukakis.” Dukakis’ poll numbers plummeted, his campaign never recovered, and George H. W. Bush became the 41st President of the United States.Why were voters so put off by his response to the question? His stance on capital punishment was well publicized at the time, so it shouldn’t have come as a surprise. And it may well have been worse had he advocated a friends-and-family exception to his death penalty opposition. Dukakis appeared capable of consciously suppressing vindictive gut instincts and adhering to higher principles—and that was precisely his problem. When we judge someone by their response to a moral dilemma, be they a stranger on the street or a presidential candidate, instinct may trump reason. According to a recent study, people who make instinct-based moral judgments are perceived by their peers to be more moral and more trustworthy than those who rely on reasoning alone. In other words, we want friends who go with their gut when faced with a moral dilemma. The reverse is true as well: We tend to be wary of people who react to moral dilemmas by calculating costs and benefits—it’s a large part of why we’re so reluctant to trust robots.In politics, it’s well documented that seeming like a friend is at least as important as seeming competent. Most modern politicians have learned from Dukakis’ mistake and try their best to project warm, personable images. Yet no matter how much politicians cater to it, our tendency to judge people based on whether they act according to their instinct is flawed. The unfortunate fact is that instinct-based morality, also known as moral intuition, is often wrong. Consider, for example, how psychopaths can feign emotional expression to manipulate their peers, or how empathy—an instinctive, emotion-laden process—can distort our morals.Moral psychologists and philosophers generally agree that our capacity for moral intuition didn’t evolve to properly handle complex contemporary issues like, say, geopolitical conflict or, in Dukakis’ case, the criminal justice system. Rather, it probably evolved to help us cooperate with each other on a smaller, local scale—to steer us toward or away from certain people in our vicinity, for example. That’s why emotional expression matters so much to us, even at the expense of virtuousness; it’s a quick and easy way to decide whether to trust somebody.Our moral intuition arguably makes many of society’s problems harder to solve. Its power over reason can affect politics, as Dukakis’ case illustrates, as well as seemingly straightforward endeavors like philanthropy. Effective altruism, for example, is a philosophical movement that seeks to coordinate charitable giving “based on reason and evidence.” It operates according to the utilitarian premise of weighing the costs and benefits of choices in order to maximize overall well-being. Despite its noble intentions, however, the movement has some trouble with its image. Peter Singer, one of the figureheads of effective altruism, has aroused controversy for his calculating views on topics like infanticide and disability. Despite his contributions to philanthropy, Singer’s views are, for many people, an example of reason prevailing over emotion to an unsettling extent. Although he and his colleagues may have outlined a good strategy for minimizing suffering worldwide, their lack of emotional appeal seems to be dissuading people from supporting their cause.How, then, can effective altruists—and other well-meaning people who may appear to lack empathy—get their message across? One solution is for them to pretend to be wrestling with their emotions more than they actually are. “Fake it,” said David Pizarro, a professor of psychology at Cornell University and an author of the aforementioned study on moral intuition and trustworthiness. “People want to see that you’ve thought a lot about a tough moral decision. They want to see that you’ve experienced some conflict between reason and emotion and deliberated through it.”Pizarro and his colleagues argue that emotional expression functions as a signal to others that you’ve incorporated feelings into your moral decision. Without that signal, an audience might get the impression that you haven’t experienced any feeling at all—a possibility most people find pretty disturbing. In Dukakis’ case, the right response may have been to mull over the debate question for longer than he did. Even if he’d formed an unconditional opposition to the death penalty long before that debate, appearing to be conflicted before giving his answer probably would have helped his image.But will we ever get to a point at which it’s okay to be morally impartial and calculating, like Spock often is?  There is emerging evidence that the average person is becoming more and more likely to calculate costs and benefits during moral dilemmas. So there’s a chance that we’ll eventually be able to leave unnecessary emotional appeals—the “faking,” the gratuitous dabs—in the rearview mirror, and focus on the facts.Still, Pizarro is skeptical. “I think there will always be a tension there (between reason and emotion),” he said. “It’s part of human nature.” So, until further notice, it may be a good idea to practice looking anguished or moved in the mirror to complement your argument.Scott Koenig is a doctoral student in neuroscience at CUNY, where he studies psychopathy, emotion, and morality. This piece was adapted with permission from Koenig’s blog post “Why don’t we like virtuous people?” published on his website.\n\tThe newest and most popular articles delivered right to your inbox! "
  },
  {
    "imageUrl": "http://static.nautil.us/14757_47f4b6321e9fd8e8f7326a6adc1a7c1e.jpg",
    "title": "How Brain Waves Surf Sound Waves to Process Speech",
    "description": "Posted by John Rennie on May 27, 2018  Reprinted with permission from Quanta Magazine’s Abstractions blog.When he talks about where his fields of neuroscience and neuropsychology have taken a wrong turn, David Poeppel of…",
    "category": "Biology",
    "content": "Reprinted with permission from Quanta Magazine’s Abstractions blog.When he talks about where his fields of neuroscience and neuropsychology have taken a wrong turn, David Poeppel of New York University doesn’t mince words. “There’s an orgy of data but very little understanding,” he said to a packed room at the American Association for the Advancement of Science annual meeting in February. He decried the “epistemological sterility” of experiments that do piecework measurements of the brain’s wiring in the laboratory but are divorced from any guiding theories about behaviors and psychological phenomena in the natural world. It’s delusional, he said, to think that simply adding up those pieces will eventually yield a meaningful picture of complex thought.He pointed to the example of Caenorhabditis elegans, the roundworm that is one of the most studied lab animals. “Here’s an organism that we literally know inside out,” he said, because science has worked out every one of its 302 neurons, all of their connections and the worm’s full genome. “But we have no satisfying model for the behavior of C. elegans,” he said. “We’re missing something.”Poeppel is more than a gadfly attacking the status quo: Recently, his laboratory used real-world behavior to guide the design of a brain-activity study that led to a surprising discovery in the neuroscience of speech.Critiques like Poeppel’s go back for decades. In the 1970s, the influential computational neuroscientist David Marr argued that brains and other information processing systems needed to be studied in terms of the specific problems they face and the solutions they find (what he called a computational level of analysis) to yield answers about the reasons behind their behavior. Looking only at what the systems do (an algorithmic analysis) or how they physically do it (an implementational analysis) is not enough. As Marr wrote in his posthumously published book, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, “… trying to understand perception by understanding neurons is like trying to understand a bird’s flight by understanding only feathers. It cannot be done.”Poeppel and his co-authors carried on this tradition in a paper that appeared in Neuron last year. In it, they review ways in which overreliance on the “compelling” tools for manipulating and measuring the brain can lead scientists astray. Many types of experiments, for example, try to map specific patterns of neural activity to specific behaviors—by showing, say, that when a rat is choosing which way to run in a maze, neurons fire more often in a certain area of the brain. But those experiments could easily overlook what’s happening in the rest of the brain when the rat is making that choice, which might be just as relevant. Or they could miss that the neurons fire in the same way when the rat is stressed, so maybe it has nothing to do with making a choice. Worst of all, the experiment could ultimately be meaningless if the studied behavior doesn’t accurately reflect anything that happens naturally: A rat navigating a laboratory maze may be in a completely different mental state than one squirming through holes in the wild, so generalizing from the results is risky. Good experimental designs can go only so far to remedy these problems.The common rebuttal to his criticism is that the huge advances that neuroscience has made are largely because of the kinds of studies he faults. Poeppel acknowledges this but maintains that neuroscience would know more about complex cognitive and emotional phenomena (rather than neural and genomic minutiae) if research started more often with a systematic analysis of the goals behind relevant behaviors, rather than jumping to manipulations of the neurons involved in their production. If nothing else, that analysis could help to target the research in productive ways.That is what Poeppel and M. Florencia Assaneo, a postdoc in his laboratory, accomplished recently, as described in their paper for Science Advances. Their laboratory studies language processing—“how sound waves put ideas into your head,” in Poeppel’s words.When people listen to speech, their ears translate the sound waves into neural signals that are then processed and interpreted by various parts of the brain, starting with the auditory cortex. Years of neurophysiological studies have observed that the waves of neural activity in the auditory cortex lock onto the audio signal’s “envelope”—essentially, the frequency with which the loudness changes. (As Poeppel put it, “The brain waves surf on the sound waves.”) By very faithfully “entraining” on the audio signal in this way, the brain presumably segments the speech into manageable chunks for processing.More curiously, some studies have seen that when people listen to spoken language, an entrained signal also shows up in the part of the motor cortex that controls speech. It is almost as though they are silently speaking along with the heard words, perhaps to aid comprehension—although Assaneo emphasized to me that any interpretation is highly controversial. Scientists can only speculate about what is really happening, in part because the motor-center entrainment doesn’t always occur. And it’s been a mystery whether the auditory cortex is directly driving the pattern in the motor cortex or whether some combination of activities elsewhere in the brain is responsible.Assaneo and Poeppel took a fresh approach with a hypothesis that tied the real-world behavior of language to the observed neurophysiology. They noticed that the frequency of the entrained signals in the auditory cortex is commonly about 4.5 hertz—which also happens to be the mean rate at which syllables are spoken in languages around the world.In her experiments, Assaneo had people listen to nonsensical strings of syllables played at rates between 2 and 7 hertz while she measured the activity in their auditory and speech motor cortices. (She used nonsense syllables so that the brain would have no semantic response to the speech, in case that might indirectly affect the motor areas. “When we perceive intelligible speech, the brain network being activated is more complex and extended,” she explained.) If the signals in the auditory cortex drive those in the motor cortex, then they should stay entrained to each other throughout the tests. If the motor cortex signal is independent, it should not change.But what Assaneo observed was rather more interesting and surprising, Poeppel said: The auditory and speech motor activities did stay entrained, but only up to about 5 hertz. Once the audio changed faster than spoken language typically does, the motor cortex dropped out of sync. A computational model later confirmed that these results were consistent with the idea that the motor cortex has its own internal oscillator that naturally operates at around 4 to 5 hertz.These complex results vindicate the researchers’ behavior-linked approach in several ways, according to Poeppel and Assaneo. Their equipment monitors 160 channels in the brain at sampling rates down to 1 hertz; it produces so much neurophysiological data that if they had simply looked for correlations in it, they would have undoubtedly found spurious ones. Only by starting with information drawn from linguistics and language behavior—the observation that there is something special about signals in the 4-to-5-hertz range because they show up in all spoken languages—did the researchers know to narrow their search for meaningful data to that range. And the specific interactions of the auditory and motor cortices they found are so nuanced that the researchers would never have thought to look for those on their own.According to Assaneo, they are continuing to investigate how the rhythms of the brain and speech interact. Among other questions, they are curious about whether more-natural listening experiences might lift the limits on the association they saw. “It could be possible that intelligibility or attention increases the frequency range of the entrainment,” she said.John Rennie joined Quanta Magazine as deputy editor in 2017. Previously, he spent 20 years at Scientific American, where he served as editor in chief between 1994 and 2009. He created and hosted Hacking the Planet, an original 2013 TV series for The Weather Channel, and has appeared frequently on television and radio on programs such as PBS’s Newshour, ABC’s World News Now, NPR’s Science Friday, the History Channel special Clash of the Cavemen and the Science Channel series Space’s Deepest Secrets. John has also been an adjunct professor of science writing at New York University since 2009. Most recently, he was editorial director of McGraw-Hill Education’s online science encyclopedia AccessScience.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: How language shaped human evolution. "
  },
  {
    "imageUrl": "http://static.nautil.us/14731_d4ff76af57c1ebcc7eca3807b9a431a6.jpg",
    "title": " Forget “Earth-Like”—We’ll First Find Aliens on Eyeball Planets",
    "description": "Posted by Sean  Raymond on May 22, 2018  Imagine a habitable planet orbiting a distant star. You’re probably picturing a variation of Earth. Maybe it’s a little cloudier, or covered in oceans. Maybe the mountains are a little…",
    "category": "Matter",
    "content": "Imagine a habitable planet orbiting a distant star. You’re probably picturing a variation of Earth. Maybe it’s a little cloudier, or covered in oceans. Maybe the mountains are a little higher. Maybe the trees are red instead of green. Maybe there are scantily clad natives…OK, let’s stop there.That image may very well be completely off-base. There is good reason to think that the first potentially life-bearing worlds that are now being detected around other stars (see here for example) probably look very different than Earth. Rather, these planets are more likely to look like giant eyeballs whose gaze is forever fixed on their host stars (which is not something I recommend doing with your own eyeballs).  Let’s take a step back. The easiest planets to find are those that orbit close to their stars. The sweet spot for finding a habitable planet—with the same temperature as Earth—is on a much smaller orbit than Earth’s around a star much fainter than the Sun. But there are consequences of having a smaller orbit. A planet close to its star feels strong tides from its star, like the tides Earth feels from the Moon, but much stronger. Strong tides change how a planet spins. Tides drive the planet’s obliquity to zero, meaning that the planet’s equator is perfectly aligned with its orbit. The planet will also be “tidally locked”: It always shows the same side to the star. It looks something like this: If you were standing on the surface of a planet like this, the Sun would remain fixed in one spot on the sky. The hemisphere facing the star is in constant daylight and the far hemisphere in constant darkness. In between lies a ring of eternal sunset, quite possibly the most romantic place in the Universe. The hottest part of the planet is the location where the star is directly overhead (the “substellar point” in astro-speak). The hottest part of Earth is spread out across the tropics, depending on the time and the season. But on a tidally locked planet the Sun stays in the same place in the sky and the hot spot never moves. This creates visible differences across the planet’s surface; the relatively small hot spot is the “pupil” of an eyeball planet.Eyeball planets come in all sorts of flavors, depending on the conditions. We are going to take a look at two examples: “hot eyeball” and “icy eyeball” planets. A hot eyeball planet is located close to its star, on an orbit that makes it hotter overall than Earth. It is blazing on the day side and deathly cold on the night side. The planet’s water is boiled on the day side and frozen on the night side. But winds transport the water vapor from the day side toward the night side to freeze. This can create a cold trap: all of the planet’s water can be locked up in a giant layer of ice on the permanent-night side. Dry day side, ice-covered night side.But the story doesn’t end there. When a layer of ice gets thick enough, its bottom layer melts from the pressure. This causes the ice to flow downhill, like glaciers do on Earth. So a hot eyeball planet’s thick night side ice cap spreads out and slowly flows toward the day side. There may be a trickle of water that flows into the light to be evaporated all over again. Our models project that there are characteristic wind patterns that pile clouds up in a specific region on the night side (gory details here). The planet’s non-uniform appearance can really look like an eyeball. Rivers that flow from the night side to eventually evaporate on the day side might even look like veins.Where on a hot eyeball planet could you live? It’s a classic Goldilocks story. The day side is roasting and dry. The night side is frigid and icy. In between, it’s just right! The sweet spot—let’s call it the “ring of life”—is at the terminator, the boundary between night and day. The ring of life is bounded by deserts on one side and ice on the other. There is a constant flow of water from the night side to the day side—a series of rivers, all flowing in the same direction. The Sun is fixed in the sky right at the horizon, and the area is in permanent light. Conditions are pretty much the same all the way across the ring of life. One can imagine vegetation following the rivers onto the day side until they dry up, with different ecosystems interspersed along the way. There could be mountains at the edge of the ice sheets, since the ice-covered continents would be heavily weighed down.Icy eyeballs are also tidally locked to their stars, though their orbits are larger than those of hot eyeball planets, and heat is in short supply. What icy eyeball planets do have is an abundance of water—the night side is covered in ice, and there is enough stellar heating at the substellar point for water to remain liquid. There is essentially a large pond in the midst of a global icy landscape. Below the surface ice, the ocean covers the entire globe. It is similar to Jupiter’s moon Europa, but with a large hole in the ice.Where would you want to live on an icy eyeball planet? Underwater life could exist throughout the subsurface ocean, but surface life that relies on liquid water would need to stay near the pond. Yet the conditions near the substellar point would be extreme: strong stellar irradiation in the midst of barren icy landscape. The best place to live would probably be by the icy shore of the pond, and it would definitely be advantageous to be amphibious (evolution, anyone?).But there is an inherent danger to living on an icy eyeball planet. If, for some reason, the pond froze over, it would likely never re-melt. Liquid water, being dark, absorbs most of the sunlight that hits it. But solid ice is very reflective. So if a planet’s oceans were to freeze, the amount of energy absorbed from the star would also drop, making the planet even colder. This feedback would push an icy eyeball into a completely ice-covered state from which it might never recover.  Hot eyeball and icy eyeball planets are extreme cases, but any planet that is tidally locked to its star is likely to look very different on its day side and its night side. Differences could come from clouds clustered in certain areas, from preferential melting of ice on the day side or freezing of ice on the night side, or from any number of other possible sources. The galaxy may be littered with wild varieties of eyeball planets! The search for life on other planets will almost certainly start with these worlds.Sean Raymond is an astronomer studying the formation and evolution of planetary systems. He also blogs at planetplanet.net.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: Which exoplanet would you visit?This classic Facts So Romantic post was originally published in February 2015. "
  },
  {
    "imageUrl": "http://static.nautil.us/14736_7101e4daaff4511510bbd4e6a0862fb7.jpg",
    "title": "The Case Against Geniuses",
    "description": "Posted by Brian  Gallagher on May 23, 2018  Once you’re called a “genius,” what’s left? Super genius? No, getting called a “genius” is the final accolade, the last laudatory label for anyone. At least that’s how several…",
    "category": "Ideas",
    "content": "Once you’re called a “genius,” what’s left? Super genius? No, getting called a “genius” is the final accolade, the last laudatory label for anyone. At least that’s how several members of Mensa, an organization of those who’ve scored in the 98th percentile on an IQ test, see it. “I don’t look at myself as a genius,” LaRae Bakerink, a business consultant and a Mensa member, said. “I think that’s because I see things other people have done, things they have created, discovered, or invented, and I look at those people in awe, because that’s not a capability I have.”Yet the notion of genius as a capability a person can possess has come under attack recently in several ways. Megan Garber, writing in The Atlantic, criticized the excuses of bad behavior that so-called geniuses—specifically men—enjoy. “Genius, a means to godliness and its best evidence, cannot be argued with,” she writes. “Genius cannot be reasoned with. Genius is the answer and the question. It will be heard. It will be respected.” Not if Garber can help it. “The genius-bias is a strong one,” she writes, but not one we must submit to.Genius, as a noun, has always been a sort of illusion.Whether genius is partly genetic is hard to say; intelligence has a hereditary component, but its link to genius is hazy, as Hans Eysenck points out in his new book, Intelligence: A New Look. “There is no case of a genius having a genius father—the best we can do is Mozart, whose father was a reasonably good musician, and Bach, who had several musically gifted relatives in his family, none of genius rank,” he writes. “For the great majority, father and mother were ordinary folk, without any special gifts or achievements…” Moreover, these parents very rarely provided the sort of promising environments one might expect future geniuses to require. Genius seems to strike without warning.Earlier this month, Yuval Sharon, artistic director of The Industry in Los Angeles, whose experimental operas have won widespread acclaim, shared his anxiety about winning a 2017 MacArthur Fellowship, notoriously called the “Genius Grant.” In an essay titled “Genius as Circumstance” in the Los Angeles Review of Books, Sharon writes, “Moments, ideas, a single poem in a collection—a work of genius, no matter how individually wrought—is never the product of a single individual. We should stop thinking of genius as an attribute and instead start to think of it as a condition, a circumstance.” As a theater director, Sharon writes, “my work consists entirely of creating the conditions for genius to flow.” He defines genius as “the oxygen that those in a shared space breathe in and are transformed by; it allows them to reach their full potential. In this way, ‘genius’ returns to its original Latin meaning of an ‘attendant spirit.’”Jazz artist Vijay Iyer, another MacArthur fellow, is also uncomfortable with the conventional meaning of genius. In a Nautilus interview with Kevin Berger, Iyer says the label “genius” narrows our understanding of art and artists, and by extension, science and scientists. “The ‘G word’ is often used to shut down conversation or inquiry into a particular artist, into his or her community and connection to others,” Iyer says. “No music happens in a vacuum. Anybody who has the privilege of making music for others got there through the help of others. Even real singular talents, and I’ve known many, are nurtured and brought into a community. So I always try to understand the term relationally, understand artists in the historical, social, and political context in which they were living and working.”Both Sharon and Iyer echo the work of Hungarian-American psychologist Mihaly Csikszentmihalyi. “The location of genius is not in any particular individual’s mind, but in a virtual space, or system, where an individual interacts with a cultural domain and with a social field,” he wrote in his 2015 book, The Systems Model of Creativity. He suggests that genius, as a noun, has always been a sort of illusion. “In popular usage, ‘genius’ is sometimes used as a noun that stands by itself, yet in reality it appears always with a modifier: musical genius, mathematical genius, scientific genius, and so forth,” he wrote. “Genius cannot show itself except when garbed in a concrete symbolic form.”David Krakauer, a complexity theorist and president of the Santa Fe Institute, seems to agree. In an interview with Steve Paulson for Nautilus, he said, “I have my own favorite explanation for what genius is. If intelligence is making hard problems easy, genius is making problems go away!” For Krakauer, genius is more of an adjective that applies to solutions rather than scientists. “Genius just changes the rules of the game,” he said, referring to Albert Einstein’s theory of general relativity, and how it superseded Isaac Newton’s view, which couldn’t account for “very large masses and very large velocities.” General relativity didn’t just make our understanding of physics “better, or easier, or more efficient,” as a merely intelligent solution might, Krakauer said; it instead changed “the nature of the representation of the problem [of gravity] so completely that you get the kind of vertigo of unfamiliarity,” which is his litmus test of genius: “When you change the rules, you make a lot of people uncomfortable, and it looks a little crazy.”Brian Gallagher is the editor of Facts So Romantic, the Nautilus blog. Follow him on Twitter @brianga11agher.\n\tThe newest and most popular articles delivered right to your inbox!\nWATCH: The meaning of genius. "
  },
  {
    "imageUrl": "http://static.nautil.us/14793_6c2665d7c3ed1e5bfd8ba600f026eb55.jpg",
    "title": "Braces Have Made Snoring a Modern Health Problem",
    "description": "Posted by Mary Ellen Hannibal on June 03, 2018  The apotheosis of my five-year orthodontic torment was a sad admission from the orthodontist: After thousands of dollars invested in what felt like medieval technology, my braces had not…",
    "category": "Biology",
    "content": "The apotheosis of my five-year orthodontic torment was a sad admission from the orthodontist: After thousands of dollars invested in what felt like medieval technology, my braces had not only failed to ameliorate a complex situation but created a new problem for which, even today, there is no solution. I won’t say it keeps me up at night, but my husband’s snoring often does—and it turns out the braces he wore as a child may be to blame for that.Suffering for the perfect smile has long been de riguer in middle-class American childhoods and beyond. Today an estimated 50 to 70 percent of children in the United States will be treated for “malocclusion,” or bad bite. Twenty-five percent of orthodontic patients today are adults belatedly getting around to this rite of passage. We take it for granted that the effort is worth it, but according to eminent biologist Paul Ehrlich and craniofacial expert Sandra Kahn—the authors of Jaws: The Story of a Hidden Epidemic—braces are a temporary band-aid on a long-unfolding evolutionary disaster. Over the ages our teeth and our tongue have become ever more crowded by the shrinking of the human jaw. Not only is this an aesthetic disaster, but it compromises our breathing, which in turn can disrupt sleep. And there, our problems really begin.It’s harder for us to breathe than it was for our jaw-endowed ancestors.Ehrlich and Kahn trace the crooked jumble of teeth many of us develop to the agricultural revolution. (As Stanford paleontologist Richard Klein says, “I’ve never seen a hunter-gatherer skull with crooked teeth.”) When humans turned to farming, food got softer and we stopped chewing so hard. This lack of exercise, Ehrlich and Kahn hypothesize, led to a reduction in Homo sapiens’ face and jaw size. With less room, our teeth have gotten crowded and often jumbled up. It became harder to breathe.Changes in the position of the larynx to make way for human speech had also had negative impacts on what Ehrlich and Kahn call the “facial-airway configuration.” The tongue, now too big for the restricted jaw, gets in the way of breathing as it falls back and restricts the passageway between the nose and the lungs, “causing a rhythmic rumbling sound” also known to challenge the equanimity of even the otherwise most contented of co-sleeping couples. Ehrlich and Kahn bolster their claim that snoring is an artifact of contemporary civilization, noting that the sound would have alerted predators to “relatively helpless human individuals.”Bulky braces can further constrict this vital breathing pathway, but in Ehrlich and Kahn’s view, the problem is really that they are a cosmetic fix with a short duration. While not banning orthodontics altogether, they advocate developing stronger jaws to begin with. Breast-feeding babies helps because it takes more effort on the child’s part than sucking on a bottle. Encourage your children to chew thoroughly—Ehrlich and Kahn even advocate gum. They warn that “poor oral posture” contributes to constricting airways. A particular culprit is mouth-breathing, often exacerbated by the fact that we find ourselves indoors so much, where concentrated allergens help stuff up our nasal passages. Mouth-breathing is a sign of a slack jaw and is an indication you are not getting enough oxygen. It’s harder for us to breathe than it was for our jaw-endowed ancestors.Smaller jaws, constricted airflow, and snoring come afoul of another signature evolutionary development—sleep, particularly the deep-dreaming phase known as rapid eye movement (REM) sleep. Ehrlich and Kahn warn that sleep disrupted by snoring (and its cousin sleep apnea) are “linked to serious lifelong health problems” including “ADHD, depression, cancer, and heart disease.”Neuroscientist and sleep expert Matthew Walker, author Why We Sleep: Unlocking the Power of Sleep and Dreams, warns that too little snoozing “demolishes your immune system” and adds anxiety and suicidality to the discontents of disrupted, inadequate sleep. According to Walker, the non-dreaming stage called NRED sleep can be detected in all species, but REM sleep, an evolutionarily more recent development, is enjoyed only by vertebrates. Walker credits REM sleep with increasing our ability to recognize and navigate “the kaleidoscope of socioemotional signals that are abundant in human culture,” thus enabling “the creation of large, emotionally astute, stable, highly bonded, and intensely social communities of humans.” REM sleep fuels creativity by integrating new memories into the “entire back catalog of your life’s autobiography,” Walker writes, night after night rebooting vast associative neural networks and revising “what a collection of information means as a whole.”Yet even if improper breathing, credited by Ehrlich and Kahn to our mealy-mouthed jaws, disrupts REM sleep and threatens all of this innovative meaning-making, exceptions still arise. An interesting case study would seem to be that of the novelist Vladimir Nabokov. In the recently published journal Insomniac Dreams, the author of Lolita recounts “Curious features of my dreams,” including “very exact clock time awareness but hazy passing-of-time feeling….” Nabokov queries his own torturously interrupted nocturnal narratives to see if his dreams might predict the future. They don’t, really—although in describing “fairly sustained, fairly clear, fairly logical (within special limits) cogitation,” Nabokov seems to anticipate Walker’s account of REM sleep. Otherwise, the hardly sleeping, fantastically creative Nabokov somewhat belies the sleep-deprivation dangers Ehrlich, Kahn, and Walker point out. Though it must be said that hard at work in one photo, the great author’s mouth does seem to be hanging open.Mary Ellen Hannibal is most recently the author of Citizen Scientist: Searching for Heroes and Hope in an Age of Extinction, and a Stanford media fellow.\n\tThe newest and most popular articles delivered right to your inbox! "
  }
]
